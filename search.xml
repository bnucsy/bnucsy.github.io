<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>A Learned Compact and Editable Light Field Representation</title>
    <url>/A%20Learned%20Compact%20and%20Editable%20Light%20Field%20Representation/</url>
    <content><![CDATA[<h2 id="A-Learned-Compact-and-Editable-Light-Field-Representation【光场压缩、编辑传播】【arxiv】"><a href="#A-Learned-Compact-and-Editable-Light-Field-Representation【光场压缩、编辑传播】【arxiv】" class="headerlink" title="A Learned Compact and Editable Light Field Representation【光场压缩、编辑传播】【arxiv】"></a><a href="https://arxiv.org/abs/2103.11314">A Learned Compact and Editable Light Field Representation</a>【光场压缩、编辑传播】【arxiv】</h2><center>P.S. 本文为自己重述，不完全忠实于原paper，难免出现错误</center>

<p>&emsp;&emsp;该论文完成了光场数据的压缩表示，使用自编码器的方法将光场数据压缩为一个深度图和一个中央视觉图，并且支持编辑之后的中央视觉图和原深度图进行光场重建。</p>
<p><img src="https://img-blog.csdnimg.cn/f7655d1624894b87a117e25801a7b14d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" align="center" style="zoom: 80%;"></p>
<span id="more"></span>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p> &emsp; &emsp; 所谓光场，即记录光线位置和方向的数据结构，这种数据表示方式经历了很大的变化历程，相关的介绍中<a href="https://www.leiphone.com/category/arvr/N14i2K6UmZzK5TcE.html">这个介绍</a>最为清晰和全面。<br>&emsp; &emsp;本文对于这种数据结构进行了自编码器的学习，综合考虑解码后与原图的结构差异，编码的中间层中央视觉图部分的RGB相似度，编辑后的中央视觉图的重建程度，完成了一种支持编辑的光场重建。这项工作的压缩部分解决了光场数据结构较大的问题，可编辑重建部分解决了二维图像处理到三维图像处理的迁移问题，总体来说意义重大。目前本文发布于<a href="https://arxiv.org/abs/2103.11314"> arxiv</a>，也许在不久之后会登上SIGGRAPH。</p>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><ul>
<li><p>光场重建相关<br>&emsp; &emsp; 以往的光场重建工作主要集中于：①<a href="https://dl.acm.org/doi/10.1145/2682631#d32752538e1">从稀疏样本合成密集光场。</a>②<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6574844">通过光场估计视差。</a>③<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7406366">对光场合成的深度学习方法——该方法可以估计扭曲的视差，并更细致地描绘颜色。</a>④<a href="https://arxiv.org/pdf/1708.03292.pdf">通过单张照片合成光场。</a>⑤<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8953925">多平面图像表示光场。</a><br>&emsp; &emsp;这些工作尽管能够解决部分问题，但仍有局限，如前几个工作会出现某些伪影、重影。最后一类工作引出一个新的问题——如何通过编辑这种多平面图象进行光场编辑。<br>&emsp; &emsp;我认为本文的工作在某种程度上重复了④的工作内容，类似于AE和GAN之间的联系。在完成④的基础上，本文解决了⑤引出的可编辑性问题——通过分离特征和视图的方法。</p>
</li>
<li><p>光场编辑相关<br>&emsp; &emsp;以往的这些工作集中于：①目标重定位。②目标变形。③目标补充。④<a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=36554C114E54C1449058A76D6684D6AB?doi=10.1.1.644.3384&rep=rep1&type=pdf">将稀疏编辑传播至全光场。</a>⑤<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7414488">将中心视图分解为不同深度的图层，通过图层进行光场编辑。</a><br>&emsp; &emsp;上述工作中④，⑤两个工作相比而言较为接近本文工作，因此没有给出①②③的原文链接，尤其是④，④最关键的难度在于进行光场编辑的一致性传播，与本文想法接近。有较强的借鉴意义。</p>
<p>&emsp; </p>
<h3 id="本文的工作"><a href="#本文的工作" class="headerlink" title="本文的工作"></a>本文的工作</h3><p>&emsp; &emsp;总体来说，本文能够对一个给定的4D光场，将其表示为一个元通道<code>在没有限制元通道的视觉形态的情况下，为什么它长得这么像一个灰度图？</code>和一个RGB图（视觉通道），这实现了4D光场数据的压缩，另一方面，针对视觉通道的编辑可以通过和元通道的结合重建光场，并一致性地传播到新的光场之中。<br>本文设计的网络结构：<img src="https://img-blog.csdnimg.cn/a35c15d6728748e487dc22097d88cf4c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" width="50%" align="center"></p>
</li>
</ul>
<ol>
<li><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>&emsp; &emsp;输入的4D光场视图即为$M\times N$张RGB图像，我们描述为$L = \{I_i\}_{i=1}^{M\times N}$，对于这个光场$L$,$L(u,v)$也就表示了某一个像素，其中$u$表示图片，$v$表示像素，也就是说$I_i$即为$L(u_i)$。</p>
<p>&emsp; &emsp;在整个自编码器的设计过程之中，我们记编码器为$E$，解码器为$D$。</p>
<p>&emsp; &emsp;一方面，对于输入的光场$L$，我们通过编码器生成一个元通道$Z$，即$Z = E(L)$，有$sizeof(Z) = sizeof(I_i)$。另一方面，对于传入的光场数据，我们获取中央视图$I_c$ <code>Ic具体是如何得到的呢？是L(x,0)么？</code>。对$I_c$进行编辑之后我们获得$\widetilde{I}_c$，接着我们把两者结合，通过解码网络获得编辑后光场$\widetilde L$，即 $\widetilde L = D(Z, \widetilde I _c)$。</p>
</li>
<li><h4 id="表示方式的选取"><a href="#表示方式的选取" class="headerlink" title="表示方式的选取"></a>表示方式的选取</h4><p>&emsp; &emsp;本节实际上主要阐述了为什么要选取元通道和视觉通道。</p>
<p>&emsp; &emsp;中心视图 $I_c$ 捕获了光场的参考视觉内容，因此自编码器只需要学习表示元通道即可。元通道作为视觉通道互补的部分，意味着包括深度、视差、纹理、遮挡等等信息被隐式地编码到元通道之中。事实上本文做了<a href="https://blog.csdn.net/flyfish1986/article/details/104812229#:~:text=%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C%20%EF%BC%88%20ablation%20experiment%EF%BC%89,%E5%9C%A8%E6%9C%BA%E5%99%A8%20%E5%AD%A6%E4%B9%A0%20%EF%BC%8C%E7%89%B9%E5%88%AB%E6%98%AF%E5%A4%8D%E6%9D%82%E7%9A%84%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%83%8C%E6%99%AF%E4%B8%8B%EF%BC%8C%E5%B7%B2%E7%BB%8F%E9%87%87%E7%94%A8%E2%80%9C%20%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6%20%E2%80%9D%E6%9D%A5%E6%8F%8F%E8%BF%B0%E5%8E%BB%E9%99%A4%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9F%90%E4%BA%9B%E9%83%A8%E5%88%86%E7%9A%84%E8%BF%87%E7%A8%8B%EF%BC%8C%E4%BB%A5%E4%BE%BF%E6%9B%B4%E5%A5%BD%E5%9C%B0%E7%90%86%E8%A7%A3%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%8C%E4%B8%BA%E3%80%82">消融实验</a>，使用深度图代替了元通道，取得了较差的结果。</p>
<p>中央视觉图、元通道、深度图：<img src="https://img-blog.csdnimg.cn/ae4a919d4c164d27b71ae77dd83e9f50.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" width="50%" align="center"></p>
<p>&emsp; &emsp;关于元通道的个数和是否必须存在的问题，本文解释道，当图片数目（角度分辨率）和图片大小（空间分辨率）增加的时候，单个元通道可能无法编码全部信息，这时候我们可以增加元通道的数目。至于和视觉图分离的原因，主要是考虑到光场的重建过程中对于编辑的视觉通道，我们需要原光场的结构，如果将元通道编码到视觉通道之中形成 $I^z$，那么编辑的过程就会破坏结构，光场不再能够重建。</p>
<p>&emsp; &emsp;事实上，如果我们不考虑光场的重建问题，我们是可以将视觉通道和元通道合二为一的，此时我们只考虑数据压缩和光场生成的任务，该方法仍旧是一个很好的想法。这也将作为我的毕业设计的主要思想。</p>
</li>
<li><h4 id="编辑敏感的光场重建（解码器网络实现）"><a href="#编辑敏感的光场重建（解码器网络实现）" class="headerlink" title="编辑敏感的光场重建（解码器网络实现）"></a>编辑敏感的光场重建（解码器网络实现）</h4><p>&emsp; &emsp;本文将解码器的解释信息的方式分为两类：视图间视差图，以及其他信息（遮挡图和<a href="https://www.cnblogs.com/qingsunny/archive/2013/03/07/2947572.html#:~:text=%E5%AF%B9%E9%9D%9E%E6%9C%97%E4%BC%AF%E4%BD%93%E8%80%8C%E8%A8%80%EF%BC%8C%E5%AE%83%E5%AF%B9%E5%A4%AA%E9%98%B3%E7%9F%AD%E6%B3%A2%E8%BE%90%E5%B0%84%E7%9A%84%E5%8F%8D%E5%B0%84%E3%80%81%E6%95%A3%E5%B0%84%E8%83%BD%E5%8A%9B%E4%B8%8D%E4%BB%85%E9%9A%8F%E6%B3%A2%E9%95%BF%E8%80%8C%E5%8F%98%EF%BC%8C%E5%90%8C%E6%97%B6%E4%BA%A6%E9%9A%8F%E7%A9%BA%E9%97%B4%E6%96%B9%E5%90%91%E8%80%8C%E5%8F%98%E3%80%82%20%E6%89%80%E8%B0%93%E5%9C%B0%E7%89%A9%E7%9A%84%E6%B3%A2%E8%B0%B1%E7%89%B9%E5%BE%81%E6%98%AF%E6%8C%87%E8%AF%A5%E5%9C%B0%E7%89%A9%E5%AF%B9%E5%A4%AA%E9%98%B3%E8%BE%90%E5%B0%84%E7%9A%84%E5%8F%8D%E5%B0%84%E3%80%81%E6%95%A3%E5%B0%84%E8%83%BD%E5%8A%9B%E9%9A%8F%E6%B3%A2%E9%95%BF%E8%80%8C%E5%8F%98%E7%9A%84%E8%A7%84%E5%BE%8B%E3%80%82%20%E5%9C%B0%E7%89%A9%E6%B3%A2%E8%B0%B1%E7%89%B9%E5%BE%81%E4%B8%8E%E5%9C%B0%E7%89%A9%E7%9A%84%E7%BB%84%E6%88%90%E6%88%90%E4%BB%BD%EF%BC%8C%E7%89%A9%E4%BD%93%E5%86%85%E9%83%A8%E7%9A%84%E7%BB%93%E6%9E%84%E5%85%B3%E7%B3%BB%E5%AF%86%E5%88%87%EF%BC%8C%E9%80%9A%E4%BF%97%E8%AE%B2%E5%9C%B0%E7%89%A9%E6%B3%A2%E8%B0%B1%E7%89%B9%E5%BE%81%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%9C%B0%E7%89%A9%E7%9A%84%E9%A2%9C%E8%89%B2%E7%89%B9%E5%BE%81%E3%80%82%20%E8%80%8C%E5%9C%B0%E7%89%A9%E7%9A%84%E6%96%B9%E5%90%91%E7%89%B9%E5%BE%81%E6%98%AF%E7%94%A8%E6%9D%A5%E6%8F%8F%E8%BF%B0%E5%9C%B0%E7%89%A9%E5%AF%B9%E5%A4%AA%E9%98%B3%E8%BE%90%E5%B0%84%E5%8F%8D%E5%B0%84%E3%80%81%E6%95%A3%E5%B0%84%E8%83%BD%E5%8A%9B%E5%9C%A8%E6%96%B9%E5%90%91%E7%A9%BA%E9%97%B4%E5%8F%98%E5%8C%96%E7%9A%84%EF%BC%8C%E8%BF%99%E7%A7%8D%E7%A9%BA%E9%97%B4%E5%8F%98%E5%8C%96%E7%89%B9%E5%BE%81%E4%B8%BB%E8%A6%81%E5%86%B3%E5%AE%9A%E4%BA%8E%E4%B8%A4%E7%A7%8D%E5%9B%A0%E7%B4%A0%EF%BC%8C%E5%85%B6%E4%B8%80%E6%98%AF%E7%89%A9%E4%BD%93%E7%9A%84%E8%A1%A8%E9%9D%A2%E7%B2%97%E7%B3%99%E5%BA%A6%EF%BC%8C%E5%AE%83%E4%B8%8D%E4%BB%85%E5%8F%96%E5%86%B3%E4%BA%8E%E8%A1%A8%E9%9D%A2%E5%B9%B3%E5%9D%87%E7%B2%97%E7%B3%99%E9%AB%98%E5%BA%A6%E5%80%BC%E4%B8%8E%E7%94%B5%E7%A3%81%E6%B3%A2%E6%B3%A2%E9%95%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E6%AF%94%E4%BE%8B%E5%85%B3%E7%B3%BB%EF%BC%8C%E8%80%8C%E4%B8%94%E8%BF%98%E4%B8%8E%E8%A7%86%E8%A7%92%E5%85%B3%E7%B3%BB%E5%AF%86%E5%88%87%E3%80%82%20%E8%AE%BE%E6%B3%A2%E9%95%BF%E4%B8%BA%CE%BB%EF%BC%8C%E7%A9%BA%E9%97%B4%E5%85%B7%E6%9C%89%CE%B4%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%E7%9A%84%E5%85%A5%E5%B0%84%E8%BE%90%E5%B0%84%EF%BC%8C%E4%BB%8E%20%28%CE%B80%EF%BC%8C%CF%860%29%20%E6%96%B9%E5%90%91%EF%BC%8C%E4%BB%A5%E8%BE%90%E5%B0%84%E4%BA%AE%E5%BA%A6L0,%28%CE%B80%EF%BC%8C%CF%860%EF%BC%8C%CE%BB%29%E6%8A%95%E5%B0%84%E5%90%91%E7%82%B9%E7%9B%AE%E6%A0%87%EF%BC%8C%E9%80%A0%E6%88%90%E8%AF%A5%E7%82%B9%E7%9B%AE%E6%A0%87%E7%9A%84%E8%BE%90%E7%85%A7%E5%BA%A6%E5%A2%9E%E9%87%8F%E4%B8%BAdE%20%28%CE%B80%EF%BC%8C%CF%860%EF%BC%8C%CE%BB%29%20%3D%20L0%20%28%CE%B80%EF%BC%8C%CF%860%EF%BC%8C%CE%BB%29%20cos%CE%B80%20d%CE%A9%E3%80%82">非朗博效应</a>）</p>
<p> 本文设计的解码器网络：<img src="https://img-blog.csdnimg.cn/e0504d3786df4b37ad59995d8c678ae5.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" align="center" width="70%"></p>
<p>&emsp; &emsp;在图中我们可以看到，解码器由三个不同的网络共同构成。$SepNet, DispNet, FusionNet$ 三个网络各自有着对应的任务。同时也有着一些方法，如 $warping$，以及涉及到 $Occlusion _ map, Disparity _ map,Feature_map$<code>遮挡图是通过特征图得到的么？是的话为什么可以直接得到，是物理方法么？</code>等的获取，接下来我们将一一介绍。</p>
</li>
</ol>
<ul>
<li><p>$SepNet$：该网络的作用是特征分离，具体地说，这个模块将元通道 $Z$ 分解为一系列特征映射 $\{F_i\}$，其中每个 $F_i$ 包含视图 $i$ 的特征信息，和视觉通道独立，可以和视觉通道结合重建视图 $i$。</p>
</li>
<li><p>$DispNet$：该网络的作用是视差恢复。具体来说，从特征图 $F_i$ 中提取出 $I_i$ 视差图 $D(u_i)$。接着我们如果给出某一个光场视图$L(u_j)$，我们可以得到合成视图（即warped方法）$\bar{L}(u_i)$：</p>
<script type="math/tex; mode=display">
\bar{L}(u_i,x)=L(u_j, x+(u_j-u_i)\times D(u_i,x))</script><p>&emsp; &emsp;因此，我们可以明确认识到，对于任何一个视差图$D(u_i)$，我们将其与$I_c$进行合成得到 $\bar L(u_i)$。此时我们得到的图理论上和$\widehat L(u_i)$一致（其中$\widehat L=L$，作为重建光场的目标），但实际上由于遮挡和非朗博效应总会出现不同。为了解决这个问题，我们需要另一个网络修复他，但是网络的执行时间可以由 $D(u_i)$ 的遮挡情况自适应地确定，为了度量遮挡程度，我们记 </p>
<script type="math/tex; mode=display">
O(u_i, x) = ||D(u_i,x)-D(u_c,x+(u_c-u_i)\times D(u_i,x))||_1</script><p>&emsp;&emsp;其中$D(u_c)$ 表示 $I_c$ 的视差图。并且有 $O(u_i,x)$越大，接下来的网络需要的时间就越长。</p>
</li>
<li><p>$FusionNet$：该网络的目的是重建遮挡细节和非朗博效应，这个网络通过大规模的训练达到将视觉图和遮挡图变回光场图像的目的。大规模的训练遇到的关键问题是数据问题，为了解决这个问题，本文提出了一个算子 $G$，算子 $G$ 通过平均采样的方式更改光场，对于原本的IO组$\{\widetilde I_c,\widehat L\}$，我们更改为$\{G( I_c),G(L)\}$，从而得到大规模数据。</p>
</li>
</ul>
<ol>
<li><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>&emsp; &emsp;网络通过最小化损失函数联合训练编码和解码子网络，本文的损失函数分为三个部分，扭曲一致性损失（也就是$warped$模块） $\zeta_W$，视差正则损失 $\zeta_D$ ，重建损失 $\zeta_R$。故而总损失为：</p>
<script type="math/tex; mode=display">
\zeta = \omega_1\zeta_W + \omega_2\zeta_D + \omega_3\zeta_R</script><p>&emsp; &emsp;在这里，我们设定$\omega_1 = 0.5,\omega_2 = 0.01, \omega_3 = 1.0$。接下来我们将逐一介绍三个损失。</p>
<p>&emsp; &emsp;$\zeta_W$：扭曲一致性损失。该损失用来度量$warped$之后的光场$\bar L$ 和中央视觉通道 $I_c$之间的差别，使得视差能够更好地被还原。公式写作：</p>
<script type="math/tex; mode=display">
\zeta_W = E_{L_i \in S}\{||\bar L_i - L_i||_1\}</script><p>&emsp;&emsp;<code>warp是一个物理方法，这个方法由于不考虑遮挡理论上本身就和预期结果不同，如果使用这种损失有没有可能会让视差的生成网络过拟合呢？（也就是视差的生成网络不去生成视差，而是某种能和中央通道直接合成光场的图片【这并不是值得开心的，因为之后还要和遮挡图进行进一步的融合】）</code></p>
<p>&emsp; &emsp;其中，$S$ 即为光场构成的数据集，这个损失即为所有光场的中央视觉通道与对应的$\bar L_i$的一阶范数。该损失限制了从$DispNet$中生成的视差图，使其更加接近于真实视差。</p>
<p>&emsp; &emsp;但是仅仅由这个损失进行限制仍然是不够的，由于这个损失并不能度量到没有纹理的区域的扭曲视图是否相近，这种失误将会造成遮挡部分无法进行一致性地传播，将会加大 $FusionNet$ 的训练量。为了解决这个问题，我们引入正则损失 $\zeta_D$，加强相邻视图的视差一致性。</p>
<script type="math/tex; mode=display">
\zeta_D = E_{L_i \in S}\{||D(u_i,x)-D(u_i-1,x+D(u_i,x))||_1\}</script><p>&emsp;&emsp;这个损失函数实际上限制了在预测后的光场中，相邻视图需要尽可能地一致，保证相邻视图之间的视差一致性。每个视图在某种程度上都会受到其他视图的影响和制约。</p>
<p>&emsp; &emsp;在最后，我们通过重建的光场和原本的光场相一致的限制构造重建损耗 $\zeta_R$：</p>
<script type="math/tex; mode=display">
\zeta_R = E_{L_i\in S}\{\alpha||\widetilde L_i-\widehat L_i||_1+\beta||SSIM(\widetilde L_i, \widehat L_i)||_1\}</script><p>&emsp;&emsp;实验中，$SSIM(L_1,L_2)$表示两个光场的所有视角的平均$SSIM$值，<a href="https://baike.baidu.com/item/SSIM/2091025">计算见此</a>。并且预设$\alpha=1.0,\beta=0.02$。选取$SSIM$度量将在模糊区域获得更精确的细节。</p>
</li>
</ol>
<h3 id="实现的细节"><a href="#实现的细节" class="headerlink" title="实现的细节"></a>实现的细节</h3><ol>
<li><p>数据集<br>&emsp; &emsp;本文的数据集来自两个公开数据集，斯坦福和MIT的相机光场数据，总共收集了406个光场，每个光场有14*14个角度和376*541个像素。在训练的过程中，随机选取326个作为训练集，剩余80个作为测试集，对每个光场裁取7*7个角度以及每个角度128*128个像素组成的照片。对训练集进行了数据增强，即水平翻转或者旋转。通过数据增强，最终得到14447个光场样本进行训练。</p>
</li>
<li><p>训练<br>&emsp; &emsp;训练时编码子网和解码子网是联合训练的，但是这种训练方式较慢，本文提出将这个训练分为两个部分。首先是进行除了 $FusionNet$ 之外的网络进行训练，这个阶段迭代10000次，这个训练的目的是加速以及预热$DispNet$ 。在第二阶段，包括 $FusionNet$ 在内 的所有网络进行 50000 次训练。训练中使用Adam优化器，第一阶段学习率为0.0002，第二阶段学习率下降为第一阶段学习率的1%。</p>
<p>&emsp; &emsp;另外，本文采用pytorch实现了方法，这个实验在两台NVDIA GeForce GTX 1080 Ti 上运行了48个小时。完成的模型对于7*7的光场输入编码元通道约需要0.04秒，重建光场约需要2秒。</p>
<p>&emsp; &emsp;源代码和数据尚未开源，等待本文正式发布将会开源代码和数据。</p>
</li>
</ol>
<h3 id="实验的结果"><a href="#实验的结果" class="headerlink" title="实验的结果"></a>实验的结果</h3><p>&emsp; &emsp;我们可以看到，无论是颜色编辑、颜色转换、阴影去除、风格转换、对比增强等针对2D图片的编辑方法，在本模型的支持下均完成了对于重建光场的一致性传播。<br> <img src="https://img-blog.csdnimg.cn/f7655d1624894b87a117e25801a7b14d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" width="110%" align="center"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>Anchor_based_colorization</title>
    <url>/Anchor_based_colorization/</url>
    <content><![CDATA[<h2><center> Disentangled Image Colorization via Global Anchors </center></h2>

<p>【图像上色】【TOG2022】【<a href="https://dl.acm.org/doi/10.1145/3550454.3555432">paper</a>】【<a href="https://github.com/MenghanXia/DisentangledColorization">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;对于图像上色问题，其具备两个特性：1. 任意像素点的色彩值具备不确定性，2.逻辑上属于同一个物体的像素点在上色之后应当具有一致性。归结来看，第一个特性即为色彩分布，需要预测一个具体切合理的色彩分布概率，这个问题可以转化为颜色分类问题，第二个特性即为空间一致性，需要一个可以用于区分不同位置色彩是否一致的数据，这个问题可以转化为聚类问题。本文从上述两个特性入手，一方面使用SPixNet等方法对图像进行分割，并将分割的子“超像素”进行聚类从而解决空间一致性，另一方面在同类的“超像素”上应用分类器预测出的色彩值。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Anchor_based_colorization/image-20230211202429399.png" alt="image-20230211202429399"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>解离了颜色一致性和不确定性的特征</li>
<li>使用像素分割+聚类的方式解决色彩一致性问题</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/Anchor_based_colorization/image-20230212213822100.png" alt="image-20230212213822100"></p>
<p>&emsp;&emsp;总结性质地讲，上图中各个符号/网络都有其含义</p>
<ul>
<li>$L$，输入的灰度图 $(N,C,H,W)$</li>
<li>SPixNet，用于从 $L$ 中提取基于像素属于周边超像素的概率信息 $A (N,9,H,W)$，表示每个像素位于哪个相邻的超像素</li>
<li>Backbone，用于从 $L$ 中提取基于像素的特征信息 $F (N,64,H,W)$，表示基于像素的特征图</li>
<li>SP-Pooling，用于使用像素到超像素的关联图 $A$ 池化基于像素的特征信息 $F$，获得基于超像素的特征信息 $F_s (N,64,H’,W’)$<ul>
<li>同时获取的还有基于超像素的色彩信息 $C_s (N,2,H’,W’)$，基于超像素的位置信息 $L_s (N,64,H’,W’)$</li>
<li>这里的 $H’, W’ = H/16, W/16$，具体和池化尺寸有关</li>
</ul>
</li>
<li>Probabilitic Color Modeler，用于从基于超像素的信息 $F_s,C_s,L_s$ 中估计超像素的颜色预测值 $P_s (N,313,H’,W’)$，313 指色彩分类，该值回传用以计算超像素色彩和 GT 的 loss</li>
<li>Anchor Locator，用于从基于超像素的信息 $F_s,C_s,L_s$ 中估计超像素的聚类信息，生成掩码 $M_s (N,1,H’,W’)$，第二维表示属于哪一个聚类</li>
<li>Color Generator，使用基于超像素的色彩概率预测以及基于超像素的聚类信息，生成空间一致，色彩确定的最终超像素色彩特征 $F_s’$ $(N,64,H’,W’)$，该特征直接通过 MLP 映射至 $(N,2,H’,W’)$ 作为超像素的色彩回传，用以计算 loss</li>
<li>SP-Diffusion，使用基于像素属于周边超像素的概率信息 A 引导关于超像素的特征 $F_s’$ 进行向像素的扩散，生成含有上述所有信息的、基于像素的特征 $F’ (N,2,H,W)$，该结果可以作为最终输出</li>
<li>RefineNet，对 $F’$ 进行进一步的色彩增强，增强的监督方式为使 VGGLoss 更小，从图像上看可以解决无聚类中心导致的无色彩 / 多聚类中心导致的色彩不一致问题</li>
</ul>
<h4 id="前向"><a href="#前向" class="headerlink" title="前向"></a>前向</h4><p>&emsp;&emsp;首先介绍<strong>超像素</strong>，其为比一般像素更大的“软像素”，通常初始化时将各个<strong>超像素</strong>均匀地分配为方格，接着对于某个<strong>像素</strong>位置，其可能位于其本身所在的<strong>超像素</strong>或者周围八个<strong>超像素</strong>之中，因此对于一个<strong>像素</strong> $p$，其有可能位于 9 个<strong>超像素</strong> $s$ 中。通过不停地迭代细致化<strong>超像素</strong>的边界，即可得到<strong>超像素</strong>分割图。</p>
<p><img src="/Anchor_based_colorization/image-20230213170955533.png" alt="image-20230213170955533" style="zoom: 33%;"></p>
<ul>
<li>在训练阶段，输入包括灰度图 $L$ 和色彩图 $C$，首先经过 SPixNet 和 backbone 分别得到 $A,\ F$，这两个特征分别表示超像素图和像素特征图<ul>
<li>所谓的 backbone 其实就是基于 CNN 的特征提取器， $A\to(N,9,H,W)$，$F\to(N,64,H,W)$</li>
</ul>
</li>
<li>使用 $A$ 引导 $F$ 和 $C$ 进行池化得到含有超像素信息的特征 $F_s$，具体来说，使用属于不同相对位置的超像素的概率对特征进行加权，等价于多次 $Avgpool(Concat(F,C) \times A[i])$<ul>
<li>$F_s$ 对应 $(F,C)$，包含两个部分，超像素特征，超像素色彩，这两部分特征均是基于超像素的。</li>
<li>超像素的数目由分割方式决定，本文采用均匀分割成多个大小为 $S\times S$ 的超像素，$S=16$，因此 $F_s\to(N,64+2,\frac H S,\frac W S)$</li>
<li>同时在这一步还会产生一个对超像素特征进行位置编码的位置向量 $F_p\to(N,64,H’,W’)$</li>
<li>最终记录的也就是超像素的特征向量和位置向量，其数学表示如下</li>
</ul>
</li>
</ul>
<p><img src="/Anchor_based_colorization/image-20230214085902061.png" alt="image-20230214085902061" style="zoom:25%;"></p>
<ul>
<li>使用基于超像素的特征用来进行基于超像素的颜色概率预测（Probabilitic Color Modeler），具体来说<ul>
<li>拉平超像素特征和超像素位置获得 $F_{fs},\ F_{fp}\to(H’W’,N,64)$</li>
<li>基于 $A$ 获得了一个针对每个超像素的 mask，拉平后得到 $M_{fs}\to(N,H’W’)$</li>
<li>输入一个 6 层的 transformer，$q=k=F_{fs}+F_{fp},\ v=F_{fs},\ mask=M_{fs}$</li>
<li>上一步得到的输出再经过一个投射层即为图中 $P_s\to(N,313,H’,W’)$，其表示了每个超像素的色彩的分类概率</li>
</ul>
</li>
<li>使用超像素色彩进行聚类（Anchor Locator），这是一个无参的过程。聚类的结果是一个 mask $(N,1,H’,W’)$，其表示了某个超像素属于哪一个类别，拉平之后为  $M_s\to(H’W’,N,1)$</li>
<li>使用基于超像素的色彩分类预测和基于超像素的特征进行超像素色彩生成（Color Generator），具体来说<ul>
<li>将超像素的色彩进行色彩映射（2维ab色彩空间映射至313维分类色彩）之后进行 one hot 编码，得到一个 313 维的向量，该向量本质上是由 $C_{fs}$ 生成的，因此仍由 $C_{fs}$ 代替</li>
<li>输入一个 6 层的 transformer，$q,k,v$ 均来自于 $Embedding(F_{fs}+M_s\odot F_{fp})$ 的变形</li>
<li>输出的值记为 $F_s’\to(N,313,H’,W’)$，其表示了超像素最终的色彩预测分类</li>
<li>使用 $A$ 引导 $F_s’$ 进行扩散（逆池化）可以还原出基于<strong>像素</strong>的特征和位置信息，即使用超像素的特征向量和位置向量（$v_s$ 和 $l_s$）可以还原像素的对应特征 $F’\to(N,313,H,W)$：</li>
</ul>
</li>
</ul>
<p><img src="/Anchor_based_colorization/image-20230214090133667.png" alt="image-20230214090133667" style="zoom:25%;"></p>
<ul>
<li>最终使用 RefineNet（是一个由 CNN 构成的网络）进行微调，并将 313 的色彩分类转化为 ab 空间的色彩特征，即可以输出色彩图 $C_{pred}$</li>
</ul>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>&emsp;&emsp;本文采用<strong>两阶段</strong>的训练，首先训练SPixNet，这个网络来自已有的工作SPixelCNN。其中与原文不同的是，原文使用 $(c)$ 预测 $(b)$ 然后使用新的 $(c)$ 作为输入，而本文则使用 $(c)$ 预测 $(b)$ 之后使用新的 $(a)$ 作为输入，其原理是图像的灰度图和RGB图具备基本一致的亮度信息，因此使用色彩图训练得到的超像素分割网络也可以分割灰度图。第一阶段的损失函数遵循 SPixelCNN 的设置，目标是：1. 使超像素还原出的像素的色彩信息和像素的 GT 色彩信息尽可能一致，2. 使每个超像素包含的像素位置信息尽可能紧凑</p>
<p><img src="/Anchor_based_colorization/image-20230214092048596.png" alt="image-20230214092048596" style="zoom:25%;"></p>
<p>&emsp;&emsp;第一阶段训练完成之后接着训练第二阶段，第二阶段的目标是优化剩余的所有含参网络，其目标主要包含三个：</p>
<ul>
<li>对每一个超像素，将 $P_s$ 和基于真实像素做池化后的 313 色彩分类类别计算交叉熵损失，此时由于超像素分割网络可以很好地进行池化和扩散，因此这里可以同步优化 backbone，Probabilitic Color Modeler</li>
</ul>
<p><img src="/Anchor_based_colorization/image-20230214093507049.png" alt="image-20230214093507049" style="zoom:25%;"></p>
<ul>
<li>对每一个超像素，将 $F_s’$ 和基于真实像素做池化后的 313 色彩分类类别计算交叉熵损失，可以优化 backbone，Probabilitic Color Modeler，Color Generator</li>
<li>计算生成的 ab 空间图像和 GT 图像计算感知损失，可以优化 backbone，Probabilitic Color Modeler，Color Generator，RefineNet</li>
</ul>
<p><img src="/Anchor_based_colorization/image-20230214093919050.png" alt="image-20230214093919050" style="zoom:25%;"></p>
<p>&emsp;&emsp;第二阶段最终的损失函数写为：</p>
<script type="math/tex; mode=display">
\mathcal L = \mathcal L_{dist}+\lambda\mathcal L_{color},\ \lambda=1.0</script><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;效果图如下：</p>
<p><img src="/Anchor_based_colorization/image-20230214094622907.png" alt="image-20230214094622907" style="zoom:80%;"></p>
<p>&emsp;&emsp;量化指标上，由于彩色化的目的是生成视觉上看似合理的颜色，而不是恢复 GT，那些基于 GT 的保真度度量(如PSRN、SSIM、LPIPS)不适用于评估，相反，与视觉感知相关的度量(如FID、IS 和 Colorfulness)更能反映所需的彩色化质量。</p>
<p><img src="/Anchor_based_colorization/image-20230214103937701.png" alt="image-20230214103937701"></p>
<p>&emsp;&emsp;消融实验上主要做了锚点的个数对超像素聚类的影响探索</p>
<p><img src="/Anchor_based_colorization/image-20230214105919533.png" alt="image-20230214105919533" style="zoom: 50%;"></p>
<p>&emsp;&emsp;同时在是否存在锚点分支、是否进行软像素分割、是否进行基于像素级别的增强、是否进行特征聚类四个方面分别进行了消融实验。</p>
<p><img src="/Anchor_based_colorization/image-20230214105953306.png" alt="image-20230214105953306" style="zoom: 67%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>Arbitrary_Resolution_rPPG</title>
    <url>/Arbitrary_Resolution_rPPG/</url>
    <content><![CDATA[<h2><center> Learning Motion-Robust Remote Photoplethysmography
through Arbitrary Resolution Videos </center></h2>

<p>【心率监测】【AAAI2023】【<a href="http://arxiv.org/abs/2211.16922">paper</a>】【<a href>code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文针对人脸在运动过程中产生的远近问题提出了两个即插即用的模块，将远近移动的问题转化为了多分辨率下的 rppg 信号估计问题，同时使用光流解决了人脸在转动的过程中可能导致的关键点缺失问题，在选择的 baseline（physnet）上取得了一定的进步。主要包括：1. 编码了分辨率信息从而对分辨率鲁棒的 PFE 模块，2. 使用光流恢复人脸运动从而对运动鲁棒的 TFA 模块。效果确实在对比的情况下有所进步，但是确实也缺乏一个 SOTA 的结果。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Arbitrary_Resolution_rPPG/image-20221206145520889.png" alt="image-20221206145520889" style="zoom:67%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>提出了可以帮助从可变分辨率视频中估计 rppg 信号的两个即插即用的块</li>
<li>其中 PFE 模块能够编码不同帧的分辨率信息，并且使用多分辨率的分支交叉训练</li>
<li>其中 TFA 模块能够从正负两个方向的光流恢复人脸运动，使输出包含正面人脸信息特征</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><h4 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h4><p><img src="/Arbitrary_Resolution_rPPG/image-20221206143650642.png" alt="image-20221206143650642" style="zoom: 67%;"></p>
<p>&emsp;&emsp;对于输入的视频，将其记为 $X$，其中每个视频有 $T$ 帧，将单个帧记为 $x_t$，需要注意的是，由于网络允许不同分辨率的视频输入，因此 $x_t \in \mathbb R^{3\times h_t\times w_t}$，也就是每个帧的分辨率之间是不同的。</p>
<p>&emsp;&emsp;对于 $X$，其需要通过两个不同的分支，分别是 PFE 和 TFA，而对于 PFE 分支，同样需要 sample 出两个不同的分辨率 $X_1,\ X_2$，经过整个网络之后再计算交叉损失。</p>
<ul>
<li>对于某个分辨率下的 PFE 分支，其输入为 $X\in \mathbb R^{3\times T\times h\times w}$（$X=X_1/X_2$），首先需要通过一个 conv 模块，将 $X$ 中的每一帧 $x_t$ 进行一个降采样得到 $x_{ar}\in \mathbb R^{C\times \frac h 2\times\frac w 2}$，在论文里选择的 $C=16$，接着将整个视频的 $x_{ar}$ 帧输入 PFE 模块，得到输出记为 $x_{st}\in\mathbb R^{C\times H\times W}$</li>
<li>对于 TFA 分支，其输入和 PFE 分支完全一致，TFA 模块使用双向光流恢复图像转动并且提取正向光流与反向光流特征，接着将正反向对应位置的光流特征进行融合，最终得到 $x_{mo}\in\mathbb R^{C\times H\times W}$</li>
<li>接着将两个分支得到的结果进行按位相加得到 $x_{st-mo}$，将其输入 backbone 并得到最终的 rppg 信号</li>
</ul>
<p>&emsp;&emsp;需要注意的是在正向的过程中，每一个 video 被确定为两个分辨率，分别进入两个同样的 PFE 和 TFA 模块，并且得到两个分辨率下的 rppg 信号估计计算损失从而进一步加强分辨率鲁棒性，其示意图如下</p>
<p><img src="/Arbitrary_Resolution_rPPG/image-20221206220935265.png" alt="image-20221206220935265" style="zoom: 55%;"></p>
<p>&emsp;&emsp;其中，两个不同分辨率的视频从同一个视频中采样得到，两个视频经过不同的 PFE 模块（其中的参数仅包含一个 MLP 层）以及共享参数的 TFA 模块（其中的参数主要包含 15 个 ResBlock 层），在上图中需要注意的是实际上经过 TFA 模块得到的 $X_{mo}$ 是两个视频共享的，只使用了一个数据得到 $X_{mo}$，可以理解为下面的大分辨率视频压根没进 TFA 模块。</p>
<p>&emsp;&emsp;对于得到的两个 rppg 信号，记其为 $Y_1,\ Y_2$，则和 GT 做一致化的损失包括 $\cal L_{time},\ L_{fre}$，这两个 loss 都是只有 $Y_1$ 和 GT 做的。而为了保证 $Y_1,\ Y_2$ 的一致性，则采用 L1Loss 限制一致性 $\mathcal L_{crc}=||Y_1-Y_2||_1$，总损失描述为：</p>
<script type="math/tex; mode=display">
\mathcal L =\mathcal L_{time}+ \mathcal L_{fre}+\alpha\cdot\mathcal L_{crc}</script><p>&emsp;&emsp;其中，时间一致性损失由负皮尔森相关性度量，频率一致性用 $CE(PSD(Y),HR_{GT})$ 度量，$\alpha$ 用于平衡多方因素，论文中设置为 0.1。</p>
<h4 id="PFE-模块"><a href="#PFE-模块" class="headerlink" title="PFE 模块"></a>PFE 模块</h4><p><img src="/Arbitrary_Resolution_rPPG/image-20221206152253971.png" alt="image-20221206152253971" style="zoom: 50%;"></p>
<p>&emsp;&emsp;对于 PFE 模块，其同样具备两条道路（注意这里是逐帧的操作）：</p>
<ul>
<li>其一用于上采样的固定分辨率，同时聚合周边像素信息<ul>
<li>首先将 $x_{ar}$ 进行上采样（直接插值）得到 $x_{cr}\in\mathbb R^{C\times H\times W}$，其中 $H,\ W$ 是固定值，与 $h,\ w$ 无关，而 $X_1,\ X_2$ 的帧之间的区别是 $h,\ w $ 不同</li>
<li>然后聚合 $x_{cr}$  的信息，具体的做法为：将每个像素位置开始向右 / 下各 $n$ 个像素点总共 $n^2$ 的像素点进行 channel 的聚合，得到 $\hat x_{cr}\in\mathbb R^{n^2C\times H\times W}$（注意上图有纰漏，应当是 $n^2$），其数学表述如下：</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
\hat x_{cr}=Concat(\{x_{cr}(i+n,j+n) \}_{n\in Neighbor})</script><ul>
<li>其二用于编码原始帧 $x_{ar}$ 的分辨率信息，也就是记录 $\sigma=\frac h H$，然后 repeat 到和 $x_{cr}$ 一样的形状，同时记录 $H,\ W$ 的信息，最终得到 $x_{size}\in\mathbb R^{2\times H\times W}$，其数学表达式如下：</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
&x_{szie}(i,j)=[\sigma_H,\ \sigma_W]\\& where\ \ \  \sigma_H=\frac h H,\ \sigma_W=\frac w W
\end{align}</script><ul>
<li>接着将两个道路得到的数据进行拼接、拉平、MLP、reshape之后得到输出 $x_{st}\in\mathbb R^{C\times H\times W}$，其数学表达式如下，同时整个视频经过 PFE 模块之后的输出记为 $X_{st}\in\mathbb R^{T\times C\times H\times W}$</li>
</ul>
<script type="math/tex; mode=display">
x_{st}=Reshape(MLP(Flatten(Concat(\hat x_{cr},\ x_{size}))))</script><h4 id="TFA-模块"><a href="#TFA-模块" class="headerlink" title="TFA 模块"></a>TFA 模块</h4><p><img src="/Arbitrary_Resolution_rPPG/image-20221206222451103.png" alt="image-20221206222451103" style="zoom:67%;"></p>
<p>&emsp;&emsp;设计上 TFA 的设计主要为了解决这些在提取 ROI 区域时会遇到的问题：</p>
<ul>
<li>landmark 位置：由于头部的运动，人脸标记的位置可能会发生快速剧烈变化，这导致 ROI 的不准确</li>
<li>插值：ROI 的形状可能不同，通常使用插值来保持一致性。然而，插值可能会破坏像素的颜色变化，消除rppg 信息</li>
<li>landmark  消失：当头部运动遇到大角度旋转时，部分面部可能从画面中消失。这时 landmark  将随机标记一些区域</li>
</ul>
<p>&emsp;&emsp;结构上 TFA 模块较为复杂，由于其每一帧的输入都包含了前向 / 反向送入的两个输出，因此看上去比较混乱，可以将目光集中于某一个蓝色 / 红色块内：</p>
<ul>
<li>每个块 $i$ 输入包含两个部分：1. 前一帧的视频图像（用于计算光流），2. 前一帧提取出的运动特征（用于恢复运动）</li>
<li>对于输入的视频帧 $x_{i-1}$ 和特征 $h_{i-1}$， 首先利用 $x_{i-1}$ 和 $x_i$ 计算出光流 $s_i$，使用的方法为已有的 SPyNet</li>
</ul>
<script type="math/tex; mode=display">
s_i=Optical(x_i,x_{i-1})</script><ul>
<li>将计算出的光流和之前提取出的运动特征 $h_{i-1}$ 进行扭曲，具体方法不含参数，代码入下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">flow_warp</span>(<span class="params">x, flow, interp_mode=<span class="string">&#x27;bilinear&#x27;</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>, align_corners=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Warp an image or feature map with optical flow.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x (Tensor): Tensor with size (n, c, h, w).</span></span><br><span class="line"><span class="string">        flow (Tensor): Tensor with size (n, h, w, 2), normal value.</span></span><br><span class="line"><span class="string">        interp_mode (str): &#x27;nearest&#x27; or &#x27;bilinear&#x27;. Default: &#x27;bilinear&#x27;.</span></span><br><span class="line"><span class="string">        padding_mode (str): &#x27;zeros&#x27; or &#x27;border&#x27; or &#x27;reflection&#x27;.</span></span><br><span class="line"><span class="string">            Default: &#x27;zeros&#x27;.</span></span><br><span class="line"><span class="string">        align_corners (bool): Before pytorch 1.3, the default value is</span></span><br><span class="line"><span class="string">            align_corners=True. After pytorch 1.3, the default value is</span></span><br><span class="line"><span class="string">            align_corners=False. Here, we use the True as default.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Tensor: Warped image or feature map.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> x.size()[-<span class="number">2</span>:] == flow.size()[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">    _, _, h, w = x.size()</span><br><span class="line">    <span class="comment"># create mesh grid</span></span><br><span class="line">    grid_y, grid_x = torch.meshgrid(torch.arange(<span class="number">0</span>, h).type_as(x), torch.arange(<span class="number">0</span>, w).type_as(x))</span><br><span class="line">    grid = torch.stack((grid_x, grid_y), <span class="number">2</span>).<span class="built_in">float</span>()  <span class="comment"># W(x), H(y), 2</span></span><br><span class="line">    grid.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    vgrid = grid + flow</span><br><span class="line">    <span class="comment"># scale grid to [-1,1]</span></span><br><span class="line">    vgrid_x = <span class="number">2.0</span> * vgrid[:, :, :, <span class="number">0</span>] / <span class="built_in">max</span>(w - <span class="number">1</span>, <span class="number">1</span>) - <span class="number">1.0</span></span><br><span class="line">    vgrid_y = <span class="number">2.0</span> * vgrid[:, :, :, <span class="number">1</span>] / <span class="built_in">max</span>(h - <span class="number">1</span>, <span class="number">1</span>) - <span class="number">1.0</span></span><br><span class="line">    vgrid_scaled = torch.stack((vgrid_x, vgrid_y), dim=<span class="number">3</span>)</span><br><span class="line">    output = F.grid_sample(x, vgrid_scaled, mode=interp_mode, padding_mode=padding_mode, align_corners=align_corners)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO, what if align_corners=False</span></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
\bar h_{i-1}=Warp(h_{i-1},s_i)</script><ul>
<li>接下来将得到的 $\bar h_{i-1}$ 和 $x_i$ 进行 cat 并通过一系列 ResBlock（15个）之后得到该帧提取出的特征 $h_i$ 并馈送到下一帧，需要注意的是 ResBlock 其实只包含了两层不含 BN 的卷积，只是加入了残差连接</li>
</ul>
<script type="math/tex; mode=display">
h_i = ResBlock(Concat(x_i,\  \bar h_{i-1}))</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBlockNoBN</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Residual block without BN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has a style of:</span></span><br><span class="line"><span class="string">        ---Conv-ReLU-Conv-+-</span></span><br><span class="line"><span class="string">         |________________|</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_feat (int): Channel number of intermediate features.</span></span><br><span class="line"><span class="string">            Default: 64.</span></span><br><span class="line"><span class="string">        res_scale (float): Residual scale. Default: 1.</span></span><br><span class="line"><span class="string">        pytorch_init (bool): If set to True, use pytorch default init,</span></span><br><span class="line"><span class="string">            otherwise, use default_init_weights. Default: False.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_feat=<span class="number">64</span>, res_scale=<span class="number">1</span>, pytorch_init=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ResidualBlockNoBN, self).__init__()</span><br><span class="line">        self.res_scale = res_scale</span><br><span class="line">        self.conv1 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pytorch_init:</span><br><span class="line">            default_init_weights([self.conv1, self.conv2], <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        out = self.conv2(self.relu(self.conv1(x)))</span><br><span class="line">        <span class="keyword">return</span> identity + out * self.res_scale</span><br></pre></td></tr></table></figure>
<ul>
<li>上述分析都是基于正向或者反向计算光流的结果，这些结果 $h_i$ 序列的 shape 都和 $x_i$ 一样大，其中在实现的过程中第一帧获取到的 $h_{0}$ 由全 0 矩阵初始化。这样得到的双流数据比 $X$ 大了一倍，因此接下来要按照时间维度降采样</li>
<li>方式为将每一帧对应的正向反向特征进行拼接，然后输入到一个 1x1 Conv 降采样到 $\frac 1 2$ channel（T)</li>
</ul>
<script type="math/tex; mode=display">
x_{mo}=F(Concat(h_i^b,\ h_i^f))</script><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;按照一贯的方式，应当测试数据集内有效性、跨数据集有效性、分辨率和运动鲁棒性以及消融实验，但是分辨率变化大的数据集几乎没有，因此有效性实验更多的是在自己构造的分辨率变化数据集上测试</p>
<p>&emsp;&emsp;对于数据集内测试，本文选取了 UBFC、PURE、COHFACE 三个数据集，PURE 运动信息更多，COHFACE 数据质量更差，UBFC 数据集内误差较大</p>
<p><img src="/Arbitrary_Resolution_rPPG/image-20221206225925297.png" alt="image-20221206225925297" style="zoom:50%;"></p>
<p>&emsp;&emsp;可以看出在不同的数据集上都可以降点，这里注意到两点：1. 选择的 SOTA 都比较差，现在的 UBFC 指标大概在 0.4 左右，2. 明明 PURE 更适合 TFA 模块，但是在 PURE 上降点反而不如 UBFC 明显。</p>
<p>&emsp;&emsp;针对这两点，原因为：1. 本文实现的方法在最好的 baseline 上也同样可以降点，但是效果对比度不如普通的 baseline 明显，2. 对于不同的数据集，由于其内在误差和难易程度，指标的下降的绝对值并不等同于提升的效果。</p>
<p>&emsp;&emsp;跨数据集测试上，由于测试方法过多且不统一，并且该方法在跨数据集测试上表现一般，因此没有给出结果。</p>
<p>&emsp;&emsp;消融实验如下，实验测试了将原视频逐帧调整分辨率的情况下在加入不同模块的结果。其中 128 to 64 表示：视频分辨率为 128 分辨率逐帧降低到 64 分辨率</p>
<p><img src="/Arbitrary_Resolution_rPPG/image-20221206230404607.png" alt="image-20221206230404607" style="zoom:80%;"></p>
<p>&emsp;&emsp;同时测试了在固定分辨率下的有效性，可以看出固定上的有效性显然不如可变分辨率的有说服力，但也有涨点，让人有点疑惑的是只加 TFA 的话模型会出问题，这里比较奇怪，因为分辨率应该不会影响到 TFA 的，而且固定分辨率下 PFE 所起的作用应当相对较小，可以尤其参考 128x128 的通用分辨率下的结果</p>
<p><img src="/Arbitrary_Resolution_rPPG/image-20221206230648793.png" alt="image-20221206230648793" style="zoom: 67%;"></p>
<p>&emsp;&emsp;接着还测试了方法的有效性、多余 TFA 和 PFE 模块对模型参数量和推理训练速度的影响、特征图的注意力范围等等，最后还测试了在 PFE 模块中的 $n$ 的灵敏度，基本上除了在参数量和推理速度上大方承认有待改进，其他的实验比较中规中矩，基本就是证明方法有效性的实验，指标因为是自己统计的，很难和其他比较</p>
<p><img src="/Arbitrary_Resolution_rPPG/image-20221206231112039.png" alt="image-20221206231112039"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>原始transformer</title>
    <url>/Attention%20is%20all%20you%20need/</url>
    <content><![CDATA[<h2 id="attention-is-all-you-need【机器翻译】【NIPS】"><a href="#attention-is-all-you-need【机器翻译】【NIPS】" class="headerlink" title="attention is all you need【机器翻译】【NIPS】"></a><a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">attention is all you need</a>【机器翻译】【NIPS】</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;原始transformer，RNN（解决 序列建模/位置敏感）$\to$ LSTM（解决 参数依赖/难以训练）$\to$transformer（解决 并行运算）。此篇笔记简要介绍网络、详细分析过程</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Attention%20is%20all%20you%20need/image-20220504100751796.png" alt="image-20220504100751796"></p>
<span id="more"></span>
<hr>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/Attention%20is%20all%20you%20need/image-20220504100937194.png" alt="image-20220504100937194" style="zoom: 67%;"></p>
<p>&emsp;&emsp;上图是网络概览图，简单概括：transformer由编码器和解码器两部分组成，对于编码器和解码器，在输入之前都有embedding部分和位置编码部分。</p>
<h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><h4 id="embedding-and-positionEncoding"><a href="#embedding-and-positionEncoding" class="headerlink" title="embedding and positionEncoding"></a>embedding and positionEncoding</h4><p>&emsp;&emsp;embedding部分主要目的是将输入的词进行向量化表示。对输入的句子$X$，应当有$X=\{x_i\}_n \to (l,n)$，其中 $l$ 为句子个数，即batchsize，$n$为句子中单词的个数的最大值，$x_i \to (1,x_d)$，具体维度并不确定。对每一个单词进行embedding操作【可以使用的方法包括glove, word2Vec等】，将会得到$X’=\{w_i\}_n\to (n,d)$，其中$w_i\to (1,d)$，$d$代表转化之后每个单词的维度。在标准transformer中为512。</p>
<p>&emsp;&emsp;此时得到的向量$X’$中不包含位置信息，为了包含位置信息，加入位置编码矩阵$P \to (n,d)$。具体来说，对于位置为 $pos$ 的词向量，对该向量第 $i$ 个位置（$2k$ 或 $2k+1$）的值，有：</p>
<script type="math/tex; mode=display">
P(pos,2k)=\sin(\frac{pos}{10000^{2k/d}}),k\in Z</script><script type="math/tex; mode=display">
P(pos,2k+1) = \cos(\frac{pos}{10000^{2k/d}}),k\in Z</script><p>&emsp;&emsp;在得到位置编码之后，需要进行相加操作得到最终的编码结果，即 $A = P+X’\to(n,d)$。</p>
<p>&emsp;&emsp;虽然位置编码到这里就结束了，但是在实际操作中有些具体细节，包括 ①decoder网络的输入不允许网络看到当前位置之后的数据，因此需要有一个mask上三角矩阵，②对于长度不足 $n$ 的句子的padding。</p>
<h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>&emsp;&emsp;encoder层如网络结构图所示，其本身是一个循环结构，因此要求输入输出维度一致，标准transformer循环层数为6。具体到每一个encoder层，其包含一个MHA（多头自注意力层）和一个FFN（前馈网络）。更具体地，每一个encoder层对embedding得到的矩阵 $A$，有 $A^{(i)} \to MHA(A) \to Z\to FFN(Z)\to A^{(i+1)}$。</p>
<p>&emsp;&emsp;对于MHA，其全称为MultiHeadAttention，其由多个Self-Attention并行组成，对于每一个Self-Attention，有如下的计算过程：</p>
<hr>
<p>&emsp;&emsp;记 $A =\{a_i\}_n\to (n,d)$，$a_i\to (1,d)$，对每一个 $a_i$ 计算 $Q_i,K_i,V_i\to(1,k)$，其中 $k$ 可以表示对每个单词提取的特征维度，在标准transformer上是64，维度的进一步变小是为了方便计算。$Q,K,V$ 的计算公式为：</p>
<script type="math/tex; mode=display">
Q_i=a_i\times W_Q</script><script type="math/tex; mode=display">
K_i= a_i\times W_k</script><script type="math/tex; mode=display">
V_i=a_i\times W_V</script><p>&emsp;&emsp;其中，$W_Q,W_K,W_V \to (d,k)$ 是随机初始化得到的，属于网络中的参数，随梯度进行更新。将上述计算过程写成矩阵的格式，即为 $Q=A\times W_Q\to (n,k)$，$K,V$ 同 $Q$。此时对于如 $Q=\{Q_i\},Q_i\to (1,k)$ 表示第 $i$ 个单词的 $k$ 个特征。$Q,K,V$ 分别表示 Query, Key, Value。</p>
<p>&emsp;&emsp;形象化地理解 $Q,K,V$ 可以以浏览器查询为例子， 其中 $Q$ 表示需要查询的字段，$K$ 表示知识库包含的字段，$V$ 表示知识库中该字段的返回结果。使用 $Q_i\times K_i$ 即可得到对于某个查询在某字段的“得分”。将得分变化至0-1之间，再与 $V_i$ 相乘即可得到在某个字段的解，所有的 $V_i$ 求和即可得到最终 $Q_i$ 的查询结果。</p>
<p>&emsp;&emsp;按照以上思路，对于 $a_i$ 可以得到 $Z_i= \sum\limits_{j\in[1,k]} \rm softmax (\frac{ Q_i\times K_i^T}{\sqrt d_k})\times V_i \to(1,k)$，其中 $d_k$ 表示 $Q,K,V$ 的第二维度，即 $k$。重写成矩阵的形式，即可得到：</p>
<script type="math/tex; mode=display">
Z=\rm softmax (\frac{Q\times K^T}{\sqrt {d_k}})\times V \to(n,k)</script><p>&emsp;&emsp;这个式子虽然很好地表示了 $Z$ 的求法，但是在实际计算的过程之中，对于得到 $Q\times K^T$ 需要进行mask，即和mask矩阵按位相乘，目的是使被padding部分的注意力清零。</p>
<hr>
<p>&emsp;&emsp;上述过程讲述了如何通过输入 $A\to(n,d)$ 求得对应的Self-Attention输出 $Z\to(n,k)$，但是在实际操作过程中往往使用MHA，即不共享参数同时并行计算 $p$ 个 $Z$，标准transformer计算8个。</p>
<p>&emsp;&emsp;对得到的 $Z_0\to Z_7$，将其横向拼接得到 $Z_{mix}\to(n,p\times k)$，然后对 $Z_{mix}$ 进行线性变换，经过一个Linear层将 $Z_{mix}$ 映射到 $Z’\to(n,d)$，此时的 $Z’$ 将会经过 $A\&amp;N$ 后送至FFN 。</p>
<h4 id="Add-and-Norm"><a href="#Add-and-Norm" class="headerlink" title="Add and Norm"></a>Add and Norm</h4><p>&emsp;&emsp;对于 $A\&amp;N$，大致上 $Add$ 对应残差连接，$Norm$ 对应 LayerNorm。具体来说，对于MHA，$A\&amp;N$ 输出的结果是 $\rm LayerNorm(A + MHA(A))$。对于FFN，输出为 $\rm LayerNorm(Z’+FFN(Z’))$ 。</p>
<h4 id="FFN"><a href="#FFN" class="headerlink" title="FFN"></a>FFN</h4><p>&emsp;&emsp;FFN网络结构简单，由两个线性层组成，在第一个线性层之后经过RELU，再经过第二个线性层，具体来说两个线性层维度为 $d\to 2048\to d$。</p>
<p>&emsp;&emsp;FFN包括上述的 A&amp;N 部分看起来很容易，但是原论文中进行了消融实验，这两个部分非常重要，尤其对于FFN部分。FFN部分是positionWise的，即这个网络实际上是对每一个维度进行的，类似于kernel=1的Conv2d。</p>
<h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><p>&emsp;&emsp;decoder部分整体和encoder部分没有太大的差别，区别在于decoder具备两个MHA，两个MHA在实现上和encoder的均略有不同。</p>
<p>&emsp;&emsp;对于第一个Masked MHA，首先是起始与终止符，对于输入的序列，起始符号为\<begin\>，终止符号是\<end\>，分别代表句首和句尾。其次是mask，在计算输入的时候就需要进行mask，对每个输入句子，输入的第 $i$ 个句子仅能看到前 $i$ 个词语。除此之外，在计算出 $Q\times K^T$ 之后也需要进行mask。这两个mask操作都是直接按位乘mask矩阵。mask矩阵为上三角矩阵，举例来说，对 4 个单词。</end\></begin\></p>
<script type="math/tex; mode=display">
mask = \begin{pmatrix}
1~1~1~1\\
0~1~1~1\\
0~0~1~1\\
0~0~0~1
\end{pmatrix}</script><p>&emsp;&emsp;对于第二个MHA，其 $Q,K,V$ 不单独计算，其中 $K,V$ 直接使用encoder输出的结果，$Q$ 使用第一个masked MHA输出的 $A$ 单独进行 $Q$ 的计算。之后的 $A\&amp;N$ 部分也使用第一个masked MHA的输出作为输入。</p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>&emsp;&emsp;标准transformer的损失采用交叉熵损失，但是在实际的操作过程中需要忽略padding的输出和ground truth。除此之外，学习率 $l$ 优化器使用Adam优化器，随着训练轮数 $s$ 先变大后变小。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>nlp</category>
      </categories>
  </entry>
  <entry>
    <title>BoxSnake</title>
    <url>/BoxSnake/</url>
    <content><![CDATA[<h2><center> BoxSnake: Polygonal Instance Segmentation with Box Supervision </center></h2>

<p>【segmentation】【ICCV2023】【<a href="http://arxiv.org/abs/2303.11630">paper</a>】【<a href="https://github.com/Yangr116/BoxSnake">code</a>】</p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>&emsp;&emsp;本文首次仅使用 box-level anno 监督 polygon-level 分割，在假设（1.物体边界通常有像素变化；2.在物体 box 的邻域内，物体内外区域像素应分布均匀）成立下，提出了针对两个假设进行约束的损失函数，使 box-level 的分割可以细化至 polygon-level 的分割。</p>
<h3 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h3><p><img src="/BoxSnake/image-20231110094336781.png" alt="image-20231110094336781" style="zoom:50%;"></p>
<span id="more"></span>
<hr>
<h3 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h3><ul>
<li>首次仅使用 box-level 监督模型进行 polygon-level 的分割预测</li>
<li>提出 local pairwise loss 用以约束 instance 边缘的像素变化</li>
<li>提出 global pairwise loss 用以约束 instance 内外的像素分布均匀（仅考虑 box 领域内）</li>
</ul>
<h3 id="formulation"><a href="#formulation" class="headerlink" title="formulation"></a>formulation</h3><p>&emsp;&emsp;输入图像 $\mathcal{I}\in\mathbb{R}^{H\times W\times3}$，其像素点集合记为 $\Omega_{\mathcal{I}}$；模型将会对 $N$ 个物体预测 $N$ 个多边形集合，每个多边形集合为 $\mathcal{C}=\{(x_i,y_i)\}_{i=1}^K$ 表示每个多边形由 $K$ 个点组成。</p>
<h3 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h3><p><img src="/BoxSnake/image-20231110110238854.png" alt="image-20231110110238854" style="zoom: 30%;"></p>
<ol>
<li><p>模型首先使用 ResNet 输出 box（此 box 采用 Mask R-CNN 的方式监督）和 FPN Features，并<strong>直接在全图按照椭圆采样</strong>初始化 polygon points</p>
</li>
<li><p>将 polygon points 进行 embedding 之后输入由一系列 SA 和 Cross-SA 组成的 transformer blocks，根据输出的 offset for each point 得到最终的输出</p>
</li>
</ol>
<p>&emsp;&emsp;本文的工作重点不在于此框架，更应该关注的是如何使用损失函数约束 refined points。本文共提出了三个损失函数从三个不同的层面进行约束：</p>
<ul>
<li><strong>Point-based Unary Loss</strong>：用于优化 refined points 的 box</li>
</ul>
<p>&emsp;&emsp;这里的损失函数非常简单，使用 CIoU 进行 box-level 的监督。具体来说，i) 对最终输出的点集 $\mathcal{C}_r$，计算 $(x_1,y_1)=\min(\mathcal{C}_r),\ (x_2,y_2)=\max(\mathcal{C}_r)$，ii) 依据 $(x_1,y_1)$ 作为左上角，$(x_2,y_2)$ 作为右上角计算得到预测 box $b_c$，iii) 计算损失 $\mathcal{L}_u=1-CIoU(b_c,b_{gt})$。</p>
<p>&emsp;&emsp;在这样的损失监督下，即使是最好的情况，也只能保证 box-level 的预测，反映在多边形上可能是 random 的点或者在“更期望和原始 box 一致”的要求下尽可能像边缘移动，如下图 $(b)$ 所示。</p>
<p><img src="/BoxSnake/image-20231112211157085.png" alt="image-20231112211157085" style="zoom:50%;"></p>
<ul>
<li><strong>Local Pairwise Loss：</strong>用于优化最终输出，使多边形边缘和像素变化位置重合</li>
</ul>
<p>&emsp;&emsp;这是这篇文章最主要的贡献，讲道理想到“多边形边缘是像素变化较大的位置”并不困难，但如何设计对应的损失函数，既要反映此规律，又能正确回传梯度、有效率地优化不太容易。$\mathcal{L}_{lp}$ 大致可以描述为：<strong>在一个邻域内，任意两个像素之间的色彩相似度 $w$ 和分类惩罚 $\Delta u$（in box or not）乘积之和</strong>。简单来说，对于两个相近的色彩，色彩相似度 $w$ 会比较大，而如果此时两个像素没有被分到同一类，则 $\Delta u$ 也会很大，就会产生很大的惩罚项。模型会被迫让相似度大的像素被分为一类。</p>
<p>&emsp;&emsp;在位置 $(i,j)$ 处，记其 $k\times k$ 邻域内的像素集合为 $\Omega_k^{(i,j)}$，类别函数（一元）为 $\mathcal{U_C}$，相似度计算函数（二元）为 $\mathcal{W}$。定义相似度计算函数为：$\mathcal{W}(\mathcal{I}(i,j),\mathcal{I}(p,q))=\exp(-\frac{|\mathcal{I}(i,j)-\mathcal{I}(p,q)|_2}{2\tau_1^2})$，其中 $\tau_1$ 为温度超参。简单来说就是，颜色越像，值越大。</p>
<p>&emsp;&emsp;类别函数相对复杂：$\mathcal{U_C}(i,j)=\sigma(\frac{2\cdot(\Phi(i,j)-0.5)\cdot \mathcal{D_C}(i,j) }{\tau_2})$，其中 $\sigma$ 即为 Sigmoid；$\Phi(i,j)=1\ \mathrm{if}\ (i,j)\ \mathrm{in\ Polygon\ else\ 0}$ 表示点是否在多边形之中；$\mathcal{D_C}$ 是距离函数，表示 $(i,j)$ 距离最近的多边形边缘的欧氏距离；$\tau_2$ 是另一个温度超参。简单来说可以理解为 soft 的 0-1 分类函数。大致示意如下图：</p>
<p><img src="/BoxSnake/image-20231112214532066.png" alt="image-20231112214532066" style="zoom:50%;"></p>
<p>&emsp;&emsp;总结来说，约束相似颜色之间不存在多边形的损失函数为：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{lp}=\sum\limits_{(p,q)\in\Omega_k^{(i,j)}}\mathcal{W}(\mathcal{I}(i,j),\mathcal{I}(p,q))\cdot\|\mathcal{U_C}(i,j)-\mathcal{U_C}(p,q)\|_1</script><p>&emsp;&emsp;尽管 $\mathcal{U_C}$ 看上去很复杂，但其实际起到的作用就是硬分类函数 $\Phi$ 的作用，之所以搞复杂是因为 $\Phi$ 无法回传梯度，而 $\mathcal{U_C}$ 的梯度可以通过 $\mathcal{D_C}$ 回传到多边形点集上。</p>
<ul>
<li><strong>Global Pairwise Loss：</strong>防止模型在 $\mathcal{L}_{lp}$ 优化下受到太多干扰（但实验似乎表明没有这种现象，同时 $\mathcal{L}_{gp}$ 的超参只有 0.03）</li>
</ul>
<p>&emsp;&emsp;对于 $\mathcal{L}_{lp}$，模型可能受到大量的干扰（instance 内的像素变化位置太多，混淆了边界位置像素变化），因此 $\mathcal{L}_{gp}$ 用于清除这种混淆， $\mathcal{L}_{gp}$ 将会使 $\mathcal{U_C}$ 内/外的像素方差更小。由于 $\mathcal{U_C}$ 是 soft boundary，因此 $\mathcal{L}_{gp}$ 定义为：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{gp} = \sum\limits_{(x,y)\in\Omega}\|\mathcal{I}(x,y)-u_{in} \|_2\cdot \mathcal{U_C}(x,y)+\sum\limits_{(x,y)\in\Omega}\|\mathcal{I}(x,y)-u_{out} \|_2\cdot (1-\mathcal{U_C}(x,y))</script><p> &emsp;&emsp;其中 $u_{in},u_{out}$ 就表示 instance 内外的像素色彩均值，在 soft 的情况下定义为加权平均：</p>
<script type="math/tex; mode=display">
u_{in}=\frac{\sum\limits_{(x,y)\in\Omega_c}\mathcal{I}(x,y)\cdot\mathcal{U_C}(x,y)}{\sum\limits_{(x,y)\in\Omega_c}\mathcal{U_C}(x,y)}\\
u_{out}=\frac{\sum\limits_{(x,y)\in\Omega_c}\mathcal{I}(x,y)\cdot(1-\mathcal{U_C}(x,y))}{\sum\limits_{(x,y)\in\Omega_c}(1-\mathcal{U_C}(x,y))}</script><p>&emsp;&emsp;需要特别指出：上式中 $\Omega_c$ 指的并不是所有像素集合 $\Omega_{\mathcal I}$，而是按照预测的多边形点集 $\mathcal C$ 进行简单扩展并 crop&amp;resize 得到的新图像，过程如下图所示。这很容易理解，因为 GP 的假设是 instance 内外的像素一致性，但是一张图有多个 instance，外部像素不可能一致，因此这种 clipped-image 当做的“global”才是更合理的。</p>
<p><img src="/BoxSnake/image-20231112221542743.png" alt="image-20231112221542743" style="zoom: 33%;"></p>
<p>&emsp;&emsp;总结而言，三个损失函数直接按照线性进行组合：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{ploygon}=\mathcal{L}_u+\alpha\mathcal{L}_{lp}+\beta\mathcal{L}_{gp},\ \mathrm{where\ \alpha=0.5,\ \beta=0.03}</script><h3 id="experiment"><a href="#experiment" class="headerlink" title="experiment"></a>experiment</h3><p>&emsp;&emsp;大部分实验结果表明本方法有一定的提升能力，但也要注意到本方法是 box-2-polygon 的，和其他 box-2-mask 的方法本身就有着一定的优势。另外需要指出，在定量、定性的消融实验中，global pairwise 损失相对于 local pairwise 损失的有效性相当“轻微”</p>
<p><img src="/BoxSnake/image-20231112222622703.png" alt="image-20231112222622703" style="zoom:50%;"></p>
<p><img src="/BoxSnake/image-20231112222703927.png" alt="image-20231112222703927" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>CVD</title>
    <url>/CVD/</url>
    <content><![CDATA[<h2><center> Video-Based Remote Physiological Measurement via Cross-Verified Feature Disentangling </center></h2>

<p>【心率检测】【ECCV2020】【<a href="https://arxiv.org/pdf/2007.08213.pdf">paper</a>】【<a href="https://github.com/nxsEdson/CVD-Physiological-Measurement">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种基于自编码器的架构的特征学习器，并且将输入的 video 表示为多尺度 STmap，通过多编码器-单解码器的方式生成编码不同特征的编码器，同时通过交叉验证解离（CVD）的策略训练。同时对解码出的特征估计时使用了可以输出多种生理信号的估计器，是结合了自编码器、解离表征、rppg 估计网络的 SOTA。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/CVD/image-20221107093200677.png" alt="image-20221107093200677" style="zoom:80%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>MSTmap（多尺度 STmap）的提出</li>
<li>新的训练策略 CVD（交叉验证解离）</li>
<li>一种用于估计 rppg 信号和 HR 信号的双头网络</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><h4 id="MSTmap"><a href="#MSTmap" class="headerlink" title="MSTmap"></a>MSTmap</h4><p>&emsp;&emsp;首先介绍 STmap，所谓 STmap，即时空图。由于在心率估计中，人脸 ROI 区域蕴含的信息相比于环境信息来说更加重要，因此试着将输入的 video 通过某种<strong>非学习的策略</strong>变成仅含有人脸信息的色彩特征图，以便于接下来的估计网络学习这些特征（正是因为在 RhythmNet 和 CVD 中都含有这一部分，一些工作认为这样的处理将整个网络变成了非端到端的架构）。</p>
<p><img src="/CVD/image-20221107093947556.png" alt="image-20221107093947556"></p>
<p>&emsp;&emsp;其具体的处理为：1.通过开源工具 <a href="https://github.com/seetaface/SeetaFaceEngine">SeetaFace</a> 标定人脸的关键点，将输入的 video 的每一帧都对齐至同一个位置并且进行 crop 使得人脸位于中央，此时 $video\to(T,H,W,C)$，2.将现在的 video 按照空间维度将每帧分解为 $n$ 个块，将每个块按照 YUV 通道展开，记每个块中的像素点个数为 $c$，这里的 $c=\frac{HW}n$，这里的结果进行一个重排，在不改变维度的情况下就成为了 STmap，此时的 $STmap\to(T,n,c)$。</p>
<p><img src="/CVD/image-20221107093640013.png" alt="image-20221107093640013" style="zoom:67%;"></p>
<p>&emsp;&emsp;所谓多尺度 STmap，即相对于在 RhythmNet 中的 STmap 多了环境信息。具体来说，首先利用开源工具 <a href="https://github.com/seetaface/SeetaFaceEngine">SeetaFace</a> 标定第 $t$ 帧人脸上信息量最大的 $n$ 个 ROI 区域，将这 $n$ 个区域进行排列组合总共形成 $2^n-1$ 个图像（除了所有 ROI 均不存在的情况）。同时将第 $t$ 帧的图像进行通道分解为 RGBYUV 共六个，之后和 ROI 组合进行笛卡尔积，考虑每一帧，最终得到多尺度 STmap 的 shape 为 $(T,6,2^n-1)$。</p>
<h4 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h4><p><img src="/CVD/image-20221107093200677.png" alt="image-20221107093200677" style="zoom:80%;"></p>
<p>&emsp;&emsp;网络比较复杂，因此这里对着图介绍。网络进行联合训练，直接算出所有的 loss，求和之后进行梯度回传。首先介绍前向过程。</p>
<ul>
<li>将输入的 video clip 进行预处理，得到 MSTmap，具体地，对于一对视频 $v_1,v_2$，生成的时空图分别记为 MSTmap $M_1,\ M_2$，实现中这一对视频由随机下标在同一个 batch 内取得。</li>
<li>将 $M_1,\ M_2$ 分别通过两个不同的编码器 $E_n,\ E_p$ 得到 4 个值 $f_{p1},\ f_{n1},\ f_{p2},\ f_{n2}$ ，其中 $p$ 下标代表生理信号，$n$ 下标代表非生理信号</li>
<li>将得到的 4 个信号组合进入解码器 $D$，得到 4 个解码之后的特征 $M_1,\ pseudo\ M_1,\ M_2,\ pseudo\ M_2$，其中 $M_1,\ M_2$ 即为正常的编解码器的输出，用于监督训练 AE；其中 $\ pseudo\ M_1,\ pseudo\ M_2$ 代表使用不对应的生理特征和非生理特征组合得到的伪特征</li>
<li>将两个伪特征再一次进行编码，分别通过 $E_n,\ E_p$，得到四个输出 $f_{pse_p1},\ f_{pse_n1},\ f_{pse_p2},\ f_{pse_n2}$，如果我们的  $E_n,\ E_p$ 能够有效分离生理和非生理特征，这里的 $f_{pse_p1}$ 应该和 $f_{p1}$ 接近，而 $f_{pse_n1}$ 反而应该和 $f_{n2}$ 接近，其他同理</li>
<li>将得到的 $f_{pse_p1},\ f_{p1}$ 分别输入双头估计器，只取 $f_{p1}$ 生成的 rppg 信号 $s_{pre}$，并且取两个输入生成的 HR 信号 $HR_1,\ HR_{pse1}$，对 $f_{pse_p2},\ f_{p2}$ 同理。这里生成的 $s_{pre},\ HR_1$ 用于监督训练双头网络，其余的都用于进行 CVD 训练，目的是监督 $E_n,\ E_p$ 的各司其职</li>
</ul>
<p>&emsp;&emsp;接下来逐个介绍 loss 的计算。</p>
<ul>
<li>$\mathcal L_{rec}$</li>
</ul>
<script type="math/tex; mode=display">
\mathcal L_{rec}=\lambda_{rec}\sum\limits_{i=1}^2||M_i-M_i'||_1</script><p>&emsp;&emsp;使用 L1Loss 限制 AE 的表征学习能力。</p>
<ul>
<li>$\mathcal L_{CVD}$</li>
</ul>
<script type="math/tex; mode=display">
\mathcal L_{CVD}=\lambda_{cvd}\sum\limits_{i=1}^2||f_{pi}-f_{pse\_pi}||_1 +\lambda_{cvd}\sum\limits_{i=1}^2||f_{ni}-f_{pse\_n(3-i)}||_1+\sum\limits_{i=1}^2||HR_i-HR_{psei||_1}</script><p>&emsp;&emsp;这个损失函数限制各个对应的预测值相等，通过此限制强迫编码器 $E_n,\ E_p$ 各司其职。</p>
<ul>
<li>$\mathcal L_{pre}$</li>
</ul>
<script type="math/tex; mode=display">
\mathcal L_{rppg}=1-\frac{Cov(s_{pre},s_{gt})}{\sqrt{Cov(s_{pre},s_{pre})}\sqrt{Cov(s_{gt},s_{gt})}}</script><script type="math/tex; mode=display">
\mathcal L_{rppg\_hr}=CE(PSD(s_{pre}),HR_{gt})</script><script type="math/tex; mode=display">
\mathcal L_{pre}=||HR_{pre}-HR_{gt} ||+\lambda_{rppg}\mathcal L_{rppg}+\mathcal L_{rppg\_hr}</script><p>&emsp;&emsp;这个函数通过限制 rppg 信号的估计准确性和心率的预测准确性强迫双头网络估计器的学习。</p>
<p>&emsp;&emsp;总体来说，loss 由下式给出，通过之前的前向过程中的各个中间变量获得损失函数，并且对各个网络进行联合训练。在实现的过程中，backbone 为 res18，编解码器为同一个网络实现，因此代码中的前向过程和描述的过程先后性并不完全一致，但是得到的值和逻辑都一致。</p>
<script type="math/tex; mode=display">
\cal L=L_{rec}+L_{\rm CVD}+L_{pre}</script><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;和大部分余老师的论文一致，这篇论文也是主要关注于：1.消融实验，2.单数据集和跨数据集结果</p>
<ul>
<li>消融实验</li>
</ul>
<p>&emsp;&emsp;这部分探究了 a）STmap 的多尺度性，b）是否含有 MTL，c）是否含有 CVD，其中是否含有 CVD 探索了包含、不含（使用头部运动作为非生理信号解离）、不含（使用面部像素标准差作为非生理性解离）三种情况。</p>
<p><img src="/CVD/image-20221107121019688.png" alt="image-20221107121019688" style="zoom:80%;"></p>
<ul>
<li>单数据集（OBF，左）和跨数据集结果（MMSE-HR，右，训练集为 VIPL-HR）</li>
</ul>
<p><img src="/CVD/image-20221107121210877.png" alt="image-20221107121210877" style="zoom:80%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>AutoHR</title>
    <url>/AutoHR/</url>
    <content><![CDATA[<h2><center> AutoHR: A Strong End-to-end Baseline for Remote Heart Rate Measurement with Neural Searching </center></h2>

<p>【心率检测】【SPL2020】【<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9133501">paper</a>】【<a href>code未开源</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种基于 NAS 的心率检测 backbone 设计策略，在操作域中加入了 TDC（时空差分卷积），同时提出了一种时域和频域的混合损失函数，在单数据集和跨数据集上的测试都取得了较好的结果，并且为了解决现有方法泛化能力弱的问题提出了新的数据增强策略，取得了端到端的 SOTA。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/AutoHR/image-20221106184645470.png" alt="image-20221106184645470"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>首次使用 NAS 搜索 HR 任务的网络，并且在 NAS 搜索中加入了 TDC</li>
<li>提出了一种时域频域的 loss，提出两种数据增强策略</li>
</ul>
<h3 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h3><h4 id="NAS（神经结构搜索）"><a href="#NAS（神经结构搜索）" class="headerlink" title="NAS（神经结构搜索）"></a>NAS（神经结构搜索）</h4><p>&emsp;&emsp;NAS(Neural Architecture Search) 意图使用深度学习任务自主设计神经网络，其基本工作流程如下图所示。首先由人工给定搜索的空间以及搜索策略，由机器自主探索空间中的操作组成的网络结构，并根据其网络的表现更新网络架构和超参数。</p>
<p><img src="/AutoHR/e082b8ba4993c229c993b7dfa19d4641.png" alt="img"></p>
<p>&emsp;&emsp;但是 NAS 有着难以避免的问题：1.超参数和搜索空间的网络架构都是离散的，2.大量的超参数使得训练时间太长。为了解决这些问题，提出了层次化表示：Learning Transferable Architectures for Scalable Image Recognition，权重共享：Reinforcement Learning for Architecture Search by Network Transformation，表现预测：Progressive Neural Architecture Search 等方法。</p>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><h4 id="TDC"><a href="#TDC" class="headerlink" title="TDC"></a>TDC</h4><p>&emsp;&emsp;时空差分卷积（TDC）是一种新的卷积策略，其卷积过程大致如下图所示。具体来说，对第 $t$ 帧图像 $p_0$ 为位置的 3x3 卷积过程如下：对第 $t$ 帧图像的 $p_0$ 位置进行 expand 操作，扩展到整个 3x3 的大小，并且分别和 $t-1$，$t+1$ 帧的对应位置进行作差。注意这里的作差中 expand 的部分带有一个参数 $\theta$ 用来控制当前帧对临近帧的影响程度，对原位置直接进行卷积，并且将最终的结果相加。</p>
<p><img src="/AutoHR/image-20221106184757405.png" alt="image-20221106184757405" style="zoom:50%;"></p>
<p>&emsp;&emsp;上面的过程使用公式可以表示为：</p>
<script type="math/tex; mode=display">
O(p_0^t)=\mathop\sum\limits_{p_n^{t-1}\in\mathcal R_{t-1}}w(p_n^{t-1})·(I(p_0^{t-1}+p_n^{t-1})-\theta·I(p_0^t))</script><script type="math/tex; mode=display">
+\mathop\sum\limits_{p_n^{t+1}\in\mathcal R_{t+1}}w(p_n^{t+1})·(I(p_0^{t+1}+p_n^{t+1})-\theta·I(p_0^t))</script><script type="math/tex; mode=display">
+\mathop\sum\limits_{p_n^{t-1}\in\mathcal R_{t}}w(p_n^{t})·I(p_0^{t}+p_n^{t})</script><h4 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h4><p>&emsp;&emsp;正如概览图中所示，整个网络由一个有向无环图进行估计，记所有的操作空间为 $\mathcal O$，对于图中的每一个边 $(i,j)$，其输出为：$\tilde o^{(i,j)}(x_i)=\sum_{o\in\mathcal O}\eta_o^{(i,j)}·o(x_i)$，其中的 $o$  代表操作空间中的每一个操作，$\eta$ 由一个 softmax 的式子给出，$\eta_o^{(i,j)}=\frac{exp(\alpha_o^{(i,j)})}{\sum_{o’\in\mathcal O}exp(\alpha_{o’}^{(i,j)})}$，其中的 $\alpha_o^{(i,j)}$ 表示在边 $(i,j)$ 中操作 $o$ 的权重或系数。</p>
<p>&emsp;&emsp;在这种表示下，可以通过如下式子进行训练从而找到最适合的 $\alpha$，在训练的过程中，由于有四个 block，这四个 block 可以以共享权重或不共享权重的方式训练，其中权重共享实际上是 NAS 的共识，这样不仅可以加速搜索，同样可以取得更好的结果。本文的消融实验也探讨了这部分。</p>
<script type="math/tex; mode=display">
\min\limits_\alpha \mathcal L_{val(\Phi^*(\alpha),\alpha)},\ s.t.\ \Phi^*(\alpha)=\arg \min\limits_\Phi\mathcal L_{train}(\Phi,\alpha)</script><p>&emsp;&emsp;通过这种方式找到的网络结构每个 cell 中的每一条边都含有所有的 $\mathcal O$ 中的网络算子，并且每一个算子都具备对应的权重，但是这样的网络结构太大，其中的多个算子中某些算子的权重其实非常低，因此作者采用这样的方法进行网络的剪枝以确定最终的网络：1.对于最终的输出 output 的入边，每个边选取一个权重最大的操作保留，2.对于其他的中间节点 B1,B2,B3 的入边，每个选取权重最大的两个操作保留。</p>
<p><img src="/AutoHR/image-20221106195942710.png" alt="image-20221106195942710" style="zoom:50%;"></p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>&emsp;&emsp;损失函数包括两个部分，分为时间和频率，其中时间部分就是所谓的线性相关性，具体由下式给出，并且这个函数在 rPPGNet 之中也用到了，不过 rPPGNet 更晚一些就是了。</p>
<script type="math/tex; mode=display">
\mathcal L_{np}=1-\frac{T\mathop\sum\limits_{i=1}^T y_iy_i^g-\mathop\sum\limits_{i=1}^Ty_i\mathop\sum\limits_{i=1}^Ty_i^g}{\sqrt{(T\mathop\sum\limits_{i=1}^Ty_i^2-(\mathop\sum\limits_{i=1}^Ty_i)^2)(T\mathop\sum\limits_{i=1}^T(y_i^g)^2-(\mathop\sum\limits_{i=1}^Ty_i^g)^2)}}</script><p>&emsp;&emsp;对于频率域，就是对 $y_i$ 求功率谱密度，然后和 GT 做比较，即：</p>
<script type="math/tex; mode=display">
\mathcal L_{fre}=\sum CE(PSD(y_i),y_i^g)</script><p>&emsp;&emsp;整体来说的损失函数由 $\mathcal L_{overall}=\lambda·\mathcal L_{time}+\mathcal L_{fre}$ 给出，其中 $\lambda$ 用于平衡损失。</p>
<h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p>&emsp;&emsp;本文针对现有数据集的两个问题提出了两个数据增强策略：1.头部运动可能导致 ROI 遮挡，2.HR 分布严重不平衡，呈倒 V 形。</p>
<p>&emsp;&emsp;对于这两个问题的改进为：1.在随机时间片段（小于 20% 的空间大小和 20% 的时间长度）内随机擦除或切除部分时空管，模拟 ROI 遮挡的情况，2.对视频进行时间上采样和下采样，以生成具有极小或极大 HR 值的额外训练样本。具体来说，HR 值大于 90 bpm 的视频将被时间插值两次，而 HR 值小于 70 bpm 的视频将以采样率 2 下采样，分别模拟一半和两倍的心率。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;本文做了简单的消融实验，针对性地探究了 a）损失函数，b）TDC 中的 $\theta$ 值，c）是否具备 NAS / TDC 部分，d）是否使用数据增强策略。结果如下图所示，虽然基本上都没有提太多，但是每个方法都或多或少提了0.几 的点。</p>
<p><img src="/AutoHR/image-20221106202041824.png" alt="image-20221106202041824" style="zoom:70%;"></p>
<p>&emsp;&emsp;除了消融实验，本文还和目前的 SOTA 进行了比较，其中的△表示传统方法，◇表示非端到端的方法，☆表示端到端的深度学习方法。这里的 RhythmNet 是同年的论文，提出了 VIPL-HR，也是很棒的 SOTA 文章。</p>
<p><img src="/AutoHR/image-20221106202146459.png" alt="image-20221106202146459" style="zoom:80%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>multi-network learning rPPG</title>
    <url>/Co-rectification/</url>
    <content><![CDATA[<h2><center> Contactless Pulse Estimation Leveraging Pseudo Labels and Self-Supervision </center></h2>

<p>【rPPG】【ICCV2023】【<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Contactless_Pulse_Estimation_Leveraging_Pseudo_Labels_and_Self-Supervision_ICCV_2023_paper.html">paper</a>】【<a href>code not available</a>】</p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>&emsp;&emsp;本文提出了一种使用伪标签监督辅助对比学习的无监督范式，使用 2SR 生成伪标签并通过课程式学习逐步平衡伪标签监督和对比学习。关于伪标签监督是多网络学习在 rPPG 的一种成功迁移。需要指出本文将视频 $x$ 转化为了 STMap，但在之后多次用到 $\phi(x)$，难免造成模型直接处理视频的误会。</p>
<h3 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h3><p><img src="/Co-rectification/image-20231109185005424.png" alt="image-20231109185005424" style="zoom:50%;"></p>
<span id="more"></span>
<hr>
<h3 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h3><ul>
<li>在普通自监督的基础上使用伪标签进行额外监督</li>
<li>使用双模型联合训练的方式对伪标签进行优化</li>
</ul>
<h3 id="formulation"><a href="#formulation" class="headerlink" title="formulation"></a>formulation</h3><p>&emsp;&emsp;输入视频被转化为 STMap $x$；双模型 $\phi_1:x\to y_1$ 和 $\phi_2:x\to y_2$ 均为 Dual-GAN 的 backbone（不共享参数） ；2SR 作为传统方法直接从视频中估计伪标签 $y_p$；通过对 $y$ 计算 PSD $\mathcal{F}(y)$ 获取其对应心率 $h$。</p>
<h3 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h3><p><img src="/Co-rectification/image-20231109191326777.png" alt="image-20231109191326777" style="zoom:50%;"></p>
<ul>
<li>自监督学习范式</li>
</ul>
<p>&emsp;&emsp;这部分基本与之前的相关工作没有区别，本文使用时间移位来构建 pos.，即对于 anchor，将其在视频内的位置平移 $\pm f$ 帧得到正样本：$\mathcal{L}_p=d(\mathcal{F}(\phi(x)),\mathcal{F}(\phi(x_{\pm f})))$，也就是期望模型在 anchor 和 pos. 上获取一致的 PSD。</p>
<p>&emsp;&emsp;本文使用频率的上采样 $u_{\uparrow}$ 和下采样 $d_{\downarrow}$ 获取 neg.：$\mathcal{L}_n=-d(\mathcal{F}(\phi(x)),\mathcal{F}(\phi(x_{d_\downarrow})))-d(\mathcal{F}(\phi(x)),\mathcal{F}(\phi(x_{u_\uparrow})))$，也就是期望模型对于上下采样之后的 anchor 和 neg. 获取不同的 PSD，上下采样的范围是 [0.6,1.3]。</p>
<p>&emsp;&emsp;由于构造 neg. 时使用了上采样和下采样，因此视频对应的心率也应该呈现对应的变化，即 $h_{u\uparrow}&lt;h&lt;h_{d\downarrow}$，针对这种理应的变化设计损失函数为：$\mathcal{L}_{rank}=\max(h_{u\uparrow}-h,0)+\max(h-h_{d\downarrow},0)$。</p>
<p>&emsp;&emsp;综合自监督部分，损失函数为：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{self-sup}=\mathcal{L}_p-\mathcal{L}_n+\alpha\mathcal{L}_{rank},\ \mathrm{where\ \alpha=0.01}</script><ul>
<li>伪标签学习范式</li>
</ul>
<p>&emsp;&emsp;本文实质上是 multi-network learning 在 rPPG 上的成功应用，在两个模型上的协同优化循环优化 $\phi_1$ 和 $\phi_2$，优化 $\phi_1$ 可以表述为：</p>
<p>&emsp;&emsp;i ) 计算 batch 内的 $\epsilon_1= \mathrm{abs}(h_p^b-h_1^b)\mathrm{\ for\ b\in[1,bs]}$，其表示模型输出和伪标签直接的差值</p>
<p>&emsp;&emsp;ii) 在 $\epsilon_2$ 中选择误差最大的 $\lambda$ 个下标 $\{b_i\}_1^\lambda$，此时认为这些下标对应的伪标签是置信度低的，从而使用 $h_2^{b_i}$ 替换掉 $h_p^{b_i}$ 作为对 $\phi_1$ 的监督，此时并不更新 $\phi_2$</p>
<p>&emsp;&emsp;对于伪标签学习，损失函数 $\mathcal{L}_{pseudo}$ 为简单的 MSE。但考虑到前期同一个 batch 内会出现多个由于网络没有充分训练的“偏差值”，因此 $\lambda$ 应当随着训练缓慢增大。具体来说，$\lambda=\frac{e_t}{e_{max}}\lambda_{max}$，其中 $e_t$ 表示第 $t$ 个 epoch，$\lambda_{max}=8$。</p>
<p>&emsp;&emsp;综合伪标签学习和对比学习时，应该考虑到前期的对比学习相对于伪标签学习这种“伪监督学习”具备更低的置信度，因此 trade-off 应该先考虑伪标签学习，再考虑对比学习。</p>
<script type="math/tex; mode=display">
\mathcal{L}=(1-\frac{e_t}{e_{max}})\mathcal{L}_{pseudo}+\mathcal{L}_{self-sup}</script><h3 id="experiment"><a href="#experiment" class="headerlink" title="experiment"></a>experiment</h3><p>&emsp;&emsp;是第一个在 VIPL 上测试的模型，分成 10s 的 clip 做 video-level 测试，intra- 和 cross- 都 OK</p>
<p><img src="/Co-rectification/image-20231109204253372.png" alt="image-20231109204253372" style="zoom:50%;"></p>
<p><img src="/Co-rectification/image-20231109204310352.png" alt="image-20231109204310352" style="zoom:50%;"></p>
<p>&emsp;&emsp;消融实验很有说服力</p>
<p><img src="/Co-rectification/image-20231109204403769.png" alt="image-20231109204403769"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>CodeFormer</title>
    <url>/CodeFormer/</url>
    <content><![CDATA[<h2><center> Towards Robust Blind Face Restoration with Codebook Lookup Transformer </center></h2>

<p>【盲人脸重建】【NIPS2022】【<a href="http://arxiv.org/abs/2206.11253">paper</a>】【<a href="https://github.com/sczhou/CodeFormer">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文借鉴 VQGAN 的思想细化了其应用领域并进行了启发式的改造，将条件生成限制到盲人脸重建，通过增加 CFT 模块优化过度平滑的问题，并且将采样器去掉，作为必选项在重建图像时优化隐编码。由于 CFT 模块可选，因此在一定程度上避免了如 GPEN 的跳跃连接，防止过差的 LQ 影响重建质量。其本质与其说参考了更多的盲图重建工作，倒不如说参考了 VQ 系列的工作。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/CodeFormer/image-20230208213428701.png" alt="image-20230208213428701"></p>
<span id="more"></span>
<hr>
<h3 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h3><h4 id="VQVAE"><a href="#VQVAE" class="headerlink" title="VQVAE"></a>VQVAE</h4><p><a href="http://arxiv.org/abs/1711.00937">Neural Discrete Representation Learning</a>  NIPS2017</p>
<p><img src="/CodeFormer/image-20230208195628508.png" alt="image-20230208195628508"></p>
<p>&emsp;&emsp;所谓 VQVAE，即矢量量化（Vector Quantized）VAE，其基础仍是 VAE，其具备和 VAE 基本一致的训练过程，只是在 Encoder $q$ 编码出 latent code $z_e$ 之后并不直接给 Decoder，而是经过通过一个码本 CodeBook 进行量化，得到离散的编码表示 $z_q$，再输入 Decoder $p$ 生成重建图像 $p(x|z_q)$。</p>
<p>&emsp;&emsp;首先，论文本身上，其区别于 VAE，基于量化的编码可以很好地避免隐空间在采样重建时出现崩溃，但是同时由于 codebook 的限制，采样的多样性也相应减弱了，这种量化的思想更适用于重建（生成）质量大于多样性的场景，比如盲图重建。对于原文来说，VQVAE 主要进行图像重建、条件生成、语音生成，其中重建效果甚至不如好的 AE，条件生成结果更是和 CGAN 比不了，更不用提最近的 GAN。语音重建是 VQVAE 卖点之一，其宣称自己证明了深度学习模型可以无监督地从音频里学到语义。和盲图重建无关，就不展开了。</p>
<h5 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a>backbone</h5><p>&emsp;&emsp;codebook 和量化过程可以描述为：</p>
<ul>
<li>codebook 实际上是向量的集合，其 shape 为：$(m\cdot n)\times d$ ，其中 $m,\ n,\ d$ 是原图经过编码器得到的隐编码 $z_e$ 的 shape<ul>
<li>该集合有 $m\cdot n$ 个元素，每个元素是连续的 $(1,d)$ 的向量，元素下标范围是 $[0,m\cdot n)$，必须为整数</li>
<li>codebook 中每个元素的初始值随机生成，在训练的过程中不断优化，优化目标是量化过程中损失最少的信息</li>
</ul>
</li>
<li>量化过程为：<ul>
<li>对于编码器的输出 $z_e\to(m,n,d)$，对其中的 $z_e[i,j]$ 有：<ul>
<li>$z_q[i,j] = e_k,\ \ where\ \ k=arg\min_j||z_e[i,j]-e_j||_2$，从而输出 $z_q\to(m,n,d)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>&emsp;&emsp;其网络训练过程主要可以被描述为：</p>
<ul>
<li>正向传播<ul>
<li>输入原图 $x$，经过编码器 E 得到 $z_e$</li>
<li>$z_e$ 和 codebook 进行最近邻查询，将 $z_e$ 量化为 $z_q$</li>
<li>$z_q$ 经过解码器 D 得到重建图像 $p(z_q)$</li>
</ul>
</li>
<li>反向传播<ul>
<li>$L=\log p(x|z_q(x))+||sg[z_e(x)]-e||_2^2+\beta||z_e(x)-sg[e]||_2^2$</li>
<li>其中，第一项为重建损失，也是 VAE 的 ELBO 中的一项，使用 L1 或者 L2 损失度量重建图像质量</li>
<li>第二项和第三项为优化量化过程中的信息损耗，其中 $sg$ 表示 detach，即分别优化编码器 E 和码本 codebook</li>
</ul>
</li>
</ul>
<p>&emsp;&emsp;相对于 VAE 来说，ELBO 缺少了 KL 散度的度量项，这是因为 $z$ 并非高斯分布，实际上，$z$ 的分布是离散的分类分布，具体可以描述为：</p>
<script type="math/tex; mode=display">
q(z=k|x)=\begin{cases}
1\ \ \ \ \rm{for} \ k=arg\min_j||z_e(x)-e_j||_2\\
0\ \ \ \ \rm{otherwise}
\end{cases}</script><p>&emsp;&emsp;以此分布推理 KL 散度可以得到最终的结果为 $\log K$，$K=m\cdot n$。</p>
<h5 id="从分类分布中采样"><a href="#从分类分布中采样" class="headerlink" title="从分类分布中采样"></a>从分类分布中采样</h5><p>&emsp;&emsp;对已经训练好的 backbone，按照 VAE 的思路接下来只需要使用解码器 $p$ 然后从高斯分布中采样隐编码 $z$ 就可以了，但是明显可以看到，$z$ 并不属于高斯分布，因此需要额外构建一种采样方式，本文采用的方式为 PixelCNN。</p>
<p>&emsp;&emsp;PixelCNN 是自回归模型的一种，事实上使用任意一种自回归模型均可。所谓自回归模型，即模型接收当前位置前的序列并预测当前位置的值，类似于语言生成。对于 PixelCNN，具体的实现方式为采用卷积的方式进行 mask。</p>
<p><img src="/CodeFormer/image-20230208205107336.png" alt="image-20230208205107336" style="zoom:50%;"></p>
<p>&emsp;&emsp;上图表示了卷积核的卷积过程，对于黑色位置，卷积核只能看到灰色区域内的像素点，得到红色的像素分布并以此预测黑色位置像素，卷积核具体设计为下图：</p>
<p><img src="/CodeFormer/image-20230208205307160.png" alt="image-20230208205307160" style="zoom:50%;"></p>
<p>&emsp;&emsp;因为之后再 VQGAN 之中 PixelCNN 就使用 GPT-2 的 transformer 架构代替了，因此不再专门精读 PixelCNN。</p>
<p>&emsp;&emsp;下图展示了使用 IN 训练的 PixelCNN 和 VQVAE 以不同的类别采样的结果，可以看到还是比较差的，只能说是初具雏形。</p>
<p><img src="/CodeFormer/image-20230208205636427.png" alt="image-20230208205636427"></p>
<h4 id="VQGAN"><a href="#VQGAN" class="headerlink" title="VQGAN"></a>VQGAN</h4><p><a href="http://arxiv.org/abs/2012.09841">Taming Transformers for High-Resolution Image Synthesis</a>  CVPR2021</p>
<p><img src="/CodeFormer/image-20230208205940844.png" alt="image-20230208205940844"></p>
<p>&emsp;&emsp;VQGAN 针对 VQVAE 进行了两方面的改进：</p>
<ul>
<li>在原本的 AE 大框架下加入了判别器 D</li>
<li>使用 transformer 架构代替了 PixelCNN</li>
</ul>
<p>&emsp;&emsp;相对于 VQVAE，VQGAN 在训练阶段只是多了一个 GAN 损失，其中量化损失描述为：</p>
<script type="math/tex; mode=display">
L_{VQ}(E,G,Z)=||x-\hat x^2||^2+||sg[E(x)]-z_q||_2^2+||sg[z_q]-E(x)||_2^2</script><p>&emsp;&emsp;其中，第一项 MSE 即为重建损失 $L_{rec}$，后两项和 VQVAE 一致。</p>
<p>&emsp;&emsp;VQGAN 对抗（交叉熵损失）损失描述为：</p>
<script type="math/tex; mode=display">
L_{GAN}(\{E,G,Z\},D)=\log D(x)+\log(1-D(\hat x))</script><p>&emsp;&emsp;总损失描述为：</p>
<script type="math/tex; mode=display">
L_Q=arg\min_{E,G,Z}\max_D\mathbb E_{x\sim p(x)}[L_{VQ}+\lambda L_{GAN}]\\
\mathrm {where}\ \  \lambda=\frac{\nabla_{G_L}[L_{rec}]}{\nabla_{G_L}[L_{GAN}]+\delta}</script><p>&emsp;&emsp;其中，$\nabla_{G_L}$ 表示损失关于 G 的最后一层的梯度，$\delta=10^{-6}$。</p>
<p>&emsp;&emsp;对于采样部分，transformer 的训练方式为：对已经量化的序列进行预测，使用交叉熵函数学习如何把 codebook 映射到合理的分布，假设量化之后的序列为 $s$，则 transformer $p$ 预测序列 $s$ 可以表述为： $p(s)=\Pi_ip(s_i|s&lt;i)$，训练损失描述为：</p>
<script type="math/tex; mode=display">
L_{transformer}=\mathbb E_{x\sim p(x)}[-\log p(s)]</script><p>&emsp;&emsp;即对所有的 $x$ 采样之后预测出所有的 codebook 序列。</p>
<p>&emsp;&emsp;需要注意的是，对于 VQGAN，其 transformer 需要预测的序列仍然较长，为了能够生成更高分辨率的图像，需要对 transformer 进行滑动窗口的方式进行预测，其示意图如下：</p>
<p><img src="/CodeFormer/image-20230208215258931.png" alt="image-20230208215258931"></p>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>基于 transformer 的全局感知能力保证了更好的重建质量和保真度</li>
<li>可控特征变换模块，以此解决 codebook 带来的多样性降低问题</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/CodeFormer/image-20230208214441190.png" alt="image-20230208214441190"></p>
<p>&emsp;&emsp;在本文中，一些符号被重新命名，HQ 输入图像记为 $I_h$，编码器，解码器，判别器分别为 $E,\ G,\ D$，通过编码器之后的隐编码记为 $Z_h\in\mathbb R^{m\times n\times d}$，码表记为 $\mathcal C =\{c_k\in\mathbb R^d\}_{k=0}^N$，量化之后的隐编码记为 $Z_c\in\mathbb R^{m\times n\times d}$，量化之后的码表下标序列为 $s\in\{0,…,N-1\}^{m\cdot n}$，通过解码器之后的重建 HQ 图像记为 $I_{rec}$。</p>
<p>&emsp;&emsp;整个网络结构很好理解，类比于 VQGAN，采用了三阶段的训练方式，<strong>第一阶段</strong>训练 VQVAE，如上图 (a) 所示，训练的损失函数为：</p>
<script type="math/tex; mode=display">
L_{codebook}=L_1+L_{per}+L_{code}+\lambda_{adv}\cdot L_{adv}</script><p>&emsp;&emsp;其中，$L_1$ 是重建损失，使用 L1Loss，$L_{per}$ 是感知损失，使用 VGG19 计算，$L_{code}$ 是码表量化损失，按照 $||sg[Z_h]-Z_c||_2^2+\beta||sg[Z_c]-Z_h||_2^2$ 计算，$L_{adv}$ 是对抗损失，按照 $\log D(I_h)+\log(1-D(I_{rec}))$ 计算，$\lambda_{rec}=0.8$。</p>
<p>&emsp;&emsp;<strong>第二阶段</strong>训练 transformer，具体来说，将 LQ 的输入 $I_l$ 进入编码器之后的隐编码 $Z_l\in\mathbb R^{m\times n\times d}$ 拉平为向量 $Z_l^v\in\mathbb R^{(m\cdot n)\times d}$，然后输入标准的 transformer 结构，位置编码使用 sin 相对编码，预测出的结果为用于重建 HQ 图像的 codebook 下标序列 $\hat s$，训练过程的损失函数为：</p>
<script type="math/tex; mode=display">
L_{tf}=\lambda_{s}\cdot L_{s}+L_{code'}</script><p>&emsp;&emsp;其中，$L_s$ 即为预测出的 $\hat s$ 和真实 HQ 图像的量化序列 $s$ 的交叉熵损失，$\sum\limits_{i=0}^{mn-1}-si\log(\hat s_i)$，$L_{code’}$ 即为在 LQ 预测下的量化损失，其只用来优化 $E_L$，$L_{code’}=||Z_l-sg[Z_c]||_2^2$。</p>
<p>&emsp;&emsp;编解码器由 12 个残差模块和 5 个上下采样模块组成，因此压缩比为 $32^2$，对于 codebook，$N=1024,\ d=256$，这个序列长度允许使用 transformer 直接进行全局建模。</p>
<p>&emsp;&emsp;<strong>第三阶段</strong>参照 <a href="https://arxiv.org/pdf/1804.02815.pdf">SFTGAN</a> 的 SFTlayer 设计，具体如上图所示，其中淡淡色的方块是卷积的组合，$\alpha,\ \beta$ 相当于 SFT 的 $\gamma,\ \beta$，相当于 style，通过乘和加的操作实现了仿射变换，即风格嵌入，这些淡淡色的方块的目标即为融合 $F_e$ 和 $F_d$ 的风格。</p>
<p>&emsp;&emsp;第三阶段的训练过程是在已经训练好的二阶段模型上进行微调，前向过程中调整的是解码器 $D_H$ 每一层的特征，具体来说对于某个层的特征 $F_d$，有，$\hat F_d=F_d+(\alpha\odot F_d+\beta)\times w$，其中 $\alpha,\beta=\mathcal {P}_\theta(c(F_d,F_e))$，其中，$\cal P$ 表示一系列的卷积组合。其损失函数是一二阶段的损失函数之和（除去和 code 相关的部分），梯度回传时更新除了码表之外的全部。</p>
<p>&emsp;&emsp;在训练的过程中全部设置 $w=1$ 以保证模型学习到融合特征的能力，在推理的过程中设置 $w=0$ 能够最好地保证重建质量，在 $[0,1]$ 之间的调整则可以产生连续变化的生成结果，增加了多样性。</p>
<p><img src="/CodeFormer/image-20230209100444849.png" alt="image-20230209100444849"></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;本文首先在 Celeb-A，LFW，WebPhoto，WIDER 数据集上分别测试了性能，以 LPIPS，FID，NIQE，IDS，PSNR，SSIM 作为指标显示了模型优越的性能，对比的模型中不包括 VQGAN</p>
<p><img src="/CodeFormer/image-20230209100801049.png" alt="image-20230209100801049"></p>
<p>&emsp;&emsp;另一方面本文测试了和 VQGAN 在预测 codebook 序列上的正确性，由于 transformer 结构，准确度提升了很多（但是这里的 transformer 就是普通的 transformer，是否可以针对 transformer 的架构进行改进？）</p>
<p><img src="/CodeFormer/image-20230209100935785.png" alt="image-20230209100935785"></p>
<p>&emsp;&emsp;最后本文还论证了在第二阶段锁定解码器的必要性，通过测试是否在第二阶段更新解码器，得出结论：在第二阶段微调解码器会破坏学习到的先验知识。然后将本文的网络应用于了人脸图像修补，人脸图像上色等任务，同样取得了不错的结果。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>Contrast-Phys</title>
    <url>/Contrast-Phys/</url>
    <content><![CDATA[<h2><center> Contrast-Phys: Unsupervised Video-based Remote Physiological Measurement via Spatiotemporal Contrast </center></h2>

<p>【心率监测】【ECCV2022】【<a href="http://arxiv.org/abs/2208.04378">paper</a>】【<a href="https://github.com/zhaodongsun/contrast-phys">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文实现了一种新的对比学习框架，提出了一种新的数据增强策略，这个策略基于对同一人脸不同位置的 ppg 信号 PSD 基本一致的观察，以及对不同人脸的 ppg 信号 PSD 基本不一致的观察，并且使用一个 3DCNN 块对 ppg 信号进行提取，得到 channel-wise 的 ppg 信号，这种方式极大地扩展了正负样本的数量，时间上这篇文章早于 video-base SSL，但是效果实际上不相上下。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Contrast-Phys/image-20221205100842471.png" alt="image-20221205100842471" style="zoom:60%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>观察到了 rppg 信号的时空相似性和跨视频不相似性等，基于这些观察提出了新的样本提取方法</li>
<li>改编 physnet 得到了一个新的 3DCNN 网络，能够以 channel-wise 的方式对 rppg 信号建模，每个 channel 具备较大的感受野</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><h4 id="前置观察"><a href="#前置观察" class="headerlink" title="前置观察"></a>前置观察</h4><ul>
<li><strong>rppg 信号具备空间相似性</strong>，也就是说将每个视频划分为多个子块，每个子块估计出的 rppg 信号在相位和振幅上或许会有差别，但在频率域上（PSD 信号）应当基本一致</li>
</ul>
<p><img src="/Contrast-Phys/image-20221205110212520.png" alt="image-20221205110212520"></p>
<ul>
<li><strong>rppg 信号具备时间相似性</strong>，也就是说将 T 帧的数据估计出的长度为 T 的 rppg 信号一分为二，则这两个子信号在相位和振幅上或许会有差别，但在频率域上（PSD 信号）应当基本一致<ul>
<li>需要注意的是，这里显然在 T 过大时不成立，在较大的 T 下，近似于每个人时刻都保持 HR 一致，这显然不对；同时多次分割也会降低这个假设的准确性，文章在最后探索了 T 的大小对该假设成立的影响</li>
</ul>
</li>
</ul>
<p><img src="/Contrast-Phys/image-20221205105522714.png" alt="image-20221205105522714"></p>
<ul>
<li><strong>rppg 信号具备跨视频不一致性</strong>，也就是说在不同的视频中不仅振幅、相位、周期可能不同，尤其在于 PSD 应当不同，即使两个视频具备相同的心率（PSD 最大值点横坐标），也不太可能具备基本一致的 PSD 图像<ul>
<li>下图是在 OBF 数据集中 PSD 之间 MSE 最小的两个视频（左）和最大的两个视频（右）的 PSD 曲线对比</li>
<li>这个图确实可以看出不同人之间的 PSD 信号不同，但是我觉得应该再放一个最大值横坐标（HR）一致但曲线明显不同的例子更具备说服力，不过我想到的 oulu 肯定也想得到，没放只能说 DDDD</li>
</ul>
</li>
</ul>
<p><img src="/Contrast-Phys/image-20221205110035408.png" alt="image-20221205110035408" style="zoom:50%;"></p>
<ul>
<li><strong>rppg 信号的频率应具备一定的区间</strong>，这个结论算是前置知识了，不过每个文章对这个区间的限定很主观随心，比如这篇文章就是 40-250 bpm，因此 PSD 只选取 0.66-4.16 Hz，而 physnet 则是 40-180 bpm，不太统一</li>
</ul>
<h4 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h4><p><img src="/Contrast-Phys/image-20221205100842471.png" alt="image-20221205100842471" style="zoom:60%;"></p>
<p>&emsp;&emsp;前向过程中同时输入两个 video，首先使用 openface 进行人脸关键点标定和裁剪，裁剪时有几个 trick：1. 对所有的 video，仅处理第一帧的关键点标定，随后固定裁剪大小，2. 裁剪的 crop 大小由最高和最低的 landmark 坐标确定，具体来说是最高和最低相对于中心位置扩大 1.2 倍，然后按照这个长度进行正方形的 crop，3. 裁剪之后的图像全部被缩放到 128x128 用于后续处理。</p>
<p>&emsp;&emsp;处理之后的 video 分别进入同一个 3DCNN 网络，接着输出的并非 Bx1xT，而是 BxNxT（B=2），也就是说总共输出的块中包括了 N 个 rppg 信号（N = SxS），我们描述该块的 shape 为 TxSxS。这 N 个通道每个通道都意味着从某个感受野获得的 rppg 信号估计，而当 N 足够小或网络足够大时，可以近似认为每个通道都包含了足够识别出准确 rppg 信号的感受野，这些感受野之间按照 <strong>rppg 信号具备空间相似性</strong> 的观察，应当具备同样的 PSD。</p>
<p>&emsp;&emsp;接着对 N 个通道分别做随机选取等长的 rppg 信号段，选择示意图如下，具体方法为：</p>
<ul>
<li>首选对 N 个通道分别取出 N 个长度为 T 的 rppg 信号，然后对每个信号：<ul>
<li>将其从任意的 $t$ 帧开始选出长度为 $\Delta t$ 的信号，整个过程重复 K 次，按照 <strong>rppg 信号具备时间相似性</strong> 的观察，这些信号也应具备基本一致的 PSD</li>
</ul>
</li>
</ul>
<p><img src="/Contrast-Phys/image-20221205122158452.png" alt="image-20221205122158452" style="zoom:67%;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ST_sampling</span>(nn.Module):</span><br><span class="line">    <span class="comment"># spatiotemporal sampling on ST-rPPG block.</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, delta_t, K, Fs, high_pass, low_pass</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.delta_t = delta_t <span class="comment"># time length of each rPPG sample</span></span><br><span class="line">        self.K = K <span class="comment"># the number of rPPG samples at each spatial position</span></span><br><span class="line">        self.norm_psd = CalculateNormPSD(Fs, high_pass, low_pass)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>): <span class="comment"># input: (2, N, T)</span></span><br><span class="line">        samples = []</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">input</span>.shape[<span class="number">0</span>]): <span class="comment"># loop over videos (totally 2 videos)</span></span><br><span class="line">            samples_per_video = []</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">input</span>.shape[<span class="number">1</span>]): <span class="comment"># loop for sampling over spatial dimension</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.K): <span class="comment"># loop for sampling K samples with time length delta_t along temporal dimension</span></span><br><span class="line">                    offset = torch.randint(<span class="number">0</span>, <span class="built_in">input</span>.shape[-<span class="number">1</span>] - self.delta_t + <span class="number">1</span>, (<span class="number">1</span>,), device=<span class="built_in">input</span>.device) <span class="comment"># randomly sample along temporal dimension</span></span><br><span class="line">                    x = self.norm_psd(<span class="built_in">input</span>[b, c, offset:offset + self.delta_t])</span><br><span class="line">                    <span class="comment"># print(f&#x27;x.shape = &#123;x.shape&#125;&#x27;)</span></span><br><span class="line">                    samples_per_video.append(x)</span><br><span class="line">                <span class="comment"># print(f&#x27;samples_per_video.len = &#123;len(samples_per_video)&#125;&#x27;)</span></span><br><span class="line">            samples.append(samples_per_video)</span><br><span class="line">        <span class="comment"># print(f&#x27;samples.len = &#123;len(samples)&#125;&#x27;)</span></span><br><span class="line">        <span class="keyword">return</span> samples <span class="comment"># (2, NxK, t) 其中 t 是 PSD 信号过滤后的长度，和具体计算过程有关</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;因此共选出 2xKxN 个 rppg 信号，这些信号分别求 PSD，得到的就是 2xKxN 个 正/负 样本。对同一个 video 选出来的样本之间进行拉进，对不同的则拉远。</p>
<p>&emsp;&emsp;需要特别注意的是，这里虽然是 2xKxN 个长度为 $\Delta t$ 的信号，但是无论网络 backbone 图还是接下来对 loss 的描述，统统只描述了 N，那么<strong>接下来的 N 就是 NxK</strong>，这一点从上述代码的返回值也可以大致看出，不知道为啥不统一标准。但总之提取出了 2N 个 rppg 信号 $p_i\ /p_i’\ ,\ i\in1\to N$，这些信号计算 PSD 之后又变成了 2N 个 PSD 信号 $f_i\ /f_i’\ ,\ i\in1\to N$ 。</p>
<p>&emsp;&emsp;而模型在推理过程中的最终输出就是将一个 video 得到的 N 个 rppg 信号直接进行取平均值即可。</p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>&emsp;&emsp;经过前向，总共得到了从两个 video 中采样出的 2xN（再次提醒，这里的以及之后的 N 统统表示上面的 NxK）个 PSD 信号样本 $f_i\ /f_i’\ ,\ i\in1\to N$，从同一个 video 中采样的 N 个样本互为正样本，而来自不同 video 的样本则互为负样本，本文对任意两个正样本和任意两个负样本都计算 loss，具体的计算方法为：</p>
<ul>
<li>正样本损失：对两组正样本，每组 N 个样本之中进行 $\mathbb C_N^2$ 操作，选出来的正样本计算 MSE，然后取平均，总共计算了 $\rm 2N(N-1)$ 次</li>
</ul>
<script type="math/tex; mode=display">
L_p=\sum_{i=1}^N\sum_{j=1,j\neq i}^N \frac{||f_i-f_j||^2+||f_i'-f_j'||^2}{2N(N-1)}</script><ul>
<li>负样本损失：对正样本和负样本，任意两个之间计算 -MSE，然后取平均，总共计算了 $\rm N^2$ 次</li>
</ul>
<script type="math/tex; mode=display">
L_n=-\sum_{i=1}^N\sum_{j=1}^N\frac{||f_i-f_j'||^2}{N^2}</script><p>&emsp;&emsp;整体损失为：$L=L_p+L_n$，没有平衡项。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;本文做了详实的实验，包含数据集内测试、跨数据集测试、心率测试、其他生理指标测试、显著图测试、训练时间-IPR 实验、抗噪音实验。</p>
<p>&emsp;&emsp;首先是数据集内的测试，本文总共测了 UBFC，PURE，OBF，MR-NIRP 四个数据集，前两个数据集相对简单，对于无监督任务基本都会在这两个数据集上做测试，毕竟 SSL 在简单的任务上更能取得好的效果，而 OBF 是包含心房颤动病人的数据集，更偏临床，MR-NIRP 则包含了额外的红外信息，在一定程度上可以说明模型的泛化能力。这个测试时主要指标，可以对比 video based XXX 那篇看。</p>
<p><img src="/Contrast-Phys/image-20221205133230644.png" alt="image-20221205133230644" style="zoom:50%;"></p>
<p>&emsp;&emsp;在其他生理指标测试中，生理指标主要对比 the way to my heart，具体如下</p>
<p><img src="/Contrast-Phys/image-20221205142321191.png" alt="image-20221205142321191" style="zoom: 50%;"></p>
<p>&emsp;&emsp;同时包括了训练时间的测试，纵坐标为 irrelevant power ratio（无关功率比），其值越小说明信号质量越高（偏小众的指标），在跨数据集测试中，跨数据集测试为 UBFC $\to$ MMSE-HR，区别于常见的 UBFC 和 PURE 的互相转化，大概能猜到在 UBFC 和 PURE 上的迁移性差一些吧</p>
<p><img src="/Contrast-Phys/image-20221205142537429.png" alt="image-20221205142537429" style="zoom:50%;"></p>
<p>&emsp;&emsp;所谓显著性测试，指的是使用一种基于梯度的方法获得显著性图 <a href="https://arxiv.org/pdf/1312.6034.pdf">ref</a>，该图度量了模型更加关注的区域，因此在一定程度上可以看出模型是否学到了关键信息，并且对于噪音是否具备鲁棒性。左图在 UBFC 数据集上加入了一个随机闪烁区域，然后可以发现 the way to my heart 直接 G 了，说明对噪音太不鲁棒，右图在 PURE 中选择了旋转的 clip，可以发现本文的模型旋转的时候也能很好地捕捉头部运动（相比于 the way to my heart），总之就是噪声鲁棒，原因很简单，毕竟本文最终的结果是多感受野平均得到的，自然对单区域的噪音鲁棒，如果在 clip 中加入 N 个随机闪烁的区域估计这个模型也危险了。</p>
<p><img src="/Contrast-Phys/image-20221205142732306.png" alt="image-20221205142732306" style="zoom:50%;"></p>
<p>&emsp;&emsp;最后是消融实验，这里的消融实验主要测试的是针对两个假设的设定：1. 不同的 ST block 的空间分辨率，2. 不同的 $\Delta t$ 大小。用以证明模型对该设置灵敏性较高，该设置较为有效。</p>
<p><img src="/Contrast-Phys/image-20221205143618001.png" alt="image-20221205143618001" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>DRNet</title>
    <url>/DRNet/</url>
    <content><![CDATA[<h2><center> DRNet: Decomposition and Reconstruction Network for Remote Physiological Measurement </center></h2>

<p>【心率监测】【arxiv】【<a href="http://arxiv.org/abs/2206.05687">paper</a>】【<a href>code未开源</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种非端到端的 bvp 信号估计网络，其基本思想来自 CVD 和 DualGan，可以简要概括为：将生理信号和非生理信号进行分离，并且通过互相组合使 rppg 估计器能够对各种噪声都具备鲁棒性。其整个网络基于两个无法证明的假设：1. STMap 的生理信号和非生理信号是线性组合的，2. 。同时本文提出了一种数据增强策略（仅针对 STMap）、一个即插即用的注意力模块（仅针对 STMap）、一种新的损失函数（并不新），在 VIPL、PURE、UBFC 上基本全指标超了 Dual-GAN，是新的 SOTA，暂未开源。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/DRNet/image-20221124131215819.png" alt="image-20221124131215819"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>提出了一种数据增强方法，专用于 STMap 的增强</li>
<li>非生理信号的产生是由对生理信号建模后作差得到的</li>
<li>提出了一种即插即用的注意力模块 SAB，可以使 STMap 中生理信号的显著性更高</li>
<li>打败了 Dual-GAN，成为了新的 SOTA</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p><img src="/DRNet/image-20221124132742293.png" alt="image-20221124132742293" style="zoom: 50%;"></p>
<p>&emsp;&emsp;本文提出的新的数据增强策略名字为 Patch Cropping（PC），目的是合成具备不同噪声水平的新样本。通过上图可以看出来，所谓数据增强，增强的并不是原始视频或者图像，而是生成了新的 STMap，首先简单回顾原始 STMap：将原始 clip 分成 N 个 ROI，每个 ROI 逐帧地 pooling 出一个像素值，得到的 shape 为 CxNxT，其中 C 是 RGB/YUV，N 为 ROI 的个数，T 为帧数。而 PC 具体来说其做法如下：</p>
<ul>
<li>首先将原本的每个 ROI 分成更细的粒度，即将 N 个 ROI 变成 $\rm N\times \gamma\times\gamma$ 个 ROI</li>
<li>据此生成的 STMap 的shape 为 CxN’xT，$\rm N’=N\times \gamma\times\gamma$</li>
<li>从 $\rm N ‘$ 中随机裁剪 $\rm N$ 个序列返回，并且用另一个超参 $\rho$ 用来控制是否进行增强的概率</li>
<li>具体来说，$\gamma=2,\ \rho=0.5$</li>
</ul>
<p>&emsp;&emsp;数据增强过程的伪代码如下</p>
<p><img src="/DRNet/image-20221124142339598.png" alt="image-20221124142339598" style="zoom:70%;"></p>
<h4 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h4><p><img src="/DRNet/image-20221124131210059.png" alt="image-20221124131210059" style="zoom: 45%;"></p>
<p>&emsp;&emsp;首先，这篇论文的网络结构基本接近于 CVD 和 Dual-GAN，借鉴 CVD，本文的训练目的也是解耦合生理信号和非生理信号，区别在于：1. 非生理信号的生成：CVD 通过训练两个自编码器分别预测生理信号和非生理信号，而本文则只预测生理信号，通过全部信号和生理信号作差表示费生理信号，2. 单独信号预测器的工作保证：CVD 采用交叉解耦训练的策略，通过拉进或推离最终的信号距离保证每个 AE 各司其职，而本文因为只训练了生理信号，通过使生理信号具备周期性保证其信号确实为生理信号。至于 Dual-GAN 的借鉴，则主要在于 backbone，一会放个对比图。</p>
<p>&emsp;&emsp;在上图之中提到的 PixelMap，其来源如下图所示，将每个 ROI 提取出的生理信号直接按行排列得到的就是 PixelMap，而那个 Magnifing Operation 操作就是一个最大-最小归一化，也就是说 PixelMap 就是还没有做归一化的 STMap，他们带有的信息没有区别，也没必要刻意区分，这里做这种区分大概只是网络实现过程中发现的 trick。</p>
<p><img src="/DRNet/image-20221124144029559.png" alt="image-20221124144029559" style="zoom:67%;"></p>
<p>&emsp;&emsp;接下来详细描述本文的前向过程：</p>
<ul>
<li>网络同时读取两个输入，分别记为 $v^1,\ v^2$，其 GT 分别记为 $s_{gt}^1,\ s_{gt}^2$，对第 $i$ 个输入，分别进行以下过程：<ul>
<li>将输入 $v^i$ 生成 PixelMap，记为 $pm^i$</li>
<li>将输入 GT $s_{gt}^2$ 通过生理信号生成器 $G_p$ 生成生理信号时空图 $pm_p^i$</li>
<li>将整体信号时空图减去生理信号时空图得到非生理信号时空图：$pm_{np}^i=pm^i-pm_p^i$</li>
<li>同时将 PixelMap $pm^i,\ pm_p^i$ ，归一化之后生成 STMap，分别记为：$m^i,\ m_p^i$</li>
</ul>
</li>
<li>将两个输入的生理信号和非生理信号互相求和得到伪信号，这一步是为了模拟在 $v^1$ 中加入 $v^2$ 的噪声分别生成：<ul>
<li>$pm_{pse}^1=pm_p^1+pm_{np}^2$，其记录了 $v^1$ 的生理信息</li>
<li>$pm_{pse}^2=pm_p^2+pm_{np}^1$，其记录了 $v^2$ 的生理信息</li>
<li>将两个 PixelMap 进行归一化生成 STMap，分别记为：$m_{pse}^1,\ m_{pse}^2$</li>
</ul>
</li>
<li>将前两步生成的所有 STMap 进入 rppg 估计器 $E_p$ 分别生成 BVP 预测信号<ul>
<li>具体地，对于 $m^i,\ m_p^i,\ m_{pse}^i$ 进入 $E_p$ 之后分别得到信号：$s^i,\ s_p^i,\ s_{pse}^i$</li>
</ul>
</li>
</ul>
<p>&emsp;&emsp;本文的损失函数限制两个部分，分别是：1. 生成信号和 GT 信号的一致性损失 $\cal L_{phy}$，用于训练 rppg 估计器的准确性和对噪声的鲁棒性 2. 生成的生理信号的周期性损失 $\cal L_{cyc}$，用于督促生成器的有效性。</p>
<p>&emsp;&emsp;其中，$\cal L_{phy}$ 用的是标准负皮尔森系数，$\cal L_{cyc}$ 用的是 AutoHR 的频率损失（其实就是 HR 交叉熵），其具体表达式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\mathcal L_{phy}=1-\frac{\sum_{t=1}^T(x^i-\bar x)(y^i-\bar y)}{\sqrt{\sum_{t=1}^T(x^i-\bar x)^2}\sqrt{\sum_{t-1}^T(y^i-\bar y)^2}}\\&\mathcal L_{cyc}=\frac 1 c\sum\limits_{i=1}^cCE(PSD(pm_{avg}^i),HR_{gt})
\end{aligned}</script><p>&emsp;&emsp;其中，$\bar x,\ \bar y$ 表示 $x,\ y$ 的平均，$c$ 代表 PixelMap $pm$ 的通道数，$pm_{avg}$ 是指随机从 $\rm N$ 中选择多次子序列的平均值。loss 的最终表达式为 $\cal L=L_{phy}+L_{cyc}$，两个损失之中不使用参数进行平衡（估计是测出来 $\lambda=1$ 更合适就直接不写了）</p>
<h4 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a>backbone</h4><p>&emsp;&emsp;下图左边是 Dual-GAN 的 backbone，右图是本文的 backbone，相似性无需多言。</p>
<p><img src="/DRNet/image-20221124162013710.png" alt="image-20221124162013710"></p>
<p>&emsp;&emsp;简单来说，基本没有什么变化，本文的 backbone 去除了 Dual-GAN 的辅助任务部分，这是因为 Dual-GAN 希望通过这一部分进行有监督的预训练从而使第二阶段的无监督训练更加快速，而本文没有无监督的部分自然也就没有预训练的需求。</p>
<p>&emsp;&emsp;除此之外，本文相对应 Dual-GAN 的 ROI-AF block，本文也提了一个 SAB，他们做法并不完全一致，具体来说，ROI-AF 是将 STMap 进行 frame-wise 的 1D 卷积，目的是将 BVP 和噪声的分布实现对齐从而有利于估计器分辨出噪声，而 SAB 是通过一系列的 2D 卷积，给每个位置加一个权重，让估计器能够更加容易关注到 STMap 中的生理信号部分。虽然卷积的维度不同，本质上目的是一样的，都是让估计器分辨噪声和生理信号更加容易。</p>
<p>&emsp;&emsp;其中 SAB（Spatial Attention Block）的结构如下图所示，可以看出虽然为了和 Dual-GAN 区分开加入了 2D 卷积，但是两个分支之中右面那个分支仍然是 frame-wise 的子模块结构。</p>
<p><img src="/DRNet/image-20221124163304582.png" alt="image-20221124163304582" style="zoom:50%;"></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;文章结果分为两个部分：1. 数据集内和跨数据集训练与测试，2. 消融研究</p>
<ul>
<li>文章效果</li>
</ul>
<p>&emsp;&emsp;这里他的效果基本完全超了 Dual-GAN，比 physformer 更好了不少，估计之后引用量不会太少，毕竟是实打实的 SOTA</p>
<p><img src="/DRNet/image-20221124164359344.png" alt="image-20221124164359344"></p>
<ul>
<li>消融实验</li>
</ul>
<p>&emsp;&emsp;本文的消融实验主要关于两个部分：1. 数据增强策略的有效性，2. SAB 的有效性。可以看出两个模块都是有用的，并且这感觉没这两个小策略，整个网络别说 Dual-GAN 了，连 AutoHR 都打不过。</p>
<p><img src="/DRNet/image-20221124164530532.png" alt="image-20221124164530532"></p>
<hr>
<h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3><p>&emsp;&emsp;本文算是典范的缝合 SOTA 论文了，网络的整体结构和训练目标来源于 CVD，backbone 的提出和即插即用块的提出来源于 Dual-GAN，损失函数来源于 AutoHR，稍微改了改就成了自己的。文章之中原话说他们提出了一种新的损失 $\cal L_{cyc}$，但是这表达式和 PhysNet、AutoHR 的 $\cal L_{fre}$ 完全一样，这不扯吗，非要说改动就是他随机取值然后算 loss。</p>
<p>&emsp;&emsp;但是这是模板，是榜样，是干活的方向和学习的目标，虽然整个文章的图像风格都和 Dual-GAN 一模一样，损失函数也照抄的 AutoHR，但是确实是 SOTA，不知道能不能中 CVPR。</p>
<p>&emsp;&emsp;特别是他的消融研究指出的，他的 baseline 其实效果相当一般，根本发不出来，正是通过一些模块的调优才达到的 SOTA，这大概也是一个思路，提出一个普通的 baseline 或者直接找一个 backbone，在不影响性能的情况下进行简单的改动以便于讲出不一样的故事，然后加入一些调优模块达到 SOTA 发出文章。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>Diffusion_Model</title>
    <url>/Diffusion_Model/</url>
    <content><![CDATA[<h2><center> Denoising Diffusion Probabilistic Models </center></h2>

<p>【图像生成】【NIPS2020】【<a href="https://arxiv.org/abs/2006.11239">paper</a>】【<a href="https://github.com/hojonathanho/diffusion">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文将介绍扩散模型（diffusion model）的开山作 DDPM，着重介绍其数学原理，并给出一个简单的实现。简单来看，DDPM 包含两个过程：1.扩散过程，即将原始图像逐步加入高斯噪声最终变成 $\mathcal N(0,I)$ 的过程，此过程无参数；2.逆扩散过程，即通过训练一个神经网络，将从 $\mathcal N(0,I)$ 中采样的 $\epsilon$ 逐层迭代还原至原始输入的过程，此过程含参数。 本质上，DDPM 就是将 VAE 进行了多层的扩展，区别在于：1.扩散过程中无参，2.模型的中间部分与原始输入 shape 保持一致；DDPM 和 VAE 同时都是使用最大似然估计的方式构造损失函数。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Diffusion_Model/image-20221118092337442.png" alt="image-20221118092337442" style="zoom: 80%;"></p>
<span id="more"></span>
<hr>
<h3 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h3><p><a href="https://zhuanlan.zhihu.com/p/525106459">参考1</a> <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#forward-diffusion-process">参考2</a> <a href="https://www.bilibili.com/video/BV1b541197HX/?spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=dcb058cda9a9a123a9dfe0dab7ab5116">参考3</a></p>
<h4 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h4><h5 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h5><ul>
<li>对于分布 $\mathcal N(\mu,\sigma^2)$，其概率密度函数为 $f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$</li>
</ul>
<h5 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h5><p><a href="https://blog.csdn.net/TonyShengTan/article/details/82814010">参考</a></p>
<ul>
<li><p>对于联合分布 $p(X=x,Y=y)$，我们通常简写为 $p(x,y)$</p>
</li>
<li><p>对于 $p(x,y)$，有 $p(x,y)=p(y|x)·p(x)$，即同时取得 $x,y$ 的概率是取得 $x$ 的概率乘以在给定 $x$ 下取得 $y$ 的概率，通常使用变式： $p(y|x)=\frac{p(x,y)}{p(x)}$ 表示 $y\ \ \mathrm {given}\ \  x $ 的概率（即已知 $x$ 求 $y$）</p>
</li>
</ul>
<h5 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h5><p><a href="https://zhuanlan.zhihu.com/p/134036707">参考</a></p>
<ul>
<li>贝叶斯公式主要阐述了更复杂情况下的条件概率</li>
<li>通常将 $p(x)$ 称为 $x$ 的先验概率，$p(x|y)$ 称为 $x$ 的后验概率</li>
<li>多元时，有 $p(x,y,z) = p(x|y,z)·p(y|z)·p(z)$</li>
</ul>
<h5 id="马尔科夫过程"><a href="#马尔科夫过程" class="headerlink" title="马尔科夫过程"></a>马尔科夫过程</h5><p><a href="https://zhuanlan.zhihu.com/p/35124726">参考</a></p>
<ul>
<li>马尔科夫过程即每个状态只与前一个状态有关</li>
<li>假如推理关系为 $z\to y\to x$，则 $p(x|y,z)=p(x|y)$（因为 $x$ 和 $z$ 之间被认为没有关系）</li>
</ul>
<h5 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h5><p><a href="https://zhuanlan.zhihu.com/p/55791843">参考</a></p>
<ul>
<li>所谓最大似然估计，即在 $p(\theta|x)$ 中通过输入 $x$ 解出分布函数参数为 $\theta$ 的可能性大小，是优化神经网络参数的关键，其本质可以理解为：</li>
</ul>
<script type="math/tex; mode=display">
\mathrm {Find} \ \ \  \theta^*=arg\max\limits_\theta\  p(x|\theta)</script><ul>
<li>对于 $p(\theta|x)$ ，在本文中使用 $p_\theta(x)$ 表示，也就是后面说的逆扩散过程的估计函数</li>
<li>通常对 $p_\theta(x)$ 的最大似然估计通过计算 $\log p_\theta(x)$ 的最大值得到</li>
</ul>
<h5 id="KL-散度"><a href="#KL-散度" class="headerlink" title="KL 散度"></a>KL 散度</h5><p><a href="https://zhuanlan.zhihu.com/p/438129018">参考</a></p>
<ul>
<li>所谓 KL 散度，使用 $D_{KL}$ 表示，是一个二元运算，其输入为两个分布 $p,\ q$，$D_{KL}(p,q)$ 用于度量 $p,\ q$ 的相似程度，值越小越相似，当值为 0 时分布完全一致</li>
</ul>
<p><img src="/Diffusion_Model/KL.gif" alt></p>
<ul>
<li>对于两个分布 $p,\ q$，通常来说公式可以描述为：</li>
</ul>
<script type="math/tex; mode=display">
D_{KL}(p,q)=\int p(\log p-\log q)=\mathbb E_p(\log p-\log q)</script><ul>
<li>而特殊情况下，若 $p, \ q$ 都是高斯分布，记 $p\sim\mathcal N(\mu_1,\sigma_1^2),\ q\sim\mathcal N(\mu_2,\sigma_2^2)$，此时他们的 KL 散度可以描述为：</li>
</ul>
<script type="math/tex; mode=display">
D_{KL}(p,q)=\log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac 1 2</script><h5 id="重参数技巧"><a href="#重参数技巧" class="headerlink" title="重参数技巧"></a>重参数技巧</h5><ul>
<li>所谓重参数技巧，指的是当我们需要执行这样的步骤的时候会出现问题：<ul>
<li>1.从输入 $x$ 通过网络 $\theta$ 计算一个分布（如正态分布 $\cal N$）的参数（如 $\mu_\theta,\ \sigma_\theta^2$）</li>
<li>2.然后通过在这个正态分布中进行随机采样，将采样后的值输入到网络的下一层</li>
</ul>
</li>
<li>这里会出现在 1 之后梯度消失的问题，即回传的 2 中采样的梯度无法进一步回传到 1 中的 $x$ 身上</li>
<li>为了解决这个问题，我们不改变步骤 1，但是对于步骤二，我们不直接从 $s\sim\mathcal N(\mu_\theta,\sigma_\theta^2)$ 采样，而是从 $\mathcal N(0,I)$ 中采样 $\epsilon$，再计算 $s=\mu_\theta+\sigma_\theta·\epsilon$，这样可以允许梯度回传到 $x$ 中</li>
<li>为了证明 $\mu_\theta+\sigma_\theta·\epsilon\sim\mathcal N(\mu_\theta,\sigma_\theta^2)$，推理过程如下：</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
&\because \forall X\sim\mathcal N(\mu_1,\sigma_1^2),\ \forall Y\sim\mathcal N(\mu_2,\sigma_2^2) \ \ \ \ \ s.t. \ \ \ \ aX+bY\sim\mathcal N(a\mu_1+b\mu_2,a^2\sigma_1^2+b^2\sigma_2^2)\\
又&\because\mu_\theta\sim\mathcal N(\mu_\theta,0),\ \sigma_\theta\cdot\epsilon\sim\mathcal N(0,\sigma_\theta^2I)\\
&\therefore\mu_\theta+\sigma_\theta·\epsilon\sim\mathcal N(\mu_\theta,\sigma_\theta^2)
\end{aligned}</script><h5 id="VAE-的优化"><a href="#VAE-的优化" class="headerlink" title="VAE 的优化"></a>VAE 的优化</h5><p><a href="http://www.gwylab.com/note-vae.html">参考1</a> <a href="https://blog.csdn.net/qq_42753940/article/details/124653866">参考2</a> <a href="https://blog.csdn.net/cjh_jinduoxia/article/details/84995156">参考3</a></p>
<h6 id="VAE-前向"><a href="#VAE-前向" class="headerlink" title="VAE 前向"></a>VAE 前向</h6><p><img src="/Diffusion_Model/image-20221117135708958.png" alt="image-20221117135708958" style="zoom:30%;"></p>
<ul>
<li>VAE 的过程描述为：已知图像 $x\sim p(x)$，通过 Encoder 层获得一个隐变量 $z\sim q(z)$，再通过 Decoder 层从 $z$ 中得到 $x$</li>
<li>通过 AE 的联合训练获得这样的网络，然后 sample $z$ 之后通过 Decoder 进行生成</li>
<li>我们将用于估计 $z$ 分布的函数称为 $\phi$，从 $x$ 中估计 $z$ 分布记为 $q_\phi(z|x)$，同样地，从 $z$ 中估计 $x$ 分布记为 $p_\theta(x|z)$</li>
</ul>
<p><img src="/Diffusion_Model/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NqaF9qaW5kdW94aWE=,size_16,color_FFFFFF,t_70.png" alt="img" style="zoom:50%;"></p>
<p>&emsp;&emsp;上图是李宏毅的 VAE 概述，在这之中没有经过太多的数学推理，而是直接从 NN 的角度给了结论。结论是：</p>
<ul>
<li><p>NN_Encoder 将输入 input（也就是 $x_i$） 经过网络之后输出均值 $m_i$ 和方差 $\sigma_i^2$</p>
<ul>
<li>注意：这里的方差并非 VAE 真正的选择，VAE 将其过了个 exp 以保证非负，也就是说 Encoder 输出的分布为 $\mathcal N(m_i,e^{\sigma_i^2})$</li>
</ul>
</li>
<li><p>NN_Decoder 以 $c_i$ 为输入，output（也就是生成的 $x_i$）为输出</p>
<ul>
<li>其中，$c_i$ 从 $\mathcal N(0,I)$ 中取出的 $e_i$ 进行计算：$c_i=e^{\sigma_i^2}\times e_i+m_i$</li>
</ul>
</li>
<li>Loss 分为两个部分，分别限制重构损失（类比于普通的 AE），和防止模型走捷径（下节 VAE 优化即证明了为何选择这两个 loss）</li>
</ul>
<script type="math/tex; mode=display">
\mathcal L_1=\rm MSE(input, output)\tag 1</script><script type="math/tex; mode=display">
\mathcal L_2=e^{\sigma^2}-(1+\sigma)+m^2\tag 2</script><h6 id="VAE-优化目标"><a href="#VAE-优化目标" class="headerlink" title="VAE 优化目标"></a>VAE 优化目标</h6><p>&emsp;&emsp;从宏观上看，VAE 的目标是从 $z$ 中生成尽可能和 $x$ 接近的分布，因此在数学上可以转化为 $p_\theta(x|z)$ 的最大似然估计问题。而在解决这个问题的过程中，我们需要引入一个新的分布，也就是 $q$ 分布（此时这个分布还没有参数，只是一个单纯的分布），接下来我们通过化简式子可以发现如何通过优化 $q$ 进而优化 $p$，从而我们尝试预测 $q$，也就给 $q$ 加上了参数 $\phi$。</p>
<p>&emsp;&emsp;其具体化简过程如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
最大似然估计:\log p_\theta(x)&=\int_zq_\phi(z|x)\log p_\theta(x)，将 \log p_\theta(x) 整体看成一个分布即可得到这一步\\
&=\int_zq_\phi(z|x)\log (\frac{p_\theta(x,z)}{p_\theta(z|x)})，由贝叶斯公式可得\\
&=\int_zq_\phi(z|x)\log (\frac{p_\theta(x,z)·q_\phi(z|x)}{q_\phi(z|x)·p_\theta(z|x)})，构造这一步是为了分理出KL散度\\
&=\int_zq_\phi(z|x)\log \frac{p_\theta(x,z)}{q_\phi(z|x)}+\int_zq_\phi(z|x)\log \frac{q_\phi(z|x)}{p_\theta(z|x)}，这里后面一项就是KL散度\\
&=\int_zq_\phi(z|x)\log \frac{p_\theta(x,z)}{q_\phi(z|x)}+D_{KL}(q_\phi(z|x)||p_\theta(z|x))\\
&\geq\int_zq_\phi(z|x)\log \frac{p_\theta(x,z)}{q_\phi(z|x)}，因为任意的KL散度\geq0
\end{aligned}\tag 3</script><p>&emsp;&emsp;因此我们得到了 $\log p_\theta(x)$ 的一个最小值（下界），为了使 $\log p_\theta(x)$ 最大，我们可以最大化这个最小值。接下来继续化简这个下界：</p>
<script type="math/tex; mode=display">
\begin{aligned}
最大化下界:L_{VLB}&=\int_zq_\phi(z|x)\log \frac{p_\theta(x,z)}{q_\phi(z|x)}\\
&=\int_zq_\phi(z|x)\log \frac{p_\theta(x|z)·p_\theta(z)}{q_\phi(z|x)}，为了进一步分离KL散度\\
&=\int_zq_\phi(z|x)\log \frac{p_\theta(z)}{q_\phi(z|x)}+\int_zq_\phi(z|x)\log p_\theta(x|z)，注意前面一项其实是一个KL散度的相反数\\
&=-\underbrace{D_{KL}(q_\phi(z|x)||p_\theta(z))}_{L0}+\underbrace{\int_zq_\phi(z|x)\log p_\theta(x|z)}_{L1}
\end{aligned}\tag 4</script><p>&emsp;&emsp;至此，我们发现其实 $L_{VLB}$ 可以由两个子式 $L_0,\ L_1$ 表示，为了最大化 $L_{VLB}$，可以最小化 $L_0$ 同时最大化 $L_1$。而在接下来的过程中，我们就会发现具体来说我们如何根据优化目标写出损失函数。</p>
<p>&emsp;&emsp;在优化 $L_0, \ L_1$ 之前，首先假设：</p>
<ul>
<li>对于 $z$，$z\sim p_\theta(z)= \mathcal N(0,I)$</li>
<li>对于 $q_\phi(z|x)$，我们认为 $x$ 中的每一个特定的 $x^i$, 有 $q_\phi(z|x^i)\sim\cal N$</li>
<li>记 $J$ 是 $z$ 的维度，因此总共有 $J$ 个点属于标准正态</li>
<li>记 $\mu,\ \sigma^2$ 是 $q_\phi(z|x)$ 的方差均值，$\mu_j,\ \sigma_j^2$ 是 $z$ 中第 $j$ 个点的方差均值</li>
</ul>
<p>&emsp;&emsp;因此 $L_0$ 可以优化为：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
最小化:L_0&=D_{KL}(q_\phi(z|x)||p_\theta(z))\\
&=\int_zq_\phi(z|x)(\log q_\phi(z|x)-\log p_\theta(z))\\
&=\int_zq_\phi(z|x)\cdot\log q_\phi(z|x)-\int_zq_\phi(z|x)\cdot\log p_\theta(z)\\
&=\int_z\mathcal N(z;\mu,\sigma^2)\log \mathcal N(z;\mu, \sigma^2)-\int_z\mathcal N(z;\mu,\sigma^2)\log \mathcal N(0,I)\\
&=(-\frac J 2\log 2\pi - \frac 1 2\sum\limits_{j=1}^J(1+\log \sigma_j^2) )- (-\frac J 2\log 2\pi-\frac 1 2\sum\limits_{j=1}^J(\mu_i^2+\sigma_i^2))\\
&=\frac 1 2\sum\limits_{j=1}^J(\mu_j^2+\sigma_j^2-\log \sigma_j^2-1)
\end{aligned}\tag 5</script><p>&emsp;&emsp;对于这里的 $L_0$ 中的 $\mu_j,\ \sigma_j^2$，其实有着一点点的小问题，也就是说这里的 $\sigma_j^2$ 是由网络预测得到的，但是这个值有可能为负数，其实避免也很简单，也就是将所有的 $\sigma_j^2$ 过一个 exp，那么如果将 $\sigma^2\to e^{\sigma^2}$，$L_0$ 将会变成：$L_0=\frac 1 2\sum\limits_{j=1}^J(\mu_j^2+e^{\sigma_j^2}-\sigma_j^2-1)$，接下来进一步去除常数，如果我们再换一下记号 $\mu\to m$，并且将目光从求和聚焦于某一次迭代：最终得到 $L_0^{(j)}=m^2+e^{\sigma^2}-(1+\sigma^2)$，而这正和式 $(2)$ 保持了一致。</p>
<p>&emsp;&emsp;而对于 $L_1$，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
最大化 :L_1&=\int_zq_\phi(z|x)\log p_\theta(x|z)\\
&=\mathbb E_{z\sim q}[\log p_\theta(x|z)]\\
&\simeq \frac 1 L\sum\limits_{l=1}^L\log p_\theta(x^{(i)}|z^{(i,l)})
\end{aligned}\tag 6</script><p>&emsp;&emsp;在上式中，最后一步的约等于由蒙特卡罗方法得到，其中 $x^{(i)}$ 是 VAE 的输入，其通过编码器也就是 $q_\phi(z|x)$ 得到一组 $\mu,\sigma$，从而允许我们从正态分布中采样 $L$ 个 $z$，将这些 $z$ 再输入解码器 $p_\theta(x|z)$ 得到新的输出 $p_\theta(x^{(i)}|z^{(i,l)})$，这里的最大似然 $\log p_\theta$ 目的就是让输出 $p_\theta(x^{(i)}|z^{(i,l)})$ 和输入 $x^{(i)}$ 更加接近，实际的操作中使用 MSELoss，因此 $L_1$ 可以重写成 $L_1=\mathrm {MSE}(x^{(i)}，\ p_\theta(x^{(i)}|z^{(i,l)}))$，即式 $(1)$。</p>
<p>&emsp;&emsp;<strong>综上，我们通过最大化似然函数求得了两个优化表示，并且从数学的角度上证明了两个优化表示就对应着在上一小节说明的两个损失函数，为 VAE 的损失提供了理论基础。</strong></p>
<h4 id="符号假设"><a href="#符号假设" class="headerlink" title="符号假设"></a>符号假设</h4><p><img src="/Diffusion_Model/image-20221117114810759.png" alt="image-20221117114810759"></p>
<ul>
<li>$x_0,\dots,x_T$：从初始状态到标准正态分布的特征图</li>
<li>$q$：扩散过程得到的分布，此过程不含参数</li>
<li>$p_\theta$：逆扩散过程得到的分布，$\theta$ 是网络参数</li>
<li>$\beta_0,\dots,\beta_T$： 常数，实际网络中采用 $\rm 0.0001\to0.02$ 的线性差值</li>
<li>$\alpha_t$：常数，值为 $\alpha_t=1-\beta_t$</li>
<li>$\bar \alpha_t$：常数，值为 $\bar \alpha_t=\Pi_{i=1}^t\alpha_i$</li>
</ul>
<h4 id="扩散过程"><a href="#扩散过程" class="headerlink" title="扩散过程"></a>扩散过程</h4><ul>
<li>扩散过程即为从 $x_0$ 逐步推断出 $x_T$ 的过程，其中从 $t-1\to t$ 的过程可以表示为得到分布 $q(x_t|x_{t-1})$</li>
<li>由于每一步的结果只和前一步有关，因此可以视为一个马尔科夫过程</li>
<li>整个扩散过程没有参数，目的是使最终的 $x_T\sim\mathcal N(0,I)$，而 $x_T$ 的分布可以由 $x_0,\ \beta_{0:T}$ 唯一确定，因此可以通过计算得出具体的 $T$</li>
</ul>
<h5 id="中间状态的分布表示"><a href="#中间状态的分布表示" class="headerlink" title="中间状态的分布表示"></a>中间状态的分布表示</h5><ul>
<li>对于第 $t$ 个状态，其分布表示为 $q(x_t|x_{t-1})$，这个无参过程的表达式为 $(7)$，其意义为：$\mu_t=x_t=\sqrt{1-\beta_t}x_{t-1},\ \sigma_t^2=\beta_tI$</li>
</ul>
<script type="math/tex; mode=display">
q(x_t|x_{t-1})=\mathcal N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)\tag 7</script><ul>
<li>对于 $0\to T$ 个状态，由马尔科夫过程，可以得出 $q(x_{1:T}|x_0)=\Pi_{t=1}^T q(x_t|x_{t-1})$，这也说明了所有过程的分布都是确定的</li>
</ul>
<h5 id="任意状态-t-的-x-t-与-x-0-beta-t-的关系"><a href="#任意状态-t-的-x-t-与-x-0-beta-t-的关系" class="headerlink" title="任意状态 $t$ 的 $x_t$ 与 $x_0,\ \beta_t$ 的关系"></a>任意状态 $t$ 的 $x_t$ 与 $x_0,\ \beta_t$ 的关系</h5><p>&emsp;&emsp;事实上，任何一个状态的分布 $x_t$ 都可以由 $x_0$ 和 $\beta_{0:t}$ 唯一确定。直觉上是很好理解的，由于扩散过程本质上是一个迭代的过程，而上一小节我们已经给出了 $x_{t-1}\to x_t$ 的递推，我们自然是可以总结出通项公式的，其推导过程如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
x_t&=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}z_1，其中z_1\sim\mathcal N(0,I)，这里用到了式(7)和重参数技巧，尤其注意 \sigma\&\sigma^2\\
&=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}z_1，这里只是简单的代换，为了接下来运算更方便\\
&=\sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_{t-1}}z_2)+\sqrt{1-\alpha_t}z_1，这里进一步展开 x_{t-1}，其中z_2\sim\mathcal N(0,I)\\
&=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{\alpha_t(1-\alpha_{t-1})}z_2+\sqrt{1-\alpha_t}z_1，化简\\
&=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}z_3，不理解请看重参数技巧的最后一步，其中z_3\sim\mathcal N(0,I)\\
&\cdots\cdots\\
&=\sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar \alpha_t}z，着重观察本公式第二、五行，是简单的递推改通项，其中z\sim\mathcal N(0,I)
\end{aligned}\tag 8</script><p>&emsp;&emsp;也就是说在网络的实现过程中，我们实际上式不需要逐层迭代来求 $x_t$ 的，而是可以直接根据已知的常数和 $x_0$ 计算得到 $x_t$ 的分布。</p>
<h5 id="为什么最终的-x-T-sim-mathcal-N-0-I"><a href="#为什么最终的-x-T-sim-mathcal-N-0-I" class="headerlink" title="为什么最终的 $x_T\sim \mathcal N(0,I)$"></a>为什么最终的 $x_T\sim \mathcal N(0,I)$</h5><p>&emsp;&emsp;由上一小节可以知道，对于最终的 $T$ 状态的分布，证明其在 $T$ 足够大时为标准正态分布：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\because x_T=\sqrt{\bar\alpha_T}x_0+\sqrt{1-\bar \alpha_T}z，其中x_0\sim q(x_0),\ z\sim\mathcal N(0,I)\\
&\because \bar\alpha_T=\Pi_{i=1}^T\alpha_i,\ \alpha_i=1-\beta_i<1\\
又&\because\beta_1<\cdots<\beta_T\\
&\therefore\alpha_1>\cdots>\alpha_T\\
&\therefore\lim_{T\to\infty}\bar\alpha_T\to0\\
&\therefore\lim_{T\to\infty}x_T\to z\\
&\therefore\lim_{T\to\infty}x_T\sim\mathcal N(0,I)
\end{aligned}</script><p>&emsp;&emsp;而在计算的过程中不可能真的算到无穷，因此 $T$ 可以使 $\bar\alpha_t$ 取得一个小数字的时候即可，因而通过我们对“小数字”的认定，就可以算出 $T$。</p>
<h4 id="逆扩散过程"><a href="#逆扩散过程" class="headerlink" title="逆扩散过程"></a>逆扩散过程</h4><ul>
<li>逆扩散过程即从标准正态分布采样出 $x_T$，通过预测函数 $p_\theta$ 反推出 $x_0$ 的过程，其从 $x_t\to x_{t-1}$ 的过程表示为 $p_\theta(x_{t-1}|x_t)$</li>
<li>同样由于每一步的结果只和前一步有关，因此可以视为一个马尔科夫过程</li>
<li>由于逆扩散过程的目标是尽可能还原扩散过程，因此我们可以假设在各个中间状态的特征都满足正态分布</li>
<li>整个逆扩散过程的每一步都是由同一个网络 $\theta$ 估计得到的，网络的输入是 $x_t,\ t$，输出是 $x_{t-1}$ 分布的 $\mu,\ \sigma$</li>
</ul>
<h5 id="中间状态的分布表示-1"><a href="#中间状态的分布表示-1" class="headerlink" title="中间状态的分布表示"></a>中间状态的分布表示</h5><ul>
<li>对于第 $t-1$ 个状态，其分布表示为 $p_\theta(x_{t-1}|x_t)$，这个过程的表达式为 $(8)$，其意义为：$\mu_{t-1}=x_{t-1}=\mu_\theta(x_t,t),\  \sigma_t^2=\Sigma_\theta$</li>
</ul>
<script type="math/tex; mode=display">
p_\theta(x_{t-1}|x_{t})=\mathcal N(x_{t-1};\mu_\theta(x_t,t),\Sigma_\theta)\tag 9</script><ul>
<li>对于 $T\to 0$ 个状态，由马尔科夫过程，可以得出 $p_\theta(x_{0:T}|x_T)=p_\theta(x_T)\Pi_{t=1}^T p_\theta(x_{t-1}|x_t)$，这也说明了所有过程的分布都是确定的</li>
<li>做一个简单的补充说明：$p_\theta(x_t)\sim\mathcal N(0,I)$，以及对于 $\Sigma_\theta$，文中使用常数进行赋值，并不学习 $\sigma_t^2$<ul>
<li>对于具体使用哪个常数赋值，本文尝试了：1.$\sigma_t^2=\beta_t$，<strong>2.$\sigma_t^2=\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t$</strong>，结果相差无几，1 很好理解，而 <strong>2 接下来会解释</strong></li>
</ul>
</li>
</ul>
<h5 id="后验扩散条件概率-q-x-t-1-x-t-x-0-的表示"><a href="#后验扩散条件概率-q-x-t-1-x-t-x-0-的表示" class="headerlink" title="后验扩散条件概率 $q(x_{t-1}|x_t,x_0)$ 的表示"></a>后验扩散条件概率 $q(x_{t-1}|x_t,x_0)$ 的表示</h5><p>&emsp;&emsp;我们希望通过 $x_t$ 直接在无参的情况下求得 $x_{t-1}$，但是很难求解 $q(x_{t-1}|x_t)$，不过如果再给我们一个 $x_0$ 直观上就可以实现了，因此我们尝试探索一下 $q(x_{t-1}|x_t,x_0)$，化简这个表达式的目的是直接从 $x_t$ 得到 $x_{t-1}$ 的均值，并且期望通过一个网络就能够预测所有的逆扩散过程，说白了就是写出 $p_\theta(x_{t-1}|x_t)$ 的期望输出用于构造损失。记 $q(x_{t-1}|x_t,x_0)\sim\mathcal N(x_{t-1};\tilde\mu_t(x_t,x_0),\tilde\beta_tI)$，则有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
求解后验概率q(x_{t-1}|x_t,x_0)&=q(x_t|x_{t-1},x_0)\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}，将分母乘过去，左右均为q(x_t,x_{t-1}|x_0)\\
&=q(x_t|x_{t-1})\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}，由马尔科夫过程可以去掉x_0\\
&=\mathcal N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)\cdot\frac{\mathcal N(x_{t-1};\sqrt{\bar\alpha_{t-1}}x_0,(1-\bar\alpha_{t-1})I)}{\mathcal N(x_t;\sqrt{\bar\alpha_t}x_0,(1-\bar\alpha_t)I)}，根据式(7)(8)进行替换\\
&\propto \exp(-\frac 1 2(\frac{(x_t-\sqrt{\alpha_t}x_{t-1})^2}{\beta_t}+\frac{(x_{t-1}-\sqrt{\bar\alpha_{t-1}}x_0)^2}{1-\bar \alpha_{t-1}}-\frac{(x_t-\sqrt{\bar\alpha_t}x_0)^2}{1-\bar \alpha_{t}})，按照高斯分布进行展开后化简即可\\
&=\exp(-\frac 1 2((\underbrace{\frac{\alpha_t}{\beta_t}+\frac{1}{1-\bar\alpha_{t-1}}}_{A})x_{t-1}^2-(\underbrace{\frac{2\sqrt{\alpha_t}}{\beta_t}x_t+\frac{2\sqrt{\bar\alpha_{t-1}}}{1-\bar\alpha_{t-1}}x_0}_B)x_{t-1}+\underbrace{\frac{x_t^2}{\beta_t}+\frac{\bar\alpha_{t-1}}{1-\bar\alpha_{t-1}}-\frac{(x_t-\sqrt{\bar\alpha_t}x_0)^2}{1-\bar \alpha_{t}}}_C))\\
\end{aligned}\tag {10}</script><p>&emsp;&emsp;注意到，$\forall a,b,ax^2+bx=a(x+\frac{b}{2a})^2+C$，又由高斯分布概率密度公式，式 $(10)$ 可以描述为 $e^{-\frac 1 2 A(x+\frac{B}{2A})^2}\cdot e^{C’}$， 因此对标高斯分布，可以得出：$\tilde\mu_t=-\frac {B}{2A}, \tilde\sigma_t^2=\frac 1 A$，化简之后为：$\tilde\mu_t=\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}x_t+\frac{\sqrt{\bar\alpha_{t-1}}(1-\alpha_t)}{1-\bar\alpha_t}x_0$，$\tilde \beta_t=\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\cdot\beta_t$。</p>
<p>&emsp;&emsp;对于上面的结果，我们发现 $\tilde\beta_{t}$ 就是个常数，也就对应了之前中间状态小节中的第二个常数的尝试值（加粗处）</p>
<p>&emsp;&emsp;又由式 $(8)$，得到 $x_0=\frac 1 {\sqrt{\bar\alpha_t}}(x_t-\sqrt{1-\bar\alpha_t}z_t)$，带入式 $(10)$，可得最终的化简结果：</p>
<script type="math/tex; mode=display">
\begin{cases}
\tilde\mu_t=\frac 1 {\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar\alpha_t}}z_t)\\
\tilde\sigma_t^2=\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\cdot\beta_t
\end{cases}\tag{11}</script><h5 id="最大似然估计目标函数-p-theta-及优化"><a href="#最大似然估计目标函数-p-theta-及优化" class="headerlink" title="最大似然估计目标函数 $p_\theta$ 及优化"></a>最大似然估计目标函数 $p_\theta$ 及优化</h5><p>&emsp;&emsp;和 VAE 基本保持一致，这里对 $p_\theta$ 的优化也是通过最大似然的方式寻找下界，然后最大化下界并分解为多个优化目标然后给出损失函数。在求解的过程略有不同，VAE 是最大化 $\log p_\theta(x)$，而 DDPM 是最小化 $-\log p_\theta(x)$（其实没啥区别），其具体的优化过程为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
最小化:-\log p_\theta(x_0)&\leq-\log p_\theta(x_0)+D_{KL}(q(x_{1:T}|x_0)||p_\theta(x_{1:T}|x_0))，因为KL散度一定>0\\
&=-\log p_\theta(x_0)+\mathbb E_{x_{1:T}\sim q}[\log \frac{q(x_{1:T}|x_0)}{p_\theta(x_{1:T}|x_0)}]，展开KL散度\\
&=-\log p_\theta(x_0)+\mathbb E_{x_{1:T}\sim q}[\log \frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})/p_\theta(x_0)}]，由贝叶斯公式和马尔科夫过程\\
&=-\log p_\theta(x_0)+\mathbb E_{x_{1:T}\sim q}[\log \frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})}+\log p_\theta(x_0)]，\log的展开\\
&=\mathbb E_{x_{1:T}\sim q}[\log \frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})}]，消项\\
&\leq \mathbb E_{x_{0:T}\sim q}[\log \frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})}]:=L_{VLB}
\end{aligned}\tag{12}</script><p>&emsp;&emsp;故而我们只需要最小化 $L_{VLB}$ 即可，而对于 $L_{VLB}$，有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
最小化:L_{VLB}&=\mathbb E_{x_{0:T}\sim q}[\log \frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})}]\\
&=\mathbb E_{x_{0:T}\sim q}[\log \frac{\Pi_{t=1}^T q(x_{t}|x_{t-1})}{p_\theta(x_T)\Pi_{t=1}^Tp_\theta(x_{t-1}|x_t)}]，由式(7)(8)代入后得到\\
&=\mathbb E_{x_{0:T}\sim q}[-\log p_\theta(x_T)+ \sum_{t=1}^T\log \frac{ q(x_{t}|x_{t-1})}{p_\theta(x_{t-1}|x_t)}]，将\log 内的求积放到外面求和\\
&=\mathbb E_{x_{0:T}\sim q}[-\log p_\theta(x_T)+ \sum_{t=2}^T\log \frac{ q(x_{t}|x_{t-1})}{p_\theta(x_{t-1}|x_t)}+\log \frac{ q(x_1|x_0)}{p_\theta(x_0|x_1)}]，将 t=1的情况拆分出来\\
&=\mathbb E_{x_{0:T}\sim q}[-\log p_\theta(x_T)+ \sum_{t=2}^T\log \frac{ q(x_{t}|x_0)\cdot q(x_{t-1}|x_t,x_0)}{p_\theta(x_{t-1}|x_t)\cdot q(x_{t-1}|x_0)}+\log \frac{ q(x_1|x_0)}{p_\theta(x_0|x_1)}]，贝叶斯公式和马尔科夫过程:q(x_t,x_{t-1}|x_0)\\
&=\mathbb E_{x_{0:T}\sim q}[-\log p_\theta(x_T)+ \sum_{t=2}^T\log \frac{ q(x_{t-1}|x_t,x_0)}{p_\theta(x_{t-1}|x_t)}+\sum_{t=2}^T\log \frac{ q(x_{t}|x_0)}{ q(x_{t-1}|x_0)}+\log \frac{ q(x_1|x_0)}{p_\theta(x_0|x_1)}]，拆分\\
&=\mathbb E_{x_{0:T}\sim q}[-\log p_\theta(x_T)+ \sum_{t=2}^T\log \frac{ q(x_{t-1}|x_t,x_0)}{p_\theta(x_{t-1}|x_t)}+\log \frac{ q(x_{T}|x_0)}{ q(x_{1}|x_0)}+\log \frac{ q(x_1|x_0)}{p_\theta(x_0|x_1)}]，连加拿到外面变成连乘后化简\\
&=\mathbb E_{x_{0:T}\sim q}[-\log p_\theta(x_T)+ \sum_{t=2}^T\log \frac{ q(x_{t-1}|x_t,x_0)}{p_\theta(x_{t-1}|x_t)}+\log  q(x_{T}|x_0)-\log p_\theta(x_0|x_1)]，将分式全部展开后消项\\
&=\mathbb E_{x_{0:T}\sim q}[\log\frac{q(x_{T}|x_0)}{p_\theta(x_T)}+ \sum_{t=2}^T\log \frac{ q(x_{t-1}|x_t,x_0)}{p_\theta(x_{t-1}|x_t)} -\log p_\theta(x_0|x_1)]，将\log  q(x_{T}|x_0)和第一项合并\\
&=\mathbb E_{x_{0:T}\sim q}[\log\frac{q(x_{T}|x_0)}{p_\theta(x_T)}+ \sum_{t=2}^T\log \frac{ q(x_{t-1}|x_t,x_0)}{p_\theta(x_{t-1}|x_t)} +\log \frac {q(x_0|x1,x_0)} {p_\theta(x_0|x_1)}]，因为q(x_0|x1,x_0)=1\\
&=\mathbb E_{x_{0:T}\sim q}[\log\frac{q(x_{T}|x_0)}{p_\theta(x_T)}+ \sum_{t=2}^T\log \frac{ q(x_{t-1}|x_t,x_0)}{p_\theta(x_{t-1}|x_t)}]，合并至一处\\
&=\mathbb  E_{x_{0:T-1}\sim q}[\underbrace{D_{KL}(q(x_{T}|x_0)||p_\theta(x_T))}_{L_0}]+ E_{x_{0,T}\sim q}[\sum_{t=1}^T\underbrace{D_{KL}( q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))}_{L_t}]，对第一项积 x_T，第二项积x_{1:T-1}
\end{aligned}\tag{13}</script><p>&emsp;&emsp;对于 $L_0$，其本质上在任一设置下属于确定的分布，这是因为在逆扩散过程中 $x_T$ 已知，因此 $p_\theta(x_T)$ 无法优化，又因为 $q$ 过程是无参的，因此整个 $L_0$ 部分是不需要优化的，我们只需要优化 $L_t$。</p>
<p>&emsp;&emsp;对于 $L_t$，其在宏观上表示了预测图和扩散过程图的分布一致性，由于我们假设这两个分布都是高斯分布，因此代入高斯分布下的 $KL$ 散度计算公式将会得到一个很复杂的式子，但是式 $(11)$ 注意到我们可以直接用 $\tilde\beta_t\ \mathrm{or}\ \beta_t$ 直接 $\beta_t$ 作为方差，因此其实只需要最小化带有 $\mu$ 的部分，只保留高斯 $KL$ 散度的 $\mu$ 之后的化简结果为</p>
<script type="math/tex; mode=display">
L_t=\mathbb E_q[\frac 1{2\sigma_t^2}||\tilde\mu_t(x_t,x_0)-\mu_\theta(x_t,t)||]\tag{14}</script><p>&emsp;&emsp;事实上由式 $(11)$，有 $\tilde\mu_t=\frac 1 {\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar\alpha_t}}z_t)$，因此我们对于 $\mu_\theta$ 的优化目标也应该是这个式子，考虑到重参数技巧，我们实际估计的是正态分布 $z_\theta(x_t,t)$，其表达式应当写成</p>
<script type="math/tex; mode=display">
\mu_\theta(x_t,t)=\frac 1 {\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar\alpha_t}}z_\theta(x_t,t))\tag{15}</script><p>&emsp;&emsp;据此，将式 $(8),\ (11),\ (15)$ 代入式 $(14)$，得出 $L_t$ 的最终优化目标为</p>
<script type="math/tex; mode=display">
\begin{aligned}
L_t&=\mathbb E_z[\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar\alpha_t)}||z-z_\theta(x_t,t)||^2]，代入式(11),(15)\\
&=\mathbb E_z[\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar\alpha_t)}||z-z_\theta(\sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar \alpha_t}z,t)||^2]，代入式(8)
\end{aligned}\tag{16}</script><p>&emsp;&emsp;丢掉不影响训练的常数，损失函数就可以写为：</p>
<script type="math/tex; mode=display">
L_t=\mathbb E_{x_0,t,z}[||z-z_\theta(\sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar \alpha_t}z,t)||^2]\tag{17}</script><p>&emsp;&emsp;其中，$x_0,t$ 均为已知的输入，$z$ 是标准正态分布，直白来说，我们最小化 $L_t$ 的目的就是使逆扩散过程中预测出的中间过程和扩散过程产生的对应过程特征图均值一致，而方差以不学习的方式直接给出。</p>
<p>&emsp;&emsp;至此，我们就推理并证明了 DDPM 选择的损失函数 $L_t$ 的具体表达形式，说白了就是在逆扩散过程之中通过网络 $\theta$ 输入后一时刻的状态图 $x_t$ 和 当前时刻 $t$，期望 $\theta(x_t,t)$ 的输出能够和 $x_{t-1}$ 的均值对齐，并且使用了 $\rm MSELoss$。</p>
<h3 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h3><p><a href="https://github.com/azad-academy/denoising-diffusion-model">参考</a></p>
<p>/home/chushuyang/research/practice/denoising-diffusion-model/diffusion_model_demo.ipynb</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>diffusion models 2</title>
    <url>/Diffusion_Model_2/</url>
    <content><![CDATA[<h2><center> diffusion theories </center></h2>

<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文将承接上文的 <a href>DDPM</a>，继续介绍其他的扩散模型理论，以及对扩散模型扩展的探索：</p>
<ul>
<li>SMLD（NIPS2019）：Generative modeling by estimating gradients of the data distribution</li>
<li>SDE（ICLR2021）：Score-Based Generative Modeling through Stochastic Differential Equations</li>
</ul>
<span id="more"></span>
<hr>
<h3 id="SMLD-Generative-modeling-by-estimating-gradients-of-the-data-distribution"><a href="#SMLD-Generative-modeling-by-estimating-gradients-of-the-data-distribution" class="headerlink" title="SMLD : Generative modeling by estimating gradients of the data distribution"></a>SMLD : Generative modeling by estimating gradients of the data distribution</h3><p>【图像生成】【NIPS2019】【<a href="https://proceedings.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf">paper</a>】【<a href="https://github.com/ermongroup/ncsn">code</a>】</p>
<h4 id="前置说明"><a href="#前置说明" class="headerlink" title="前置说明"></a>前置说明</h4><p>&emsp;&emsp;SMLD 全名为 Score Matching Langevin Dynamics，即基于分数匹配的郎之万采样方法，其主要提出了 NCSN。NCSN 全名为 Noise Conditional Score Network，即以噪声为条件的分数（预测）网络。本文提出了一种基于分数匹配的加噪-去噪过程，同时采用郎之万采样进行逐次迭代，是扩散模型的基础理论之一。</p>
<p>&emsp;&emsp;在阅读本方法时，最好一直带着三个疑问：1.为什么要向样本中添加噪声，2.噪声是怎么样被添加至样本的，3.噪声是如何被去除的。</p>
<ul>
<li><strong>什么是 score？</strong></li>
</ul>
<p>&emsp;&emsp;所谓的 score，定义为数据分布的对数的梯度。对于一个数据分布 $p(x)$，其 score 为 $\nabla_x\log p(x)$。这是数学上的定义，形象化地表示下，假如 $p(x)$ 如下图（左）所示，则其 score 如下图（右）所示。这个 score 的作用是方便采样，具体来说，假如从 $p(x)$ 某个随机位置开始采样，其初始值大概率没有在高密度位置，此时只需要按照梯度逐步前进，将会逐渐到达高密度区域。</p>
<p><img src="/Diffusion_Model_2/image-20230504200127087.png" alt="image-20230504200127087" style="zoom:50%;"></p>
<ul>
<li><strong>什么是 郎之万采样？</strong></li>
</ul>
<p>&emsp;&emsp;郎之万采样就是一种采样方式，其来源于郎之万动力学 Langevin dynamics，也就是扩散模型中去噪的迭代过程，具体来说，给出一个分布 $p(x)$ 的分数，对其的采样过程为：$x_t=x_{t-1}+\frac\epsilon2\nabla_x\log p(x_{t-1})+\sqrt\epsilon z_t$，其中，$\epsilon$ 表示步长，即每次走几步，$z_t\sim N(0,I)$ 表示标准正态分布（噪声），整个过程可以看做，对于初始点 $x_{t-1}$，下一步将在其基础上移动 $\frac\epsilon2$ 步的梯度，并且伴随一个低阶的噪声扰动。</p>
<p><img src="/Diffusion_Model_2/langevin.gif" alt="img" style="zoom:50%;"></p>
<h4 id="优化设计（扩散-逆扩散过程）"><a href="#优化设计（扩散-逆扩散过程）" class="headerlink" title="优化设计（扩散-逆扩散过程）"></a>优化设计（扩散-逆扩散过程）</h4><h5 id="低密度不准确问题"><a href="#低密度不准确问题" class="headerlink" title="低密度不准确问题"></a>低密度不准确问题</h5><p>&emsp;&emsp;我们可以看出来，如果我们有一个非常准确的分数时就可以进行好的采样，但是估计分数也就是在估计分布，没有办法直观地进行获取，而是需要一个神经网络进行逼近，即 $\min\ E_{p(x)}[||s_\theta(x)-\nabla_x\log p(x)||_2^2]$。但是在估计分数时有着天然的问题（即使我们已知分布）：当数据分布比较集中（方差较小）时，空间中会存在大量的低密度区域，在这些区域中没有足够的样本对分数进行估计。这种情况如下图所示：</p>
<p><img src="/Diffusion_Model_2/image-20230504203718065.png" alt="image-20230504203718065" style="zoom: 33%;"></p>
<h5 id="使用退火郎之万采样"><a href="#使用退火郎之万采样" class="headerlink" title="使用退火郎之万采样"></a>使用退火郎之万采样</h5><p>&emsp;&emsp;本文额外引入了一个探讨：当目标数据分布是由两个不同的分布之和时，能否准确进行不同分布的采样？考虑这样一个混合分布：$p(x)=\pi p_1(x)+(1-\pi)p_2(x)$，其中 $\pi$ 为常数，则 $\nabla_x\log p(x)=\nabla_x\log p_1(x)+\nabla_x\log p_2(x)$，也就是说，虽然混合分布 $p(x)$ 本身与 $\pi$ 有关系，但是其 score 却与 $\pi$ 无关，这将极大地限制从混合分布中进行正确采样。</p>
<p>&emsp;&emsp;本文进行了这样的实验：用不同的方法从混合高斯分布中获取样本。(a) 精确抽样。(b) 使用精确分数的郎之万采样进行抽样。(c)使用退火郎之万采样进行抽样。显然，朗之万动力学不正确地估计了两种模式之间的相对权值，而退火朗之万动力学则忠实地恢复了相对权值。</p>
<p><img src="/Diffusion_Model_2/image-20230504204758364.png" alt="image-20230504204758364" style="zoom: 50%;"></p>
<h5 id="NCSN：加噪与训练"><a href="#NCSN：加噪与训练" class="headerlink" title="NCSN：加噪与训练"></a>NCSN：加噪与训练</h5><ul>
<li>加噪方式</li>
</ul>
<p>&emsp;&emsp;既然我们已经明确在分布集中时难以准确估计 score，那么我们就要对其进行加噪，通过使其具备更广的分布从而能够准确估计 score，这也就回答了第一个问题。但是这也会带来一些额外的问题：加入了较大的噪声之后，我们估计出的分布将会和真实分布差距较大。</p>
<p>&emsp;&emsp;为了解决加噪之后难以去除的问题，我们通过这样的方式加入噪声：预设一个公比小于 1 的等比数列 $\{\sigma_i\}_i^L$，将其每一项分别作为正态分布的噪声 $N(0,\sigma^2I)$ 加入原始数据分布 $p(x)$ 之中以获取新数据分布 $q_\sigma(\tilde x)$，则 $q_\sigma(\tilde x)\sim\int p(x)N(\tilde x|x,\sigma^2I)dx$。此时我们的目的就是利用分数估计网络逼近 $q_\sigma(\tilde x)$，不过此时我们将对应的方差 $\sigma$ 也输入至分数网络之中：$s_\theta(\tilde x,\sigma)\simeq \nabla_{\tilde x}\log q_\sigma(\tilde x)$。</p>
<p>&emsp;&emsp;这种加噪方式很好地平衡了上述两个难题，通过先加入大噪声使数据分布更广泛，从而获取原本低密度区域的正确分数估计，再逐步缩小加噪方差，直至最终可以收敛到和原数据分布基本一致。这种以 $\sigma_i$ 作为条件的分数估计网络就称为 NCSN。</p>
<ul>
<li>训练方式</li>
</ul>
<p>&emsp;&emsp;在训练的过程中，由于 $\sigma$ 总共有 $L$ 个，那么计算 loss 时自然所有的都需要考虑在内，为了推理出整体形式，首先考虑对于某一个 $\sigma\in\{\sigma_i\}_i^L$</p>
<script type="math/tex; mode=display">
\begin{align}
&\because q_\sigma(\tilde x|x)=N(\tilde x|x,\sigma^2 I)（因为是直接在原始数据分布上加高斯噪声）\\
&\therefore\nabla_{\tilde x}\log q_\sigma(\tilde x|x)=-\frac{\tilde x-x}{\sigma^2}（展开计算可得）\\
&\therefore L(\theta;\sigma)=\mathbb E_{p(x)}\mathbb E_{\tilde x\sim N(x,\sigma^2I)}[\|s_\theta(\tilde x,\sigma)+\frac{\tilde x-x}{\sigma^2}\|_2^2]
\end{align}</script><p>&emsp;&emsp;因此若考虑全部的 $\sigma$，损失函数应该写为：$L(\theta;\{\sigma_i\}_i^L)=\frac1L\sum\limits_{i=1}^L\lambda(\sigma_i)L(\theta;\sigma_i)$。其中，$\lambda(\sigma_i)$ 用于平衡各个方差之间的数量级，在合理的假设下，平衡之后的损失应当不受到 $\sigma_i$ 的级数的影响。作者指出在训练到收敛之后 $|s_\theta(x,\sigma)|_2\propto \frac1\sigma$，因此设 $\lambda(\sigma_i)=\sigma_i^2$，以此消除由 $\sigma$ 数值带来的影响。</p>
<h5 id="NCSN-的推理过程"><a href="#NCSN-的推理过程" class="headerlink" title="NCSN 的推理过程"></a>NCSN 的推理过程</h5><p>&emsp;&emsp;使用退火郎之万采样对数据进行多次迭代，具体来说，当 score 被近似正确估计时执行下列步骤。其关键的见解在于：<strong>将 $\sigma$ 从大到小进行迭代，大的 $\sigma_i$ 可以使低密度区域更少，但逼近 $p(x)$ 效果不好，尽管如此，以此为 score 进行 $x_t$ 的郎之万采样也会大致地向高密度区域前进。通过这种方式，尽管大的 $\sigma_i$ 不够精确，但足以使 $x_{t+1}$ 进入 $\sigma_{i+1}$ 的 score 准确估计区域。</strong></p>
<p><img src="/Diffusion_Model_2/image-20230505121220625.png" alt="image-20230505121220625" style="zoom: 40%;"></p>
<hr>
<h3 id="SDE-Score-Based-Generative-Modeling-through-Stochastic-Differential-Equations"><a href="#SDE-Score-Based-Generative-Modeling-through-Stochastic-Differential-Equations" class="headerlink" title="SDE : Score-Based Generative Modeling through Stochastic Differential Equations"></a>SDE : Score-Based Generative Modeling through Stochastic Differential Equations</h3><p>【图像生成】【ICLR2021】【<a href="http://arxiv.org/abs/2011.13456">paper</a>】【<a href="https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing#scrollTo=qW1HaPZb9gDM">code</a>】</p>
<h4 id="前置说明-1"><a href="#前置说明-1" class="headerlink" title="前置说明"></a>前置说明</h4><p>&emsp;&emsp;所谓 SDE，即随机微分方程（Stochastic Differential Equations），常用来对随机过程进行建模，对于微分方程来说，常微分方程 ODE 的一般形式为：$dy=f(x)dx$，通常可以使用对 $dx$ 积分的方式求解 $f(y)$，随机微分方程 SDE 的一般形式为：$dx_t=f(x_t,t)dt+g(x_t,t)dw_t$，其中 $w_t$ 表示随机项。本文旨在使用 SDE 构造出一个一致性的扩散模型 pipeline，以指导后续的扩散模型构造。</p>
<h4 id="加噪过程"><a href="#加噪过程" class="headerlink" title="加噪过程"></a>加噪过程</h4><p>&emsp;&emsp;当噪声尺度的数量接近无穷大时，我们基本上会随着噪声水平的不断增加而扰乱数据分布。在这种情况下，噪声扰动过程是一个连续时间随机过程，如下所示</p>
<p><img src="/Diffusion_Model_2/perturb_vp.gif" alt="img"></p>
<p>&emsp;&emsp;这个过程使用 SDE 描述为</p>
<script type="math/tex; mode=display">
dx=f(x,t)dt+g(t)dw</script><p>&emsp;&emsp;其中，作者将 $f(*,t)$ 称为漂移系数，$g(t)$ 称为扩散系数，对于一个 $dt$ 内的 $dx$，前半部分描述了一个确定的移动，后半部分则表示了一个噪声，在单位时间 $dt$ 内的 $dx$ 变化如下图所示：</p>
<p><img src="/Diffusion_Model_2/image-20230507160801692.png" alt="image-20230507160801692" style="zoom: 33%;"></p>
<p> &emsp;&emsp;在上式中，SDE 的具体形式（指 $f(x,t),\ g(t)$ 的表达式）是手工设计的，接下来我们将首先探索一下不使用具体形式的抽象化推理，然后将给 $f(x,t),\ g(t)$ 赋值具体表达式并以此推理出 DDPM 与 SMLD 两种具体扩散模型。</p>
<h4 id="去噪过程"><a href="#去噪过程" class="headerlink" title="去噪过程"></a>去噪过程</h4><p>&emsp;&emsp;<strong>在有限数量的噪声尺度下，我们可以通过使用退火 Langevin 动力学</strong>反转扰动过程来生成样本，即，使用 Langevin 动力学从每个噪声扰动分布中顺序采样。对于无限噪声尺度，我们可以通过使用反向 SDE 类似地反转样本生成的扰动过程。</p>
<p><img src="/Diffusion_Model_2/denoise_vp.gif" alt="img"></p>
<p>&emsp;&emsp;为了能够得到逆向过程的 SDE 方差，我们需要对正向过程 $dx=f(x,t)dt+g(t)dw$ 进行推理，推理过程如下：</p>
<ul>
<li><p>首先，离散化扩散过程 SDE，$dx=x_{t+\Delta t}-x_t,\ dt=\Delta t$，得到：</p>
<ul>
<li><script type="math/tex; mode=display">
x_{t+\Delta t}-x_t=f(x_t,t)dt+g(t)\sqrt{\Delta t}\epsilon,\ \epsilon\sim N(0,I)\tag 1</script></li>
<li><p>这里有两个需要注意的点：1.在函数 $f$ 内，$x\to x_t$ 而非 $x_{t+\Delta t}$ 是因为这里我们是扩散过程，是给出 $x_t$ 求 $x_{t+\Delta t}$ 的过程；2.扩散项的 $dw$ 变成了 $\sqrt{\Delta t}\epsilon$，这个$\Delta t$ 加根号是结论性的，有<a href="https://kexue.fm/archives/9209#%E9%9A%8F%E6%9C%BA%E5%BE%AE%E5%88%86">非官方的解释</a>。</p>
</li>
</ul>
</li>
<li><p>由上式可得</p>
<ul>
<li><script type="math/tex; mode=display">
p(x_{t+\Delta t}|x_t)\sim N(x_t+f(x_t,t)dt,\ g^2(t)\Delta t)\propto \exp (-\frac{\|x_{t+\Delta t}-x_t-f(x_t,t)\Delta t\|_2^2}{2g^2(t)\Delta t})\tag 2</script></li>
</ul>
</li>
<li><p>为了进行逆扩散过程，我们需要知道 $p(x_t|x_{t+\Delta t})$ 因此可以列出贝叶斯公式</p>
<ul>
<li><script type="math/tex; mode=display">
p(x_t|x_{t+\Delta t})=\frac{p(x_{t+\Delta t}|x_t)p(x_t)}{p(x_{t+\Delta t})}\tag 3</script></li>
</ul>
</li>
<li><p>若此时将式 $2$ 代入式 $3$，则会保留一项 $\frac{p(x_t)}{p(x_{t+\Delta t})}$ 无法消去，因此先尝试处理此项</p>
<ul>
<li><script type="math/tex; mode=display">
\frac{p(x_t)}{p(x_{t+\Delta t})}=\exp(\log p(x_t) - \log p(x_t|x_{t+\Delta t}))\tag 4</script></li>
<li><p>对 $\log p(x_{t+\Delta t})$ 在 $x_t$ 处进行泰勒展开，得</p>
</li>
<li><script type="math/tex; mode=display">
\log p(x_{t+\Delta t})\simeq\log p(x_t)+(x_{t+\Delta t}-x_t)\nabla_{x_t}\log p(x_t)+\Delta t\frac{\partial }{\partial t}\log p(x_t)\tag 5</script><ul>
<li>需要注意的是，在上式展开时应采用多元泰勒展开方式进行，不然可能会漏掉最后一项（漏不漏不影响结果就是了）</li>
</ul>
</li>
<li><p>将式 $5$ 代入式 $4$，得</p>
</li>
<li><script type="math/tex; mode=display">
\frac{p(x_t)}{p(x_{t+\Delta t})}\simeq\exp(-(x_{t+\Delta t}-x_t)\nabla_{x_t}\log p(x_t)-\Delta t\frac{\partial }{\partial t}\log p(x_t))\tag 6</script></li>
</ul>
</li>
<li><p>因此将式 $2$ 和式 $6$ 一起代入式 $3$，即可得到</p>
<ul>
<li><script type="math/tex; mode=display">
p(x_t|x_{t+\Delta t})\propto\exp (-\frac{\|x_{t+\Delta t}-x_t-f(x_t,t)\Delta t\|_2^2}{2g^2(t)\Delta t}-(x_{t+\Delta t}-x_t)\nabla_{x_t}\log p(x_t)-\Delta t\frac{\partial }{\partial t}\log p(x_t))\tag 7</script></li>
</ul>
</li>
<li><p>在 SDE 扩散模型的假设中，$\Delta t\to0$，不难看出上式具备 $\mathcal O(1/\Delta t),\ \mathcal O(1),\ \mathcal O(\Delta t)$ 的部分，因此可以令 $\mathcal O(\Delta t)$ 的部分约去，同时以 $x_{t+\Delta t}-x_t$ 为一个整体 $\Delta X$，可以将 $7$ 化简为</p>
<ul>
<li><script type="math/tex; mode=display">
p(x_t|x_{t+\Delta t})\propto\exp (-\frac{\Delta X^2-2\Delta Xf(x_t,t)\Delta t}{2g^2(t)\Delta t}-\Delta X\nabla_{x_t}\log p(x_t))\tag 8</script></li>
<li><p>对上式进行配方，然后去掉 $\mathcal O(\Delta t)$ 可得</p>
</li>
<li><script type="math/tex; mode=display">
p(x_t|x_{t+\Delta t})\propto\exp (-\frac{1}{2g^2(t)\Delta t}\|\Delta X-(f(x_t,t)\Delta t -2g^2(t)\Delta t \nabla_{x_t}\log p(x_t))\|_2^2)\tag 9</script></li>
</ul>
</li>
<li><p>此时，由于我们应该从输入 $x_{t+\Delta t}$ 得到 $x_t$，在保证正确性的情况下（因为 $\mathcal O(\Delta t)\to 0$）对式 $9$ 进行简单的换元</p>
<ul>
<li><script type="math/tex; mode=display">
p(x_t|x_{t+\Delta t})\propto\exp (-\frac{1}{2g^2({t+\Delta t})\Delta t}\|\Delta X-(f(x_{t+\Delta t},t)\Delta t -2g^2({t+\Delta t})\Delta t \nabla_{x_t}\log p(x_{t+\Delta t}))\|_2^2)\tag {10}</script></li>
<li><p>将此式变换为正态分布可以写成</p>
</li>
<li><script type="math/tex; mode=display">
p(x_t|x_{t+\Delta t})\sim N(x_{t+\Delta t}-f(x_{t+\Delta t},t)\Delta t +2g^2({t+\Delta t})\Delta t \nabla_{x_t}\log p(x_{t+\Delta t}),\ g^2({t+\Delta t})\Delta tI)\tag {11}</script></li>
</ul>
</li>
<li><p>写成离散形式的 $x_{t+\Delta t}$ 与 $x_t$ 的关系为</p>
<ul>
<li><script type="math/tex; mode=display">
x_t-x_{t+\Delta t}=f(x_{t+\Delta t},t)\Delta t -2g^2({t+\Delta t})\Delta t\nabla_{x_t}\log p(x_{t+\Delta t})+g(t+\Delta t)\sqrt{\Delta t}\epsilon,\ \epsilon\sim N(0,I)\tag {12}</script></li>
</ul>
</li>
<li><p>将式 $12$ 写为微分形式为</p>
<ul>
<li><script type="math/tex; mode=display">
dx=[f(x,t)-g^2(t)\nabla_x\log p(x_t)]dt+g(t)dw\tag {13}</script></li>
</ul>
</li>
</ul>
<p>&emsp;&emsp;由此，我们就得到了逆扩散过程的 SDE 公式。为了计算反向 SDE，我们需要估计 $\nabla_x\log p(x_t)$，为此，我们训练一个基于时间依赖分数的模型 $s_\theta({x}, t)$，这样 ${s}_\theta({x}, t) \approx \nabla_{x} \log p_t({x})$。这类似于基于噪声条件分数的模型 ${s}_\theta({x}, i)$ 用于有限噪声。</p>
<p>&emsp;&emsp;我们的训练目标 $s_\theta({x}, t)$ 是 Fisher 散度的连续加权组合，由下式给出</p>
<script type="math/tex; mode=display">
\begin{equation} \mathbb{E}_{t \in \mathcal{U}(0, T)}\mathbb{E}_{p_t(\mathbf{x})}[\lambda(t) \| \nabla_\mathbf{x} \log p_t(\mathbf{x}) - \mathbf{s}_\theta(\mathbf{x}, t) \|_2^2], \end{equation} \tag {14}</script><p>&emsp;&emsp;其中，$\mathcal{U}(0, T)$ 表示时间间隔内的均匀分布 $[0,T]$， $\lambda: \mathbb{R} \to \mathbb{R}_{&gt;0}$ 是正加权函数。通常我们使用 $\lambda(t) \propto 1/ \mathbb{E}[| \nabla_{\mathbf{x}(t)} \log p(\mathbf{x}(t) \mid \mathbf{x}(0))|_2^2]$ 以平衡不同分数匹配损失随时间的大小。由此便可以展开训练。</p>
<h4 id="VP-SDE-和-VE-SDE"><a href="#VP-SDE-和-VE-SDE" class="headerlink" title="VP-SDE 和 VE-SDE"></a>VP-SDE 和 VE-SDE</h4><p>&emsp;&emsp;对于原始式子式 $1$，当我们令 $f(x,t)=0,\ g(t)=\frac{d}{dt}\sigma_t^2$ 时，SDE 被称为 VE（方差爆炸）SDE，也就对应着 SMLD 分数扩散模型，由此可以推出 VE-SDE 的更新公式为：$x_{t+\Delta t}=x_t+\sqrt{\sigma_{t+\Delta t}^2-\sigma_t^2}\epsilon$。</p>
<p>&emsp;&emsp;对于原始式子式 $1$，当我们令 $\{\bar\beta_i\}_1^T=\{T\beta_i\}_1^T$，$\beta(\frac iT)=\bar\beta_i$，$f(x,t)=-\frac12\beta(t)x_t,\ g(t)=\sqrt{\beta(t)}$ 时，SDE 被称为 VP（方差收缩）SDE，也就对应着 DDPM 去噪扩散模型，由此可以推出 VP-SDE 的更新公式为：$x_{t+\Delta t}=\sqrt{1-\beta_{t+\Delta t}}x_t+\sqrt{\beta_{t+\Delta t}}\epsilon$。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>pSp</title>
    <url>/Encoding%20in%20Style/</url>
    <content><![CDATA[<h2><center> Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation </center></h2>

<p>【图像生成】【CVPR2021】【<a href="https://ieeexplore.ieee.org/document/9578137/">paper</a>】【<a href="https://github.com/eladrich/pixel2style2pixel">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>ds；本文就是大名鼎鼎的 pSp，相比于 image2styleGAN 的逆映射后优化的方式，本文直接提出了一种以 ResNet 为 backbone，以 FPN 为架构的编码器，可以将任意图像通过训练好的编码器映射到隐空间。实现了人脸转向、特征融合、使用素描或分割图生成不同的人脸、人脸补全、人脸条件生成、人脸超分等任务。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Encoding%20in%20Style/image-20230116090923513.png" alt="image-20230116090923513"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>使用基于学习的编码器而不是基于优化的方式进行图像编码</li>
<li>一个通用的解决 image2image 的端到端框架</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/Encoding%20in%20Style/image-20230120114938291.png" alt="image-20230120114938291"></p>
<p>ds；pSp 框架基于 StyleGAN 的生成器和其提出的 $\cal W+$ 空间，这个 $\cal W+$ 空间是 styleGAN2 中的一个小改进，相当于把随机生成的 $z$ 向量通过几个随即层映射到了一个可控的 $\cal W+$空间。</p>
<p>ds；但是这里做的不是 GAN，而是图像风格迁移之类的任务，所以需要先将图片映射到 latent space 里，这个 latent space 是一个解耦合的空间，也就是将原本可能特征之间有相关性的矩阵映射为矩阵之间的特征无相关性。最简单的方法就是将图片通过一个 encoder (ResNet,VGG等等) 直接 embedding 到 $\cal W+$空间维度，但是这种方法效果并不够好，无法很好 encoding 原图像的细节。</p>
<p>ds；在 StyleGAN 里，其使用了一个金字塔型的 encoding，从粗到细粒度，框架图如图。按照这种思想，pSp同样提出了一个三层的金字塔特征框架，先通过 ResNet 提取三层 vector，每一层层过一个 map2style 的全连接层之后再过一个全连接层 A 输入到 styleGAN 生成器的各个 style 中去，每一个 style 都有一个 $w$ 。所以最后 pSp 生成的图像是由 styleGAN 预训练好的生成器得到的图片。模型的输出为：</p>
<script type="math/tex; mode=display">
pSp(x):=G(E(x)+\bar w)</script><p>ds；其中，$E$ 为 pSp 训练的 encoder，$\bar w$ 则是所有向量的平均，最后过一遍 styleGAN 的生成器 $G$ 。</p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>ds；接下来介绍 pSp 的损失函数，这里需要训练的只有 pSp Encoder，其损失函数则为：</p>
<script type="math/tex; mode=display">
\mathcal L(x)=\lambda_1\mathcal L_{construct}(x)+\lambda_2\mathcal L_{LPIPS}(x)+\lambda_3\mathcal L_{ID}(x)+\lambda_4\mathcal L_{reg}(x)</script><p>其中</p>
<p>第一个是一个像素级别的L2损失，即目标像素点和训练像素点的均方差。</p>
<p><img src="https://pic4.zhimg.com/80/v2-7ee43749261fdda21324d5a1154f29ab_720w.webp" alt="img"></p>
<p>像素点损失</p>
<p>感知损失，这是CVPR 2018 的一篇 poster提出的一种新的损失函数，这个是F是训出来的，没有直接的定义，整体就是传统的用cosine计算图片之间的相似度在人类眼中结果并不好，例如高斯模糊之后的图像在和原图计算L2损失的时候并没有很相近，但是却丢失了很多的特征信息，所以使用一个训练好的模型来计算两张图片之间的相似度比单纯用cosine等低层次的计算效果好。</p>
<p><img src="https://pic2.zhimg.com/80/v2-8757390f90c350a0994b6b2a308631d1_720w.webp" alt="img"></p>
<p>感知损失</p>
<p>因为上面用到了，所以计算以下生成的encoding的与w平均的均方差。</p>
<p><img src="https://pic1.zhimg.com/80/v2-c25c4d9eeb51e67579fdb8fe23070d80_720w.webp" alt="img"></p>
<p>计算生成损失</p>
<p>最后就是一个人脸损失，也是在styleGAN上的一个创新点，用预训练好的人脸识别模型计算输入和生成图片的cosine相似度</p>
<p><img src="https://pic1.zhimg.com/80/v2-ec7cc46e6568f0a1cc2aafdf5cd43434_720w.webp" alt="img"></p>
<p>最后把所有的Loss加在一起就是整个encoder的损失函数了。</p>
<script type="math/tex; mode=display">
math\_express</script><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><hr>
<h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3>]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>GPEN</title>
    <url>/GPEN/</url>
    <content><![CDATA[<h2><center> GAN Prior Embedded Network for Blind Face Restoration in the Wild </center></h2>

<p>【盲图重建】【CVPR2021】【<a href="http://arxiv.org/abs/2105.06070">paper</a>】【<a href="https://github.com/yangxy/GPEN">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文采用了训练好的 styleGAN2 作为先验网络嵌入到重建架构之中，重建架构使用 U-net 网络，对于 Encoder，其在降采样过程中的中间表示被送入对应的 Decoder (styleGAN2) 层作为 noise；其整体的输出作为 $z$ 经由 mapping 映射至 $w$ 输入预训练的 styleGAN2。整个网络架构均进行梯度更新。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/GPEN/image-20230207141302512.png" alt="image-20230207141302512"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>使用可以微调的预训练 styleGAN2 嵌入重建网络</li>
<li>styleGAN2 的 $z$ 和 noise 分别从 Encoder 的深层和浅层特征中产生，相应地重构图像的全局结构、局部人脸细节和背景</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/GPEN/image-20230207151021723.png" alt="image-20230207151021723"></p>
<p>&emsp;&emsp;网络分为两个部分，第一部分为上图中 (a) 展示的 styleGAN2 的 backbone，唯一区别之处在于在 (b) 部分中展示的 B(noise) 输入时采用的是 concat 而非原版 styleGAN2 中的 add。</p>
<p>&emsp;&emsp;其中的 Mod 和 Demod 是 styleGAN2 的操作，对于 Mod，其输入除了 A 输出的 $y_i$ 之外还有下面的 Conv 的权重 $w$，其操作即为 $w’=y_i\cdot w$，而对于 Demod，其输入为 $w’$，操作为：$w’’=w’/\sqrt{\sum w’^2+\epsilon}$，其等价于 $w’’=w’/\sigma_{w’}$，$\epsilon$ 只为了保证分母不为 0。</p>
<p>&emsp;&emsp;训练时首先训练 styleGAN2，使用 HQ 的人脸数据集沿用标准训练方法进行训练，训练得到的 GAN 网络插入 UNet 之中作为 Decoder 部分并使用构造的 LQ-HQ 样本对进行微调，在微调的过程中采用的损失函数由三部分组成，分别为：对抗学习损失 $L_A$，内容重建损失 $L_C$，特征匹配损失 $L_F$，其总表达式为：$L=L_A+\alpha L_C+\beta L_F,\ (\alpha=1,\ \beta=0.02)$，具体表达式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&L_A=\min_G\max_DE_{(X)}\log(1+\exp(-D(G\cdot E(\tilde X))))\\
&L_C=||G\cdot E(\tilde X)-X||_1\\
&L_F=\min_GE_{(X)}(\Sigma^T_{i=0}||D^i(X)-D^i(G\cdot E(\tilde X))||_2)
\end{aligned}</script><p>&emsp;&emsp;其中，$i$ 表示判别器的第 $i$ 层，$T$ 表示总层数，$X\ ,\tilde X$ 分别表示 HQ 和 LQ 的图像，$\tilde X$ 由如下公式得到：</p>
<script type="math/tex; mode=display">
\tilde X=((X\otimes k)\downarrow_s+n_\sigma)_{JPEG_q}</script><p>&emsp;&emsp;其中，$k,\ n_\sigma$ 分别表示模糊核，标准差为 $\sigma$ 的高斯噪声、$\otimes,\ \downarrow_s,\ JPEG_q$ 分别表示二维卷积、标准 $s$ 倍下采样器和压缩质量为 $q$ 的 $JPEG$ 压缩算子。对于每个图像，模糊核 $k$ 是从一组模糊模型中随机选择的，包括具有不同核大小的高斯模糊和运动模糊。其中各个参数范围为：$\sigma\in[0,25],\ q\in[5,50],\ s\in[10, 200]$。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;本文的消融实现测试了：不微调 GAN，取消 GAN 的噪声，将噪声按照 GAN 原本的方式进行相加，结果如下图，可以看出噪声级联或者相加其实影响不大，其他的倒是意料之中的产生了较大的影响：</p>
<p><img src="/GPEN/image-20230207212557985.png" alt="image-20230207212557985" style="zoom:50%;"></p>
<p>&emsp;&emsp;相比于其他的 BFR(Blind Face Restoration) 的指标：</p>
<p><img src="/GPEN/image-20230207212710232.png" alt="image-20230207212710232" style="zoom:50%;"></p>
<p>&emsp;&emsp;相比于其他 FSR(Face Super-Resolution) 的指标和结果可视化：</p>
<p><img src="/GPEN/image-20230207212930980.png" alt="image-20230207212930980"></p>
<p><img src="/GPEN/image-20230207213017505.png" alt="image-20230207213017505" style="zoom:80%;"></p>
<p>&emsp;&emsp;本文还提出了一个指标：将不同的算法得到的图像交给真人进行打分：“对从互联网上收集的113幅真实世界的LQ人脸图像的BFR结果以随机顺序呈现给17名志愿者进行主观评价”，并给出了对应的分数直方图：</p>
<p><img src="/GPEN/image-20230207213216895.png" alt="image-20230207213216895" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>HR_CL_the_way_to_my_heart</title>
    <url>/HR_CL_the_way_to_my_heart/</url>
    <content><![CDATA[<h2 id="The-Way-to-my-Heart-is-through-Contrastive-Learning-Remote-Photoplethysmography-from-Unlabelled-Video"><a href="#The-Way-to-my-Heart-is-through-Contrastive-Learning-Remote-Photoplethysmography-from-Unlabelled-Video" class="headerlink" title="The Way to my Heart is through Contrastive Learning:Remote Photoplethysmography from Unlabelled Video"></a>The Way to my Heart is through Contrastive Learning:Remote Photoplethysmography from Unlabelled Video</h2><p>【心率检测】【ICCV2021】【<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Gideon_The_Way_to_My_Heart_Is_Through_Contrastive_Learning_Remote_ICCV_2021_paper.pdf">paper</a>】【<a href="https://github.dev/ToyotaResearchInstitute/RemotePPG">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;这篇文章为了解决心率检测数据集难以采集的问题提出了一种可能的对比学习方法，并且同时缝合了能够放大 ROI 区域的显著性采样器，取得了一般的结果。实际上，这篇文章对比的 SOTA 都很有年代感，而 CVPR2021 有一篇基于 Dual-GAN 的工作在效果上已经远超这篇文章的结果，甚至是数量级级别的优势。这篇文章的结果甚至放在 20 年都毫无竞争力，基本可以说贡献仅限于某个可行的对比学习框架和一个新 loss。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/HR_CL_the_way_to_my_heart/image-20221023100822413.png" alt="image-20221023100822413" style="zoom:60%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>缝合了显著性采样器（直觉上有改进，实际上无）</li>
<li>提出了一种对比学习的框架（陷入了 MoCo 提出的两种缺陷之一：负样本太少）</li>
<li>提出了一种代理任务（通过改变心率频率获得正负样本，理论上仅说得通，但想不到更好的）</li>
<li>提出了一种新的 loss 函数 MCC（最大互相关，或许能更好适配 CL）</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/HR_CL_the_way_to_my_heart/image-20221023102357201.png" alt="image-20221023102357201" style="zoom:70%;"></p>
<h4 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h4><p>&emsp;&emsp;首先通过输入 $X_i$ 进行均匀采样得到 $x_a\to(N,C,D,H,W)$，$x_a$ 通过显著性采样器 $S$ 得到 $x_a^s$，这里的显著性采样器作用就是放大图像的 ROI 区域，输入输出 shape 不变。接着 $x_a^s$ 经过估计器（backbone）$g_\theta$ 得到 $y_a\to(N,1,D,1,1)$，也就是对 $x_a$ 的心率预测结果。</p>
<ul>
<li>监督学习框架</li>
</ul>
<p>&emsp;&emsp;在监督学习框架下，直接用 $y_a$ 对比 GT，并使用损失函数限制输出估计从而训练 $g_\theta$，对于 backbone 的选择，本文中选择了老熟人 physnet，也就是说这种“监督学习框架”只是把 physnet 加了个显著性采样器，而且效果还不如不加🤦‍♂️</p>
<ul>
<li>对比学习框架</li>
</ul>
<p>&emsp;&emsp;在对比学习框架下，我们自然需要正负样本，于是对 $x_a^s$ 经过一个频率重采样器 $R$（在代理任务中详细介绍）得到 $x_n^s\to(N,C,[\frac D {r_f}],H,W)$，这里的 $x_n^s$ 即为对显著性采样后的图像进行的频率重采样，此时的视频帧序列长度变少了，具体取决于 $r_f$ 的值，而这个值随机在 $1.25\to1.5$ 之间取得。</p>
<p>&emsp;&emsp;对于已经得到的 $x_n^s$，其经过同一个估计器 $g_\theta$ 得到 $y_n\to(N,1,[\frac D {r_f}],1,1)$ 即为对比学习之中的负样本，这里经过同一个参数的估计器可以保证负样本和预测值的一致性。对于 $y_n$ 再进行重采样变回到原来的频率即可得到 $y_p$（即正样本），虽然这里写的 $R$ 模块是一个由 $r_f$ 参数确定的采样模块，其实就是一个线性插值，甚至获得正负样本的代码里用的还不是一个函数，直接调的 nn.UpSample，还分别调了两次🤦‍♂️</p>
<p>&emsp;&emsp;对于得到的预测值 $y_a$，正样本 $y_p$，负样本 $y_n$，分别经过 rfft 和 fft 得到新的表示 $f_a,f_p,f_n$，然后用一些对比学习的常用函数进行 loss 计算，具体来说包括：IrrelevantPowerRatio、NegativeMaxCrossCov（新）、NegSNRLoss。</p>
<h4 id="代理任务"><a href="#代理任务" class="headerlink" title="代理任务"></a>代理任务</h4><p>&emsp;&emsp;这里将详细描述代理任务的正负样本构造过程以及这样做的目的、为什么有效、在何时有效、以及在后处理中为了适配视频帧数目的一些 trick。</p>
<ul>
<li>正负样本构造过程</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FrequencyContrast</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Frequency contrast wrapper around a backbone model e.g. PhysNet</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args, device, dataset</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.backbone = init_model(args.contrast_model, args, device, dataset)</span><br><span class="line">        self.upsampler = nn.Upsample(size=(dataset.options.D,), mode=<span class="string">&#x27;linear&#x27;</span>, align_corners=<span class="literal">False</span>)</span><br><span class="line">        self.get_temp_views = CalculateMultiView(args.mvtl_window_size, args.mvtl_number_views) <span class="comment"># 记两个参数分别为 ws, nv</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_a</span>):</span><br><span class="line">        B = x_a.shape[<span class="number">0</span>]    <span class="comment"># 没用上，经典写点废话</span></span><br><span class="line">        D = x_a.shape[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># x_a shape : (N, C, D, H, W)</span></span><br><span class="line">        branches = &#123;&#125;   <span class="comment"># 这个参数最终返回 anchors, positives, negatives，需要注意的是最后他们的shape都是 (nv, N, ws)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Resample input</span></span><br><span class="line">        freq_factor = <span class="number">1.25</span> + (torch.rand(<span class="number">1</span>, device=x_a.device) / <span class="number">4</span>) <span class="comment"># 随机生成频率参数 1.25 ~ 1.5</span></span><br><span class="line">        target_size = <span class="built_in">int</span>(D / freq_factor)  <span class="comment"># 计算目标频率（即目标帧数量）</span></span><br><span class="line">        resampler = nn.Upsample(size=(target_size, x_a.shape[<span class="number">3</span>], x_a.shape[<span class="number">4</span>]),</span><br><span class="line">                                mode=<span class="string">&#x27;trilinear&#x27;</span>,</span><br><span class="line">                                align_corners=<span class="literal">False</span>)    <span class="comment"># 使用三线性插值的方式将帧数量调整到目标频率</span></span><br><span class="line">        x_n = resampler(x_a)    <span class="comment"># x_n shape : (N, C, target_size, H, W)</span></span><br><span class="line">        x_n = F.pad(x_n, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, D - target_size)) <span class="comment"># 将帧数量补齐到目标频率，具体来说，就是在 target_size 之后补0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pass both samples through backbone</span></span><br><span class="line">        y_a = self.backbone(x_a).squeeze(<span class="number">4</span>).squeeze(<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># backbone : PhysNet, (N, C, D, H, W) -&gt; (N, 1, D, 1, 1)</span></span><br><span class="line">        <span class="comment"># y_a shape : (N, 1, D)</span></span><br><span class="line">        y_n = self.backbone(x_n).squeeze(<span class="number">4</span>).squeeze(<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># y_n shape : (N, 1, D) 这里需要注意的是，由于 y_n 是经过了 pad 的，因此在 target_size 之后的结果或许不准确</span></span><br><span class="line">        <span class="comment"># Remove padding from negative branch</span></span><br><span class="line">        y_n = y_n[:,:,:target_size] <span class="comment"># y_n shape : (N, 1, target_size) 正如刚刚说的，这里将 target_size 之后的结果去掉</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Resample negative PPG to create positive branch</span></span><br><span class="line">        y_p = self.upsampler(y_n)   <span class="comment"># y_p shape : (N, 1, D)，这里使用线性插值的方式将帧数量调整到目标频率</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save branches and backbone</span></span><br><span class="line">        branches[<span class="string">&#x27;anc&#x27;</span>] = y_a.squeeze(<span class="number">1</span>) <span class="comment"># anc shape : (N, 1, D) -&gt; (N, D) 这三个都一样</span></span><br><span class="line">        branches[<span class="string">&#x27;neg&#x27;</span>] = y_n.squeeze(<span class="number">1</span>)</span><br><span class="line">        branches[<span class="string">&#x27;pos&#x27;</span>] = y_p.squeeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Create backbone output</span></span><br><span class="line">        backbone_out = branches[<span class="string">&#x27;anc&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Sample random views for each branch</span></span><br><span class="line">        <span class="keyword">for</span> key, branch <span class="keyword">in</span> branches.items():</span><br><span class="line">            branches[key] = self.get_temp_views(branch) <span class="comment"># 这里的 branch key shape : (nv, N, ws)</span></span><br><span class="line">            <span class="comment"># 具体来说，这个函数从 target_size / D 中随机取 ws 帧，目的是保证序列长度一致</span></span><br><span class="line">            <span class="comment"># 这个操作会进行 nv 次，最终返回的 shape 为 (nv, N, ws)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> backbone_out, branches</span><br></pre></td></tr></table></figure>
<ul>
<li>正负样本有效性</li>
</ul>
<p>&emsp;&emsp;这里的正样本和负样本从视觉上看几乎没有差别，毕竟差不多相当于重新在输入中采样，而 HR 的输入一般都差别较小。区分正负样本的本质是<strong>改变正负样本的心跳频率</strong>，下图展示了为什么重采样会改变预测出的心跳频率。假设对于 GT 进行重采样得到 GTN，GTN 的帧数约为 GT 的 50%，接着再将 GTN 重采样回 GTP。</p>
<p><img src="/HR_CL_the_way_to_my_heart/image-20221024094202326.png" alt="image-20221024094202326"></p>
<p>&emsp;&emsp;这里假设 $y_a$ 是正弦分布，共采样了 160 个点，周期为 64。我们可以看出负样本 $y_n$ 就是频率减小一半的 $ y_a$，至于为什么这个图里面看起来只有两个，是因为 $y_a$ $y_p$ 基本相等，其中 UpSample 方法基本和原论文保持一致，生成代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正负样本分析</span></span><br><span class="line">D, target_size = <span class="number">160</span>, <span class="number">80</span></span><br><span class="line">y_a = torch.from_numpy(np.sin(np.arange(<span class="number">0</span>, D//<span class="number">10</span>, <span class="number">0.1</span>))).unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#D, target_size = 20, 10</span></span><br><span class="line"><span class="comment">#y_a = torch.from_numpy(np.sin(np.arange(0, D, 1))).unsqueeze(0).unsqueeze(0)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;y_a.shape: <span class="subst">&#123;y_a.shape&#125;</span>&#x27;</span>)</span><br><span class="line">y_n = nn.Upsample(size=(target_size,), mode=<span class="string">&#x27;linear&#x27;</span>, align_corners=<span class="literal">False</span>)(y_a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;y_n.shape: <span class="subst">&#123;y_n.shape&#125;</span>&#x27;</span>)</span><br><span class="line">y_p = nn.Upsample(size=(D,), mode=<span class="string">&#x27;linear&#x27;</span>, align_corners=<span class="literal">False</span>)(y_n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;y_p.shape: <span class="subst">&#123;y_p.shape&#125;</span>&#x27;</span>)</span><br><span class="line">plt.plot(y_a.squeeze().numpy(), label=<span class="string">&#x27;y_a&#x27;</span>)</span><br><span class="line">plt.plot(y_n.squeeze().numpy(), label=<span class="string">&#x27;y_n&#x27;</span>)</span><br><span class="line">plt.plot(y_p.squeeze().numpy(), label=<span class="string">&#x27;y_p&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;但是，如果我们稍微减少 D 和 target_size 的值，也就是使其采样频率大幅度降低，每秒采样数极少的情况下，仍然采用 sin 生成，得到的结果如下：</p>
<p><img src="/HR_CL_the_way_to_my_heart/image-20221024094219989.png" alt="image-20221024094219989"></p>
<p>&emsp;&emsp;可以看出其中正样本和锚点的预测图像也开始变得不同，实际上如果更大的采样间隔会产生更严重的问题，下图展示了实际采用 UBFC 数据集每秒采集三帧然后重采样得到的结果：</p>
<p><img src="/HR_CL_the_way_to_my_heart/image-20221024093951467.png" alt="image-20221024093951467"></p>
<p>&emsp;&emsp;从上至下依次是 $x_a,x_n,x_p$，采用方式和之前对 GT 的采样方式完全一致，代码如下所示。从上图可以看出，$x_a$ 表示了原人脸进行 睁眼-闭眼-睁眼 的过程，而重采样之后的负样本 $x_n$ 仅包含 睁眼-闭眼 两帧，这个其实没问题，毕竟负样本频率就要比锚点频率低的。但是重采样后的正样本虽然帧数和 $x_a$ 一样，也只包含了 睁眼-闭眼 的过程，整个频率预测之后 $x_n,x_p$ 反而保持了一致，这是不可接受的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重采样分析：UBFC</span></span><br><span class="line">UBFC_data_root = <span class="string">&#x27;put root here&#x27;</span></span><br><span class="line">img1 = torch.tensor(cv2.imread(<span class="string">f&#x27;<span class="subst">&#123;UBFC_data_root&#125;</span>/subject1/pic/0.png&#x27;</span>), dtype=torch.float32).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">img2 = torch.tensor(cv2.imread(<span class="string">f&#x27;<span class="subst">&#123;UBFC_data_root&#125;</span>/subject1/pic/10.png&#x27;</span>), dtype=torch.float32).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">img3 = torch.tensor(cv2.imread(<span class="string">f&#x27;<span class="subst">&#123;UBFC_data_root&#125;</span>/subject1/pic/20.png&#x27;</span>), dtype=torch.float32).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">img4 = torch.tensor(cv2.imread(<span class="string">f&#x27;<span class="subst">&#123;UBFC_data_root&#125;</span>/subject1/pic/30.png&#x27;</span>), dtype=torch.float32).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">img_all = torch.cat((img1, img2, img3, img4), axis=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(img_all.shape)</span><br><span class="line">x_a = img_all.permute(<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">(B, C, D, H, W) = x_a.shape</span><br><span class="line">freq_factor = <span class="number">1.25</span> + (torch.rand(<span class="number">1</span>, ) / <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(freq_factor)</span><br><span class="line">target_size = <span class="built_in">int</span>(D / freq_factor)</span><br><span class="line"><span class="built_in">print</span>(target_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;x_a.shape: <span class="subst">&#123;x_a.shape&#125;</span>&#x27;</span>)</span><br><span class="line">resampler = nn.Upsample(size=(target_size, x_a.shape[<span class="number">3</span>], x_a.shape[<span class="number">4</span>]),</span><br><span class="line">                                mode=<span class="string">&#x27;trilinear&#x27;</span>,</span><br><span class="line">                                align_corners=<span class="literal">False</span>)</span><br><span class="line">x_n = resampler(x_a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;x_n.shape: <span class="subst">&#123;x_n.shape&#125;</span>&#x27;</span>)</span><br><span class="line">unsampler = nn.Upsample(size=(D, x_n.shape[<span class="number">3</span>], x_n.shape[<span class="number">4</span>]),</span><br><span class="line">                                mode=<span class="string">&#x27;trilinear&#x27;</span>,</span><br><span class="line">                                align_corners=<span class="literal">False</span>)</span><br><span class="line">x_r = unsampler(x_n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;x_r.shape: <span class="subst">&#123;x_r.shape&#125;</span>&#x27;</span>)</span><br><span class="line">after_inter = x_n[<span class="number">0</span>].permute(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">0</span>).numpy().astype(np.uint8)</span><br><span class="line">row_img = x_a[<span class="number">0</span>].permute(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">0</span>).numpy().astype(np.uint8)</span><br><span class="line">re_img = x_r[<span class="number">0</span>].permute(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">0</span>).numpy().astype(np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">4</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.imshow(row_img[i])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">4</span>,i+<span class="number">5</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.imshow(after_inter[i])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">4</span>,i+<span class="number">9</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.imshow(re_img[i])</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;因此这种代理任务仅适用于采样频率较高的数据集，若采样频率稍低，可能会产生正负样本都和锚点预测频率不一致的情况。</p>
<p>&emsp;&emsp;❓❓❓这里有个小疑惑：为什么这个代理任务是奏效的❓❓❓通过阅读代码可知，这个网络实际上并没有进行预训练，也没有教师网络，如何保证其训练不会走偏❓（这大概也是对对比学习的疑惑），以及，这里的每个锚点其对应的正样本和负样本的数目是一样的，而 MoCo 认为负样本实际上越多越好，那么这里也给他加上动量更新的 memory bank 是不是会得到更好的结果呢❓</p>
<h4 id="显著性采样器"><a href="#显著性采样器" class="headerlink" title="显著性采样器"></a>显著性采样器</h4><p>&emsp;&emsp;显著性采样器来自于 ECCV2018 的这项工作：<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Adria_Recasens_Learning_to_Zoom_ECCV_2018_paper.pdf">Learning to zoom: a saliency-<br>based sampling layer for neural networks</a>，简单介绍这个工作，这一项工作是针对所有图像处理领域的前置工作，本质上是对不同任务的 ROI 进行放大，在生成放大 ROI 之后的图像时需要经过这两个步骤：1.根据不同的 task_network 获得显著性特征图 Saliency map $S$（其实就是一个注意力图，这是由已经训练好的 task_network 得到的）2.根据特征图进行重采样，并得到新的图像（这里仅使用了图像处理）</p>
<p><img src="/HR_CL_the_way_to_my_heart/image-20221024100057269.png" alt="image-20221024100057269" style="zoom:80%;"></p>
<p>&emsp;&emsp;根据生成的显著性图 $S$ 获得每个新像素（红色位置）的采样点（蓝色位置），然后进行采样。</p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>&emsp;&emsp;这里提出了一种新的计算 loss 的方法，名字为 maximum cross-correlation (MCC) 最大互相关，这个 loss 对对比学习具备更好的适配，具体来说，其计算公式如下：</p>
<script type="math/tex; mode=display">
\rm MCC=c_{pr}\times Max(\frac{F^{-1}\{BPass(F\{y\}·\overline{F\{\hat y\}})\}}{\sigma_y\times\sigma_{\hat y}})</script><p>&emsp;&emsp;这里首先将输出和 GT 分别减去均值以简化运算，分别得到了 $\rm y,\hat y$，接着对 $\rm y, \hat y$ 分别进行快速傅里叶变化 $\rm F$，然后使用输入的 fft 乘以 GT 的 fft 的共轭（即上横线），对得到的结果先通过 $\rm BPass$ 后进行逆快速傅里叶变换 $\rm F^{-1}$，这里的 $\rm BP ass$ 指的是将值位于 $\rm 40-250bpm$ 之外的填充为 0。对于这里得到的结果再除以 $\rm \sigma_y\times\sigma_{\hat y}$，这里的 $\sigma$ 表示标准差，得到结果中的最大值就表示了<strong>理想偏移量处的相关系数</strong>，这个值再乘一个常数 $\rm c_{pr}$（<strong>心跳频率内的功率比</strong>）得到的即是最终的 MCC（此段加粗的地方是原文，没理解意思）。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;结果很差，不值一提，举个例子，可见一斑。</p>
<p><img src="/HR_CL_the_way_to_my_heart/image-20221024100515924.png" alt="image-20221024100515924"></p>
<p>&emsp;&emsp;这里展示了是否有监督以及是否使用显著性采样器在 UBFC 数据集上的 RMSE 和 MAE 最好的结果，4.6 / 3.6。同时下图展示了同年发表在 CVPR2021 的论文 Dual-GAN 的结果，为 0.67 / 0.44。</p>
<p><img src="/HR_CL_the_way_to_my_heart/image-20221024100808249.png" alt="image-20221024100808249" style="zoom:60%;"></p>
<p>&emsp;&emsp;其他的一些结果我都不乐意展示，他们这篇论文刻意避开了一些更常用的数据集，并且很多数据集没选择的理由几乎没有解释（详见本文 Table. 2），并且在比较结果的时候仅比较了 HR-CNN，是一篇 2018 年的工作，经典鸵鸟比较法了属于是。</p>
<p>&emsp;&emsp;不过不管怎么说，自监督还是牛的，至少拉平了 physnet 的结果，如果选用更好的 backbone 是不是会更好呢？以及采用更加合理的对比学习框架是不是会更好呢？</p>
<hr>
<h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3><p>&emsp;&emsp;究极缝合：为了解决负样本不足的问题，可以将其余数据采样之后的负样本进入队列存储，但是这样或许会导致网络学到捷径，为了解决这个问题，可以加入教师网络进行知识蒸馏，以及为了得到更好的结果可以尝试更换 backbone。</p>
<p>&emsp;&emsp;更好的代理任务一定是可行的，但是对于心率检测，代理任务还提出的相当少，或许需要大量的假设和尝试，简单的方法就是迁移已有的代理任务，多多实验尝试。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>HR_Dual_GAN</title>
    <url>/HR_Dual_GAN/</url>
    <content><![CDATA[<h2 id="Dual-GAN-Joint-BVP-and-Noise-Modeling-for-Remote-PhysiologicalMeasurement"><a href="#Dual-GAN-Joint-BVP-and-Noise-Modeling-for-Remote-PhysiologicalMeasurement" class="headerlink" title="Dual-GAN: Joint BVP and Noise Modeling for Remote PhysiologicalMeasurement"></a>Dual-GAN: Joint BVP and Noise Modeling for Remote PhysiologicalMeasurement</h2><p>【心率检测】【CVPR2021】【<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lu_Dual-GAN_Joint_BVP_and_Noise_Modeling_for_Remote_Physiological_Measurement_CVPR_2021_paper.pdf">paper</a>】【<a href>code未开源</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种基于对偶 GAN 的心率预测网络，同时提出了一种解决不同 ROI 之间的噪声和 BVP 分布不一致的问题的即插即用块。并且本文网络的贡献除了效果非常好之外，对于噪声更加鲁棒。除此之外，该网络结构并不是纯 Dual-GAN，而是由三个生成器和一个判别器组成的网络，用到了回归的方式进行训练，并非无监督训练，并且全部的 G/D 基于 CNN，网络只需要在 1080Ti 上训练 10 个 epoch 即可收敛。可以说唯一的缺点是还没开源了😭</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/HR_Dual_GAN/image-20221026192218294.png" alt="image-20221026192218294" style="zoom:60%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>基于 Dual-GAN 的预测网络</li>
<li>调整不同 ROI 之间噪声，BVP 分布不同的即插即用块</li>
<li>集合了噪声生成器，增强了对环境噪音的鲁棒性</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><h4 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h4><ul>
<li><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Yi_DualGAN_Unsupervised_Dual_ICCV_2017_paper.pdf">Dual-GAN</a></li>
</ul>
<p>&emsp;&emsp;Dual-GAN 即对偶 GAN，是一篇发表于 ICCV2017 的文章，其通过对偶学习的方式提出了一种在两种具备同样语义信息之间的图像的转化方式。Dual-GAN 的网络结构如下图所示：</p>
<p><img src="/HR_Dual_GAN/image-20221027105943129.png" alt="image-20221027105943129"></p>
<p>&emsp;&emsp;其网络的目的是训练处两个生成器，$G_A$ 可以将素描转化为照片，$G_B$ 可以将照片转化为素描。训练的方式是使分先后顺序通过 $G_A,G_B$ 的图像分布更接近原图。简单解释这个意思：现在我们有素描的数据集和照片的数据集，把一张照片通过 $G_B$ 之后会变成素描图，这时候我们训练 $D_B$ 进行判别，强迫生成的素描图更真实。到这里和普通的 GAN 没区别，只是把 sample 的数据改成了照片。但是 Dual-GAN 还要求对于生成的素描图再通过 $G_A$ 变回去之后和原图分布一致。</p>
<p>&emsp;&emsp;这种 GAN 适用于输出和输入都为强语义信息的数据，对于原本需要建立 label 对应的数据现在就可以进行无监督的训练了。</p>
<ul>
<li>STMap</li>
</ul>
<p><img src="/HR_Dual_GAN/image-20221027153710237.png" alt="image-20221027153710237"></p>
<p>&emsp;&emsp;生成 STMap 的方式有很多，上图是其中一种方式，多项工作均显示了使用 STMap 代替原始输入能够得到更好的结果。简单来说，STMap 就是去除背景，仅保留 ROI 区域的图，然后将 ROI 图划分W为多个小块，每个小块划分为 YUV 三通道并拉平成 3D 序列，再将对应位置 YUV 通道的不同帧求平均。</p>
<h4 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h4><p>&emsp;&emsp;正如概览中的图像，网络由多个子模块组成，首先详细解释概览图中每一项表示的具体含义：</p>
<p>&emsp;&emsp;$Video\ v$ 表示原始输入视频，$m$ 表示由 $v$ 提取出的 STMap，其维度为 nx1xc。$F_b$ 表示心率/BVP估计器，是整个网络的核心，其输出最终被作为心率检测的结果。$s_{est}$ 表示 $m$ 通过 $F_b$ 的输出 BVP，$y_{est}$ 表示输出的心率。</p>
<p>&emsp;&emsp;$s_{gt}$ 表示心率的 GT，$G_{phy}$ 表示通过真实心率生成不含有噪音的 STMap，输出即为 $m_{phy}$。$Z$ 为高斯随机产生的噪声，$G_{noise}$ 表示通过 $Z$ 生成随机噪声 STMap $m_n$ 的生成器。 其中，$m_{phy}\bigoplus m_n=m_{syn}$ 表示含有噪声但从 GT 生成的 STMap。$m_{phy}$ 经过 $F_b$ 之后输出为 $s_r$ 作为重建 BVP，回想 $s_r$ 的重建过程，由 $s_{gt}\to G_{phy}\to F_b\to s_r$，这个过程即为对偶学习。</p>
<p>&emsp;&emsp;最后的判别器 $D$ 输入为 $s$ 和 $m$ 的组合，包括：1.从输入视频中得到的 STMap 和 预测出的心率，2.从输入视频中得到的 STMap 和 GT心率，3.含有噪声的 STMap 和 GT心率。其中 $D$ 认为 2 为真实，其余为虚假，以此训练。</p>
<p>&emsp;&emsp;该文章提出了很多的损失函数，共包括：</p>
<ul>
<li>为了辅助回归训练的损失函数：</li>
</ul>
<script type="math/tex; mode=display">
\zeta_p=1-PCor(s_{est},s_{gt})\tag 1</script><script type="math/tex; mode=display">
\zeta_{fre}=CE(PSD(s_{est},o_{gt}))\tag 2</script><script type="math/tex; mode=display">
\zeta_{phy}=\lambda_1||y_{est}-y_{gt}||_1+\lambda_2\zeta_p+\lambda_3+\zeta_{fre}\tag 3</script><p>&emsp;&emsp;其中 $PCor$ 表示皮尔森相关性，$CE$ 表示交叉熵，$PSD$ 表示功率谱密度。公式 $(2)$ 中的 $o_{gt}$ 表示将心率的 GT $y_{gt}$ 表示为 one-hot 向量（详见原文公式 P5.Eq2）。公式 $(1),(2)$ 用于评估预测结果的质量，整体来说使用公式 $(3)$ 进行回归训练限制预测值和 GT 保持一致。</p>
<ul>
<li>为了训练判别器的损失函数：</li>
</ul>
<script type="math/tex; mode=display">
\mathop{max}\limits_{D} \mathop{min}\limits_{F_b,G_{noise}} \zeta_{joint}=log(D(s_{gt},m))-log(D(s_{est},m))-log(D(s_{gt},m_{syn}))\tag 4</script><p>&emsp;&emsp;这个式子很简单且直观，就是要让 $D$ 将之前说的输入 2 判断为真，将输入 1 3 判断为假。</p>
<ul>
<li>为了训练 Dual-GAN 的损失函数：</li>
</ul>
<script type="math/tex; mode=display">
\zeta_r=1-PCor(s_r,s_{gt})\tag 5</script><p>&emsp;&emsp;这个式子是为了限制 $s_{gt}=F_b(G_{phy}(s_{gt}))$，即限制生成器和估计器都完成各自的工作。</p>
<p>&emsp;&emsp;网络的训练方式大致分以下四个步骤循环进行：</p>
<ul>
<li>使用公式 $(3)$ 训练估计器 $F_b$（回归训练，仅训练 $F_b$）</li>
<li>锁定 $F_b$，使用公式 $(5)$ 训练生成器 $G_{phy}$（对偶学习训练）</li>
<li>锁定 $G_{phy},D$，使用公式 $(4)$ 训练 $F_b$ 和 $G_{noise}$（相当于普通的 GAN 中 G 的训练）</li>
<li>锁定其他所有网络，使用公式 $(4)$ 训练 $D$（相当于普通的 GAN 中 D 的训练）</li>
</ul>
<h4 id="组件架构"><a href="#组件架构" class="headerlink" title="组件架构"></a>组件架构</h4><ul>
<li>$F_b,\ G_{phy},\ G_{noise}, \ D$</li>
</ul>
<p><img src="/HR_Dual_GAN/image-20221027144843034.png" alt="image-20221027144843034"></p>
<p>&emsp;&emsp;这个网络架构是纯 CNN 架构，看着这些 block 已经基本可以复现论文的网络了，但是可惜的是还没有开源。</p>
<ul>
<li>ROI-AF</li>
</ul>
<p><img src="/HR_Dual_GAN/image-20221027145003786.png" alt="image-20221027145003786"></p>
<p>&emsp;&emsp;将 STMap 按行分割，每行进行 1D-Conv，通过这种方式处理 BVP 和噪声分布差异以实现特征对齐。然后将对齐的特征根据通道和一维卷积连接起来。使用通道注意模型（即全局平均池化（GAP）后跟两个线性层（FC））来获得融合特征图。最后，融合的特征图被 reshape 成它的原始尺寸。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><ul>
<li>PURE 数据集训练，PURE 数据集测试，优于部分方法（有些方法没有展示，比如 Siamese-rPPG 在 PURE 上的效果为 MAE-RMSE = 0.51-1.56）</li>
</ul>
<p><img src="/HR_Dual_GAN/image-20221027152524476.png" alt="image-20221027152524476"></p>
<ul>
<li>PURE 数据集训练，UBFC 数据集测试，优于绝大多数方法</li>
</ul>
<p><img src="/HR_Dual_GAN/image-20221027152703290.png" alt="image-20221027152703290"></p>
<ul>
<li>消融实验：1.Noise-GAN（增强对噪音的鲁棒性）2.ROI-AF（增强 BVP 信号和噪音的分布一致性）</li>
</ul>
<p><img src="/HR_Dual_GAN/image-20221027152834826.png" alt="image-20221027152834826"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>I3D</title>
    <url>/I3D/</url>
    <content><![CDATA[<h2 id="Quo-Vadis-Action-Recognition-A-New-Model-and-the-Kinetics-Dataset-【视频动作识别】-CVPR2017"><a href="#Quo-Vadis-Action-Recognition-A-New-Model-and-the-Kinetics-Dataset-【视频动作识别】-CVPR2017" class="headerlink" title="Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset 【视频动作识别】 CVPR2017"></a><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Carreira_Quo_Vadis_Action_CVPR_2017_paper.pdf">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</a> 【视频动作识别】 CVPR2017</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种新的视频理解网络结构，和一个新的数据集。这两个都是里程碑式的工作，对于新网络架构，其名字叫做“膨胀的3D网络”，方法简单有效，仅是将 2D 卷积核进行 3D 膨胀，相对于以往的训练 3D 卷积核，关键就在于先训练 2D 网络，再在不改变权重的情况下进行扩张后进行微调。新的数据集也十分有效，大小适合，范围多样，是视频理解领域新工作绕不开的测试数据集。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/I3D/image-20221008130116447.png" alt="image-20221008130116447"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>提出了一种基于 2D 网络预训练得到的参数膨胀到 3D 的方法</li>
<li>提出了一个可用的视频领域经典数据集</li>
<li>保留了光流的预提取，结合了 3D 卷积核和光流预提取</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/I3D/image-20221008161145107.png" alt="image-20221008161145107"></p>
<p>&emsp;&emsp;这篇论文本质上是一个框架性质的论文，并没有提出自己的 backbone，而是结合旧有的网络结构进行扩展和计算，这里选取的网络是 inception，因为这个时候 inception 在视频领域表现还不错，不过很快 ResNet 就在视频领域也占据主导地位了，因此之后作者又选取 ResNet 做了一下。这些都无关紧要，仅作为例子讲解如何进行 inflate 操作。</p>
<h4 id="旧有视频理解网络结构"><a href="#旧有视频理解网络结构" class="headerlink" title="旧有视频理解网络结构"></a>旧有视频理解网络结构</h4><h5 id="2DCNN-LSTM-RNN"><a href="#2DCNN-LSTM-RNN" class="headerlink" title="2DCNN + LSTM/RNN"></a>2DCNN + LSTM/RNN</h5><p><img src="/I3D/image-20221008180646729.png" alt="image-20221008180646729"></p>
<p>&emsp;&emsp;这一类的网络通用方法是使用一个 2DCNN 网络进行逐帧的特征抽取，然后在抽取完成的特征上接入一个 LSTM/RNN，从而学习到时序信息。但是在之后的实践中我们发展这种方式得到的结果显著地不如使用光流这种手工特征进行时间建模，因此目前大致在逐渐废弃这种基于 2DCNN+LSTM，但是不排除使用 transformer 代替某一个小结构或者干脆把 CNN 和 LSTM 都换成 transformer 之后效果会变好，甚至成为 SOTA。 </p>
<h5 id="3DCNN"><a href="#3DCNN" class="headerlink" title="3DCNN"></a>3DCNN</h5><p>&emsp;&emsp;对于 3DCNN，关键就是理解如何进行 3D 卷积，为了更好地理解 3D 卷积过程，接下来从代码和可视化的方向分别表示：</p>
<ul>
<li>代码（pytorch）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#--------------------------------------conv2D------------------------------------------#</span></span><br><span class="line">layer_2d = nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">data_2d = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(layer_2d(data_2d).shape)</span><br><span class="line"><span class="comment">#data2d(N, C, H, W)</span></span><br><span class="line"><span class="comment">#layer2d(C_out, C_in, kernel_size, stride, padding)</span></span><br><span class="line"><span class="comment">#output(N, C_out, H_out = (H + 2*padding - kernel_size + 1)/stride, W_out = (W + 2*padding - kernel_size + 1)/stride)</span></span><br><span class="line"><span class="comment">#output.shape = (1, 8, 3, 3)</span></span><br><span class="line"><span class="comment">#--------------------------------------conv3D------------------------------------------#</span></span><br><span class="line">layer_3d = nn.Conv3d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">data_3d = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line"><span class="built_in">print</span>(layer_3d(data_3d).shape)</span><br><span class="line"><span class="comment">#data3d(N, C, D, H, W)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">注意：这里的D, H, W是指输入数据的维度，即视频的帧数，高度，宽度</span></span><br><span class="line"><span class="string">例如，输入的视频是每10帧进行堆叠，每帧是128*128的图片，图片的通道数为3，那么输入数据的维度就是(N, 3, 10, 128, 128)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#layer3d(C_out, C_in, kernel_size, stride, padding)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">注意：这里的kernel_size, stride, padding是指3D卷积核的大小，步长，填充，此时他们可以是一个元组分别指代每一个维度，也可以是一个数字代表每个维度都一致</span></span><br><span class="line"><span class="string">例如，kernel_size = (3, 3, 3), stride = (1, 1, 1), padding = (1, 1, 2)</span></span><br><span class="line"><span class="string">一般情况下，我们不在D维度上进行下采样，因为视频帧本就难以完整描述一个动作</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#output.shape = (1, 8, 10, 128, 128)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>图像</li>
</ul>
<p><img src="/I3D/3DCNN.png" alt></p>
<p>&emsp;&emsp;虽然 3DCNN 在一定程度上对时间序列的信息抽取能力超过了堆叠帧的 2DCNN，但是有两个缺点导致一直不温不火，实际上直到本文 I3D 提出之前，基本没有能和 SOTA 一战的任何 3DCNN 网络。这两个缺点分别是：1.3DCNN 的参数太多，相对与 2DCNN 模型要大很多倍，并且由于参数多导致不容易收敛以及效果一般。2.3DCNN 对时间序列信息的抽取能力仍然比不过光流输入的 2DCNN。</p>
<h5 id="two-stream-2DCNN"><a href="#two-stream-2DCNN" class="headerlink" title="two-stream 2DCNN"></a>two-stream 2DCNN</h5><p>&emsp;&emsp;<a href="https://bnucsy.github.io/Two_stream_CNN/">这里已经有了详细的描述。</a></p>
<h5 id="3D-fused-two-stream"><a href="#3D-fused-two-stream" class="headerlink" title="3D fused two-stream"></a>3D fused two-stream</h5><p><img src="/I3D/image-20221008215156195.png" alt="image-20221008215156195"></p>
<p>&emsp;&emsp;这种网络的思路也非常简单，但是其实和 I3D 在某些程度上已经近似了，这种网络将之前先计算出特征然后融合的 late fusion 改成了先融合经过 3DCNN 头再输出，即变成了 early fusion。 这其实确实解决了问题，对于 3DCNN 的参数量大难以循环的问题，因为这个网络里只有 3DCNN 的头，较少的参数使得其训练并不困难，模型大小也没有太大。除此之外这种网络结构也使用了光流，没有因为使用 3DCNN 而造成时间序列信息的欠学习。说白了，只有一点不太行，就是结果不好，当然 I3D 的提出巧夺天工，但是 3DCNN fusion 的提出也不能说活该被埋没，纯粹是谁效果好谁就是更好的算法，单从结构上实在很难判断孰优孰劣。</p>
<h4 id="本文具体网络架构"><a href="#本文具体网络架构" class="headerlink" title="本文具体网络架构"></a>本文具体网络架构</h4><p>&emsp;&emsp;本文的网络很简单，基本采用的就是双流网络，只不过把双流网络的两个子网络都换成 3DCNN。照理说这样会陷入一个问题：太多的参数导致难以训练。但是 I3D 提出了一个巧妙的构想：我们不从头训练 3DCNN，而是训练一个和双流网络一样的 2DCNN，再将其中的所有 2 维的卷积核和池化层全部变成 3 维。这样我们得到了参数已经初始化好的 3DCNN，然后我们在数据集上做微调即可。 </p>
<p>&emsp;&emsp;这个思路简单明了，接下来我们主要探究如何进行 2D 扩张。作者给了一个简单的想法，对于一个 NxN 的 2D 卷积层，首先把所有的单张帧进行 N 次复制粘贴得到一个 N 帧相同图片的视频，然后将 2D 的卷积核也进行 N 次复制粘贴，得到 NxNxN 的 3D 卷积核，每个卷积核和对应的图片帧进行卷积运算，这样得到 N 个相同的值，接下来就可以直接输入下一层，对于 pooling 层也是这样，复制 N 份，然后逐层 pooling。以此为基础得到的最终结果是 N 份的旧网络结果，将这个结果取平均，理论上效果应该和旧网络的结果一模一样。以此就可以初始化 3D 网络，再进行微调即可。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>video</category>
      </categories>
  </entry>
  <entry>
    <title>HR_STVEN_rPPGNet</title>
    <url>/HR_STVEN_rPPGNet/</url>
    <content><![CDATA[<h2><center>Remote Heart Rate Measurement from Highly Compressed Facial Videos: an End-to-end Deep Learning Solution with Video Enhancement</center></h2>

<p>【心率检测】【ICCV2019】【<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Remote_Heart_Rate_Measurement_From_Highly_Compressed_Facial_Videos_An_ICCV_2019_paper.pdf">paper</a>】【<a href="https://github.com/ZitongYu/STVEN_rPPGNet">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种可以加强压缩视频的网络 STVEN 和一个更有效的 rPPG 信号估计网络 rPPGNet，总共探索了包括 x264/AVC，x265/HEVC，MPEG4 三种传统压缩算法，整体的网络结构可以接受压缩后的视频并且获得在同等输入条件下相对于其他网络结构的最优解。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/HR_STVEN_rPPGNet/STVEN_network.png" alt></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>一个可以增强视频质量的网络 STVEN，相比于其余的增强/去噪网络对 HR 更有效</li>
<li>一个更加有效的 rPPG 信号估计网络 rPPGNet，其具备复杂的约束条件</li>
<li>一个在 rPPGNet 中用于计算注意力图的无参数模块</li>
<li>第一次实际意义上探索了使用压缩视频作为输入的网络结构</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p>&emsp;&emsp;整体的网络结构正如概览图所示，整体来说分为 STVEN 和 rPPGNet 两个子模块，其一负责加强视频质量，其二负责从视频输入中重建 rPPG 信号。具体来说，这两个子模块都是基于 CNN 的架构，参数分别如下图所示，其中的 ST_Block 是 R(2+1)D 的结构，SGAP 是空间 Global Average Pooling 的缩写。在训练的过程中，两个子模块进行联合训练。</p>
<p><img src="/HR_STVEN_rPPGNet/image-20221031160806850.png" alt="image-20221031160806850" style="zoom:50%;"></p>
<h4 id="STVEN"><a href="#STVEN" class="headerlink" title="STVEN"></a>STVEN</h4><p>&emsp;&emsp;STVEN 是一个 video-2-video 的网络结构，在此工作的预想下可以从一些特定的比特率下压缩的视频还原出 GT，并且具备尽可能好的 HR 语义信息。整个 STVEN 的网络基于一个未证明的假设：在不同的比特率下的压缩图像的数据分布不同，而同一个比特率下数据分布较为一致。首先介绍一些符号表示：</p>
<ul>
<li><p>压缩后的视频序列为 $\C=[0,1,2,…,C]$，其中 $0,C$ 分别表示多种压缩率下的最低和最高比特率</p>
</li>
<li><p>对于某一个比特率下的视频，记为 $c_k^\tau=[c_{k1},c_{k2},…,c_{k\tau}]$，其中 $k\in\C$，$\tau$ 表示视频的帧数</p>
</li>
<li>对于 STVEN，记为 $G$，其接受两个输入，包括：$c_i^{\tau}$ 和 $k$，即长度为 $\tau$ 的压缩率为 $i$ 的视频和数字 $k$。输出为 根据 $c_i^\tau$ 更改其数据分布得到的新的压缩率为 $k$ 的视频，表示为 $\hat c_k^\tau=[\hat c_{k1},…,\hat c_{k\tau}]$</li>
</ul>
<p>&emsp;&emsp;在训练的过程中我们主要希望 $G$ 函数能够达到两个优化目标：1.良好的重建性，2.循环处理不变性。为了达到良好的重建性，本文选取了两个指标进行约束，分别是 MSE 和 L1 损失。因此 loss 写为：</p>
<script type="math/tex; mode=display">
\rm \mathcal L_{rec} = E_{k\in\C,t}||c_0^\tau(t)-G(c_k^\tau,0)(t)||_2^2+E_{k\in\C,t}||c_k^\tau(t)-G(c_0^\tau,k)(t)||</script><p>&emsp;&emsp;其中 $t\in [1,\tau]$ 表示该视频的第几个帧，至于为什么 loss 形式不一样，作者简单说了一下，估计就是效果不好吧，毕竟 L1 L2 应该都是基于像素级别的损失。</p>
<p>&emsp;&emsp;为了达到良好的循环处理不变性，也就是过两次 $G$ 之后仍然保持一致，loss 描述为：</p>
<script type="math/tex; mode=display">
\rm \mathcal L_{cyc}=E_{k\in\C,t}||c_k^\tau(t)-G(G(c_k^\tau,0),k)(t)|| + E_{k\in\C,t}||c_0^\tau(t)-G(G(c_0^\tau,k),0)(t)||</script><p>&emsp;&emsp;两个 loss 的组合可以表示为：</p>
<script type="math/tex; mode=display">
\rm \mathcal L_{STVEN}=\mathcal L_{rec}+ \mathcal L_{cyc}</script><p>&emsp;&emsp;通过这样的约束条件训练出的 STVEN 支持将任何比特率的视频压缩/解压缩到任何比特率，尤其对于低比特率的视频可以通过解压缩的方式实现数据增强。但是局限性是明显的：参与训练的数据的压缩算法是传统且局限的，目前基于深度学习的表征压缩数据大概并不服从同一分布。</p>
<h4 id="rPPGNet"><a href="#rPPGNet" class="headerlink" title="rPPGNet"></a>rPPGNet</h4><p>&emsp;&emsp;rPPGNet 由三个部分组成：时空卷积网络，基于皮肤的注意力模块，分阶段的约束模块。</p>
<p>&emsp;&emsp;对于时空卷积网络，其本身是基于 R(2+1)D 的 3D CNN 网络，具体参数在之前已经展示，像概览图中所示，其本身是一个逐步降采样的过程。需要注意的是，这里的目的是从 ECG 信号中恢复 rPPG 信号，由于两个信号只需要具备同样的峰值而不需要具备同样的值，因此采用负皮尔森相关系数作为 loss，描述为：</p>
<script type="math/tex; mode=display">
\rm \mathcal L_{np}=1-\frac{T\mathop\sum\limits_{i=1}^T y_iy_i^g-\mathop\sum\limits_{i=1}^Ty_i\mathop\sum\limits_{i=1}^Ty_i^g}{\sqrt{(T\mathop\sum\limits_{i=1}^Ty_i^2-(\mathop\sum\limits_{i=1}^Ty_i)^2)(T\mathop\sum\limits_{i=1}^T(y_i^g)^2-(\mathop\sum\limits_{i=1}^Ty_i^g)^2)}}</script><p>&emsp;&emsp;同时，本文将在时空卷积网络的中间层（第三个 ST_Block）的输出单独拿出来，希望这个图也能和 ECG 信号保持线性一致性，因此对于第一个模块，loss 描述为下式，其中 $\alpha,\beta$ 用于平衡两部分的重要性，$\beta$ 要小一些。</p>
<script type="math/tex; mode=display">
\rm \mathcal L_{rPPG}=\alpha\mathcal L_{np}+\beta\mathcal L_{np}^{mid}</script><p>&emsp;&emsp;对于基于皮肤的注意力模块，其是一个没有参数的计算模块，具体计算方式如下图所示，这个模块理论上是即插即用的模块，但是在本文中仅在第一个 ST_Block 后加入了这个模块（代码里面写的有点乱没看懂，主要问题在于论文里面已经说了这个模块是无参数的，但是在代码之中和 skin 相关的代码里面居然有 conv：答案：基于皮肤的注意力中，皮肤分割是有参数的，注意力是没参数的），最后这个模块选择使用交叉熵函数作为 loss，论文中没有给出公式，但是根据代码看起来是这样：$\rm \mathcal L_{skin}=CB(M,M^{GT})$，其中 $\rm M=Softmax(\sigma(AvgPool(F))+\sigma(MaxPool(F))+S)$，$F\,,S$ 在下图中给出。</p>
<p><img src="/HR_STVEN_rPPGNet/image-20221031200834676.png" alt="image-20221031200834676" style="zoom:67%;"></p>
<p>&emsp;&emsp;对于分区的约束模块，将整个模型最后输出的 skin 图像分为四个部分如下图所示，对于每个部分经过 AVGPool 和一个 conv3d 可以得到输出，这些输出表示每个部分预测出的 rPPG 信号，这些 rPPG 信号也需要和 ECG 信号具备同样的线性一致性，因此也使用负皮尔森相关性 loss：$\rm \mathcal L_{parts}=\mathop\sum\limits_{i=1}^N\mathcal L_{np}^{part_i}$</p>
<p><img src="/HR_STVEN_rPPGNet/image-20221031204557742.png" alt="image-20221031204557742" style="zoom:67%;"></p>
<p>&emsp;&emsp;总的来说，对于 rPPGNet，其 loss 描述为：</p>
<script type="math/tex; mode=display">
\rm \mathcal L_{rPPGNet}=\mathcal L_{rPPG} + \gamma\mathcal L_{skin}+\delta\mathcal L_{parts}</script><p>&emsp;&emsp;为了两个模块的联合训练，本文又引入了一个联合训练的感知损失 $\mathcal L_p$，真给我整破防了，这也太长了，代码开源也就开了一半，这么复杂的限制和损失居然是有效的？</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;消融实验如下图所示，从下图可以看出每一部分都是有必要的，并且这些小模块可以结合在一起得到更好的效果。需要注意的是这里仅展示了 rPPGNet，没有 STVEN 部分。<img src="/HR_STVEN_rPPGNet/image-20221031211829326.png" alt="image-20221031211829326"></p>
<p>&emsp;&emsp;和其他方法对压缩视频的效果对比，论文包含 x264,x265,MPEG 三种压缩方法，这里只展示 x265 的：<img src="/HR_STVEN_rPPGNet/image-20221031212837912.png" alt="image-20221031212837912"></p>
<p>&emsp;&emsp;通过这张图大致可以看出本文的网络是具备一定效果的，需要注意的是这里仅展示了 rPPGNet，没有 STVEN 部分，但是也就那样，没比其他的好了多少。</p>
<p>&emsp;&emsp;接下来接上 STVEN 之后和仅有 rPPGNet 单独做了对比实验，实验结果如下图所示，可以看出加上 STVEN 之后是多少有提点的，提点的效果并不是比特率越高越明显，因为他还做了一个 MPEG 的实验，平均提点大约 2。（但是其实随便找一个自动压缩视频的网站对视频进行上传再下载，他肯定掉点，现在各大网站的压缩算法都是自己做的，不再用 HEVC 了）</p>
<p><img src="/HR_STVEN_rPPGNet/image-20221031213025215.png" alt="image-20221031213025215" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>基于transformer的光场超分辨率重建</title>
    <url>/Light%20Field%20Image%20Super-Resolution%20With%20Transformers/</url>
    <content><![CDATA[<h2 id="Light-Field-Image-Super-Resolution-With-Transformers【光场超分】【SLP】"><a href="#Light-Field-Image-Super-Resolution-With-Transformers【光场超分】【SLP】" class="headerlink" title="Light Field Image Super-Resolution With Transformers【光场超分】【SLP】"></a><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9695363">Light Field Image Super-Resolution With Transformers</a>【光场超分】【SLP】</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文将transformer方法应用到了光场超分的领域，提出了一个简单且有效的baseline，相对于SOTA具有更小的计算代价和更优的效果。具体来说，提出了角度转换和空间转换两个模块，意图更好地学习不同视图之间的互补性。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Light%20Field%20Image%20Super-Resolution%20With%20Transformers/image-20220504094155843.png" alt="image-20220504094155843"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><script type="math/tex; mode=display">
math\_express</script><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><hr>
<h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3>]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>LTML</title>
    <url>/LTML/</url>
    <content><![CDATA[<h2><center> Long-Tailed Multi-Label Visual Recognition by Collaborative Training on
Uniform and Re-balanced Samplings </center></h2>

<p>【长尾识别】【ICCV2021】【<a href="https://ieeexplore.ieee.org/document/9578135/">paper</a>】【<a href>code未开源</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种双分支网络，分别对均匀采样和<strong>重平衡采样</strong>的数据进行处理，对于每个网络的结果分别计算<strong>带有样本数先验的</strong>分类误差，同时为了消除两个网络可能针对自己的数据集产生的过拟合问题，提出了<strong>一种交叉损失</strong>使两个网络能够具备一致性的 bias。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/LTML/image-20231010104505727.png" alt="image-20231010104505727" style="zoom:50%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>一个双分支网络，同时使用均匀采样和 re-balanced 采样进行预测</li>
<li>一个新的一致性损失来约束双分支网络的一致性</li>
<li>一个仿照 focalloss 在 BCE 上的改进损失</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/LTML/image-20231009213745051.png" alt="image-20231009213745051" style="zoom:50%;"></p>
<p>&emsp;&emsp;数据准备：首先对于数据集进行两种不同的采样，使用均匀采样（每个样本被选中的概率为 $\frac1N$）得到的训练集（共 $N$ 个）称为 $X$，使用 re-balanced 采样（即每个样本被选中的概率为 $\frac1K\cdot \frac 1{N_k}$，$K$ 表示数据集共有多少类别，$N_k$ 表示该样本所属类别的数目）得到的训练集称为 $X’$。对应的 label 分别为 $Y,Y’\in(N,K)$，每行对应一个 one-hot 向量。</p>
<p>&emsp;&emsp;网络前向：如上图所示的 $\phi$ 表示去掉最后一层的卷积 ResNet，$f_1,g_1$ 均表示 ResNet 的最后一层，但是不共享参数。$f_2,g_2$ 均表示由 MLP 组成的分类头，但是不共享参数。从 $X,X’$ 中的数据（分别随机采样的数据）首先统一进入 $\phi$，得到的特征分别进入两个分支，记进入 $f$ 分支的为 $x^u$，$g$ 的为 $x^r$。经过上下两个子分支之后共输出 4 个分类结果：</p>
<ul>
<li>$u=f_2(f_1(\phi(x^u)))$</li>
<li>$\hat r=f_2(f_1(\phi(x^r)))$</li>
<li>$\hat u=g_2(g_1(\phi(x^u)))$</li>
<li>$r=g_2(g_1(\phi(x^r)))$</li>
</ul>
<p>&emsp;&emsp;因此，可以简单地想到，$u,\hat u,y^u$ 应该一致，$r,\hat r,y^r$ 应该一致，分别计算 loss 即可。</p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>&emsp;&emsp;对于 $(u,y^u)$ 和 $(r,y^r)$，其约束的是每个分支正确预测对应类别概率的能力，因此采用分类损失计算：</p>
<script type="math/tex; mode=display">
\mathcal L_{cls}=-\frac1K\sum_{k=1}^{K}w_k(y_{ik}^u\log(\zeta(u_{ik}\cdot\sigma_k^p+\mu_k^p)))+(1-y_{ik}^u)\log(1-\zeta(u_{ik}\cdot\sigma_k^n+\mu_k^n))</script><p>&emsp;&emsp;其中，$i\in[1,N]$，$y_{ik}$ 表示第 $i$ 个样本是否为第 $k$ 个类别（0/1），$w_k=y_{ik}^ue^{1-\rho}+(1-y_{ik}^u)e^\rho,\ \rho=\frac{N_k}N$ ，$\sigma_k^p,\mu_k^p$ 分别表示第 $k$ 个类别正样本的方差均值，上标为 $n$ 的表示负样本。</p>
<p>&emsp;&emsp;简单理解为：</p>
<ul>
<li>$w_k $ 是一个 class/sample-ware 的参数，该类别的正样本越多，对错误的惩罚就越小</li>
<li>所谓正样本数就是属于该类别的样本 $N_k$，$\sigma_k^p,\mu_k^p$ 表示的是 $Y$ 中所有满足第 $k$ 个值为 1 的行组成集合的均值方差</li>
<li>$u_{ik}\cdot\sigma_k^p+\mu_k^p$ 中的 $u_{ik}$ 是一个概率，属于 $(0,1)$，这样写而不是直接使用 $u_{ik}$ 可以再对由于正负样本数量不同产生的 bias 进行修正</li>
<li>整体的损失表示：当 $y_{ik}^u$ 为 1 时，$u_{ik}$ 越大越好，反之则越小越好；$w_k$ 使正样本多的类别权重减小；$u_{ik}\cdot\sigma_k^p+\mu_k^p$ 使正样本多的类别最终预测出的概率值偏低，负样本多的类别最终预测出的概率值偏高。</li>
</ul>
<p>&emsp;&emsp;而对于 $(\hat u,u)$ 和 $(\hat r,r)$，其约束的是两个模型对同样的输入得到同样输出的能力（也就是限制模型对于数据的 bias）：</p>
<script type="math/tex; mode=display">
\mathcal L_{con}(u_i,\hat u_i)=\frac1K\sum_{k=1}^K(u_{ik}-\hat u_{ik})^2</script><p>&emsp;&emsp;这个损失很直观，就是两个模型对于同样的输入应该得到同样的输出。</p>
<p>&emsp;&emsp;最终的 loss 为：</p>
<script type="math/tex; mode=display">
\mathcal L(x^u_i , x^r_j ; y^i_u, y^j_r) =\mathcal L_{cls}(u_i, y_i^u) + \mathcal L_{cls}(r_j , y_j^r)+λ(\mathcal L_{con}(u_i, \hat u_i) + \mathcal L_{con}(r_j ,\hat r_j ))</script><p>&emsp;&emsp;$\lambda$ 是平衡权重的超参。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><ul>
<li>nonsense</li>
</ul>
<p><img src="/LTML/image-20231009223810488.png" alt="image-20231009223810488" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>cv</category>
        <category>LongTail</category>
      </categories>
  </entry>
  <entry>
    <title>毕设进度记录</title>
    <url>/LightFieldCompress/</url>
    <content><![CDATA[<h2 id="基于单张RGB图像的光场表征"><a href="#基于单张RGB图像的光场表征" class="headerlink" title="基于单张RGB图像的光场表征"></a><center>基于单张RGB图像的光场表征</center></h2><p>&emsp;&emsp;本文是为了完成毕业论文进行的文献收集和阅读，在此对各个具备较高参考性的文章进行总结提炼，本文最主要参考的文章进行了精读，<a href="http://bnucsy.gitee.io/2022/05/02/A%20Learned%20Compact%20and%20Editable%20Light%20Field%20Representation/">链接见此</a>。初步的设想下，本文的自编码器分为编码器和解码器两个子网络，编码器把一个光场输入编码为一个RGB图像，光场的结构性信息理论上被隐式地编码到RGB图像之中。而解码器则利用这个图像进行光场重建。</p>
<span id="more"></span>
<p>&emsp;&emsp;事实上，已经有能够只从一个RGB图中提取信息重建光场的工作，作为自编码器，由于RGB图中存在着光场结构，至少效果要优于这篇<a href="https://arxiv.org/pdf/1708.03292.pdf">从一张RGB图重建光场</a>的文章。为了达到这个效果，我认为解码器需要能够分离出RGB特征，我将参考【1】中的 $SepNet$ 。</p>
<h3 id="2021-11-19-2021-11-26"><a href="#2021-11-19-2021-11-26" class="headerlink" title="2021/11/19 - 2021/11/26"></a><code>2021/11/19 - 2021/11/26</code></h3><p><code>参考文献【2】</code></p>
<p>&emsp;&emsp;这篇文献中对如何利用光场 $L$ 的 $L(x,u_{0})$ 还原出整个光场 $L$ 提出了一个可行的方法，该方法是一个基于物理学模型和CNN网络的学习结构，具体如下：</p>
<p>&emsp;&emsp;记模型为函数 $f$，有$\widehat L(x,u)=f(L(x,u_0))$。本文将这个函数分为了三个部分，函数 $d(·)$ 用来估计深度图，函数 $r(·)$ 用来使用深度图和光场的第一张图用来近似估计光场 $\widehat L_r$，最后使用函数 $o(·)$ 来估计遮挡和非朗博效应。</p>
<script type="math/tex; mode=display">
D(x,u) = d(L(x,u_0))\\
L_r(x,u) = r(D(x,u),L(x,u_0))\\
\widehat L(x,u)=o(L_r(x,u),D(x,u))</script><p>&emsp;&emsp;这三个部分逐个构成了本文的结构，接下来将逐个介绍这三个模块。</p>
<ol>
<li><p>深度图预测估计函数 $d$</p>
<p>&emsp;&emsp;函数 $d$ 是一个基于CNN的网络结构，我们将函数 $d$ 和函数 $o$ 的关键损失（一致性损失）进行联合计算，联合计算的原因是防止 $o$ 过拟合从而导致 $d$ 欠拟合。联合训练误差表示为：</p>
<script type="math/tex; mode=display">
\min_{\theta _d, \theta _0}\Sigma _S [||L_r-L||_1+||\widehat L - L||_1+\lambda_c \psi _c(D)+\lambda_{tv}\psi_{tv}(D)]</script><p>&emsp;&emsp;即，求解使得在数据集 $S$ 中所有的数据误差最小的 $\theta_d$ 和 $\theta_o$ ，这两个参数分别是深度估计网络和遮挡预测网络的参数<code>（具体是什么参数？）</code>。$\psi_c,\psi_{tv}$ 是一致性的正则化损失，用来限制深度图的光线角度一致性以及鼓励网络参数的稀疏性。他们分别表示为：</p>
<script type="math/tex; mode=display">
\psi_c(D(x,u))=||D(x,u)-D(x+D(x,u),u-1)||_1 \\ \psi_{tv}(D(x,u))=||\nabla_xD(x,u)||_1</script></li>
<li><p>光场预还原函数 $r$ </p>
<p>&emsp;&emsp;光场预测函数是一个基于物理的函数，和参考文献【1】的 $warp$ 部分很接近，具体的表达式为：$L_r(x,u)=L(x+uD(x,u),0)$，其中 $D(x,u)$ 就是之前预测得到的深度，本公式相当于使用深度图当做光场的光线入射角度，根据每个点的截面变化为固定值得出近似光场。</p>
</li>
<li><p>光场遮挡修正函数 $o$ </p>
<p>&emsp;&emsp;关于遮挡修正，我们实际上在函数 $d$ 中已经对其进行了限制，将之前的损失函数后向传播就可以完成函数 $o$ 的训练。值得一提的是，本函数的网络结构并不是将近似的光场 $L_r$ 直接变为 $\widehat L$，而是学习一个残差块 $\widetilde o$ ，这将更加保证学习到的网络具备填补遮挡的预测效果。其中 $\widetilde o$ 表示为：</p>
<script type="math/tex; mode=display">
o(L_r(x,u),D(x,u))[即 \widehat L]=\widetilde o(L_r(x,u),D(x,u)) +L_r(x,u)</script></li>
</ol>
<p>$~~~~$关于这几个函数预测网络的具体结构，这些网络均为普通的CNN结构，具体结构存在于支撑文件和源代码之中，我并没有找到。但是据本文作者所说，他在网络结构上并没有太大的创新，只是在网络的激活函数之中，本文发现使用 Tanh 要优于 ELU。</p>
<p><code>关于毕设的一些讨论</code><br>&emsp;在指导老师指导下进一步明确了毕设的内容：<br>&emsp;&emsp;（开题用）motivation —— 1.当前的光场数据压缩算法大多压缩结果不具有直观性，因此想要做出来一个使用和光场中心视图相似来约束的自编码器，中间层即为压缩结果。 2.这种可视化的压缩能够帮助人们认知原光场结构，并且和图片相似的结构支持基于图片的压缩（如JPEG），而这个特性能够弥补最初可能产生的压缩比不足的缺陷。<br>&emsp;&emsp;（baseline）method —— 1.数据上采用<a href="https://github.com/YingqianWang/LF-DFnet">这篇文章</a>的数据集，网络上首先使用参考文献【1】的编码器作为编码和解码网络，同时结合 UNet 完成最初版本网络设计，然后再将网络结构逐渐复杂化，预想中我们将会使用<a href="https://github.com/YingqianWang/LF-DFnet">这篇文章</a>的网络结构。 2.完成这些之后我们会进行更加精细的更改，如增加PNSR和SSIM损失来优化小型区域的分辨率问题，增加可逆的量化损失让网络具备更好的结构。<br>&emsp;&emsp;（后话）paper —— 1.在论文的编写过程中需要多评价分析现有的压缩算法，比较各自的优劣，尤其强调自己的亮点。 2.对每个视角进行统计，全方位地估计自己方法的不足。 3.除去压缩比，本方法的压缩优势还包括了可以支持一些后续工作，如进行三维重建以及重聚焦。<br>&emsp;&emsp;（当务之急）todolist —— <code>1.从 LF-DFnet 论文中下载光场相关数据</code>，<code>2.阅读参考文献【1】的代码部分，尤其是编码器以及重写的 dataset 类</code>，<code>3.多查询博客以及2012-2017年间的经典网络结构论文，这些资源将会对“为什么要”和“为什么能”的问题提供帮助，如：</code><a href="https://www.i4k.xyz/article/qq_43703185/105060277">UNet和FCN</a>、<a href="http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/">深度学习的基本原理1</a>、<a href="http://neuralnetworksanddeeplearning.com/">深度学习的基本原理2</a></p>
<h3 id="2021-11-26-2021-12-3"><a href="#2021-11-26-2021-12-3" class="headerlink" title="2021/11/26-2021/12/3"></a><code>2021/11/26-2021/12/3</code></h3><p>&emsp;&emsp;很惭愧，这周我买了两个游戏——传送门1和传送门2，总共肝了20+小时通关了这两个游戏，所以这周几乎没有做任何有意义的工作。唯一有意义的是我在大风天去爬了香山逛了植物园，让我的右膝盖疼了两天，这件事实际上意义重大，我决心老老实实开启养老模式。早睡早起按时从实验室滚蛋，每天至少喝两杯水，每坐一小时要站起来走十分钟等等。<br>&emsp;&emsp;不过我确实读完了 Unet 网络的论文，只是还没来得及写总结。<br>&emsp;&emsp;以及完成了一下当时老师布置的任务，包括：①下载数据<code>这数据也50G+，开玩笑呢，所以这个任务莫得完成</code>，②参考文献【1】的 Encoder 层试着设计一个初步的网络出来<code>这个我倒是完成了（刚完成）</code>，③深度学习基本原理<code>这个得慢慢来对吧，我暂时没有太看也可以理解对吧</code>（但是还是明白了一些东西）</p>
<p>&emsp;&emsp;任务②：本论文的网络结构由两个Unet网络组成，两个Unet具有完全相反的特性，因此在此处只给出初版的 encoder 网络（这个网络将 49 个 128*128*3 的光场照片提取特征，并最终映射到 128*128*1 的空间之中，后续将会设计一个损失函数，使用中央视觉图来限制中间层）：<br><img src="https://img-blog.csdnimg.cn/f8fd481d205e446dac7dbad06e9a0af5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>&emsp;&emsp;任务③：大致了解了这些网络设计原理（持续增加）</p>
<ul>
<li>提取特征（降采样）之后网络的通道数一般会增加，这是因为我们希望提取特征的同时尽量不损失信息。</li>
<li>在各个降采样之前（或之后）我们需要进行几次卷积，这是因为卷积层越深，其非线性可分性就越强，但是过深的网络会造成运算量和内存消耗太大以及过拟合的问题，通常使用两层卷积（像是个传统？）</li>
<li>在卷积之后往往我们需要使用一个线性激活函数，这类函数最初是 $sigmod, tanh$ ，设计他的原因是：①将输出映射到一个特定的区间里便于计算，②把纯线性的连接转化为非线性的连接（这个更重要）。但是这样的激活函数计算过于复杂，这将导致在后向传播的时候运算较慢，因此现在用 $RELU$ 的比较多，这个网络结构就是如此，只在最后一个输出映射的时候使用 $tanh$。</li>
<li>有些卷积层是 $kernel_size = stride = 1, padding = 0$ 的卷积，这种卷积看似没有进行卷积操作，实际上对不同的通道进行了运算，往往在最后需要调整通道数的时候使用这种卷积层，如本网络最后一层。</li>
<li>可变形卷积：相当于对卷积核的采样点进行了偏移，对于 $b\times h\times w\times c$ 的特征图 $U$，添加一个卷积核，通过 $padding=same$ 输出同样的大小，输出后的 $size=b\times h\times w\times 2c$，称为 offset，offset 的两个通道分别记录了该像素点向水平和竖直方向的偏移量。接着我们把 offset 和 $U$ 进行合并，得到代表绝对坐标的 $V（size=b\times h\times w\times 2c）$，如此我们再对 $U$ 进行普通卷积，卷积时对于 $U$ 中的某一个像素点，我们寻找其在 $V$ 作用下的偏移坐标，往往得到的是一个浮点数。由于此位置并没有实际像素点，我们还需要对其临近的四个实际像素点进行双线性插值得到该像素点的数值。可以看出可变形卷积具备更多的参数和更好的注意力机制，因此往往我们在整个网络的最后 3-4 层使用该卷积。</li>
</ul>
<p><code>PS. 接下来的任务</code>：<br>&emsp;&emsp;①完成Unet论文的总结<br>&emsp;&emsp;②再读几篇经典论文（VGG,ResNet）<br>&emsp;&emsp;③试着设计一下网络的损失函数</p>
<h3 id="2021-12-3-2021-12-27"><a href="#2021-12-3-2021-12-27" class="headerlink" title="2021/12/3-2021/12/27"></a><code>2021/12/3-2021/12/27</code></h3><p><code>文献【4】—— Unet 网络</code></p>
<p>&emsp;&emsp;Unet 提出了一种可以使用较少的数据进行数据增强和端到端训练图像的方法，这个方法最初用于进行细胞分割。后来人们发现这种具有特征提取结构和对称的扩展结构的网络能够在各个应用场景下表现突出，因此Unet就成为了一种经典的网络结构。<br>&emsp;&emsp;Unet 的相关工作主要体现在细胞分割上，因此这里不介绍它的训练策略（主要是分割的学习方式），仅关注其网络结构：</p>
<center><img src="https://img-blog.csdnimg.cn/78ab4c0af28d4a7ea4f380a529f9db2e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" width="80%"></center>

<p>啊这周已经过去了啊，现在已经是12/12号了，任务呢，一个也没有完成，Unet倒是看完了，其他的论文都没怎么读。这周倒也不算浑浑噩噩，把实验的代码写了个大差不差，基本架构倒是都出来了，跑了个马马虎虎的结果。把结果放一下：</p>
<p>原光场的中央视图：<img src="https://img-blog.csdnimg.cn/c9b318035f95463580cbf7c1eb2b8045.jpg" alt="在这里插入图片描述">  &emsp;&emsp;编码器的输出：<img src="https://img-blog.csdnimg.cn/cc5e399949834ca89c1880207a5355ba.jpg" alt="请添加图片描述">&emsp;&emsp;解码器的输出:<img src="https://img-blog.csdnimg.cn/16d8cc2fa919496c8ac736427a355870.jpg?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" width="24%"><br>&emsp;&emsp;问题还是十分明显的：①压缩后的图像精读并不够，但是这一个差得不是太多。②最关键的是这光场，他不会动啊！<br>&emsp;&emsp;这个问题大概是因为我设计的损失函数极其简单：$$ \zeta=0.5*||L-L_r||_2 + ||C-L_c||_2$$$~~~~$对，就是这么简单。简单的后果就是，自编码器学会了单纯地把中央视觉图复制到光场的每一个角度分辨率上，这样虽然跟光场没啥关系了，但是让我这个损失在一定程度上变得更小了，确实是一个局部的最优。<br>&emsp;&emsp;因此现在我需要思考如何设计损失才能限制住重建之后的光场长得像一个光场，而不是81个一模一样的图片。</p>
<p>&emsp;&emsp;在此之后，我又设想出来了一种新的结构：ETDnet，这种结构是将有中间层限制的自编码器的中间层和结果分别加一个判别器，所以这个结构看起来也像两个GAN。网络结构图如下：<br><img src="https://img-blog.csdnimg.cn/0d0954d88647447aa2aa3f40b9eb1583.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp;&emsp;其中 $E$ 和 $D$ 分别是两个 $U-net$，作为编码器和解码器， $L_D$,$F_D$ 分别是两个基于 $DCGAN$ 的判别器，用来和自编码器进行对抗。这个还只是初步的设想，目前跑出来了不加判别器的结果，应用于图像重上色任务：<br><img src="https://img-blog.csdnimg.cn/243f53761e19473ea905d9044df9d03b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><em>从左至右依次为：原图、压缩后灰度图、重上色图</em></p>
<p>&emsp;&emsp;事实上如果仔细观察的话，可以看出灰度图的中间有着一些竖着的条纹，这些条纹理论上就是隐编码，但是这并不是我们想要的，因此需要加入一个判别器和编码器进行对抗，使编码器能够更好地生成灰度图。然而最难的还是调参，判别器很快就可以训练得非常好，这样就会导致自编码器直接摆烂，目前我还在艰难地调学习率。</p>
<p>&emsp;&emsp;说回光场的任务：关于压缩后中央视图色调不一致的问题，加入量化损失之后即可解决，关于恢复之后不一致的问题，我尝试了：①使用更大的数据集，这个数据集是灰度数据，角度分辨率为5<em>5，空间分辨率为64\</em>64，共有40000+个光场。②损失函数中加入视差图的学习。③调整损失权重，直至只限制光场重建损失。<br>&emsp;&emsp;遗憾的是，上述三个结果都没有达到预期，计算得到的PSNR值为33+，数据上看起来并不差，但是一旦可视化，模糊和晃动就很明显。目前我获取到了一个更大的彩色数据集，我将利用这个彩色数据集进行进一步的训练。</p>
<h3 id="2021-12-27-2022-1-1"><a href="#2021-12-27-2022-1-1" class="headerlink" title="2021/12/27 - 2022/1/1"></a><code>2021/12/27 - 2022/1/1</code></h3><p>&emsp;&emsp;接下来的四天里面，我将要提交开题报告，因此我需要阅读 $LF_DFnet$ 的论文，这理论上将作为最终的网络结构，因此需要在目录中提及。<br>&emsp;&emsp;经过四天的讨论和反复修改，目前开题报告已经完整完成了，开题报告中，本文将改名为 “基于单张RGB图像的光场表征” 完成具备JPEG压缩鲁棒性的光场RGB表征和重建。本文初步定下来的不含有 $LF_DFnet$ 部分，直接使用基于 $Unet$ 的自编码器。接下来就是把代码做出来了。<br><img src="https://img-blog.csdnimg.cn/1b7b77a649f842818947cc9e2282d082.png" alt="在这里插入图片描述"></p>
<h3 id="2022-1-1-2022-1-19"><a href="#2022-1-1-2022-1-19" class="headerlink" title="2022/1/1 - 2022/1/19"></a><code>2022/1/1 - 2022/1/19</code></h3><p>&emsp;&emsp;啊首先经过了两个星期的期末考试【因为自认为能学会文科辅修了新闻传播学 : ) 】<br>&emsp;&emsp;经过不懈的努力（划水），实验终于有了进展，润回家后被隔离的第一天结果跑了出来，PSNR很神奇地达到了36+，接着我限制了中央视图，目前的PSNR已经可以交差了，光场PSNR达到35+，中央视图表征PSNR达到42+。</p>
<ul>
<li>原光场（大小：11.3MB）：<br><img src="https://img-blog.csdnimg.cn/0deee5693b514fb2b698fc87852c8d08.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li>
<li>压缩后RGB表征（大小：250KB）<br><img src="https://img-blog.csdnimg.cn/caa3e7a00ac4467387ae82c68083cec0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li>
<li>重建后光场（大小：10.9MB）<br><img src="https://img-blog.csdnimg.cn/b754cc8d9acf47a0a95cd4a907484c75.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAYm51Y3N5,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li>
</ul>
<p>&emsp;&emsp;效果非常之好，表征视图很清晰，并且重建视图很标准地还原了原光场。接下来的任务就在于能够实现JPEG压缩，这个实际很简单，就是在中间加一个JPEG压缩层，然后重建，为了能够重建出较好的结果，网络自然会把光场隐式编码到不会被JPEG压缩破坏的地方。<br>&emsp;&emsp;按照指导老师的安排，现在着手写最终的论文和实验时间尚早，因此接下来我将认真研读 $LF_DFnet$ 论文和代码，期望能够在一周之内完成论文的精读。</p>
<p>&emsp;&emsp;刚刚毕设开题报告审查反馈，反馈意见居然是题目过于宽泛。。。。。。<br>&emsp;&emsp;然而唯独题目是老师说我原本的题目（本文标题）太啰嗦又太简单才更改成现在的标题的。不过也完全可以理解，毕竟本科毕设、中文普刊、ICIP、EBCV、CVPR、SIGGRAPH各有各的标准，按照老师的意见、只答不辩毕了业了就行了。</p>
<h3 id="2022-1-20-2022-3-28"><a href="#2022-1-20-2022-3-28" class="headerlink" title="2022/1/20-2022/3/28"></a><code>2022/1/20-2022/3/28</code></h3><p>&emsp;&emsp;实际上现在已经5月初了，之所以现在才补全，主要是因为在3月份完成终稿及答辩PPT讲稿之后就再也没有继续写的欲望了，直到昨天想要尝试搭建一个自己的blog，今天完成之后才有心思继续写这一篇，总结全文，这篇记录就结束了。</p>
<p>&emsp;&emsp;自从1-20日到2-20日左右，应该是寒假时间的快乐时光，虽然在家里被隔离了半个月，但和对象一起住的日子毕竟还是更快乐一些。简单来说就是，啥也妹干。至于开题反馈，最终我在题目中加了两个字，遂通过。</p>
<p>&emsp;&emsp;从二月底返校之后，我开始快速完成JPEG编码层的搭建，虽然过程中遇到了一些问题：包括pytorch更新的问题、超参数的调优问题、训练的不拟合问题等等。最终我通过一些网络教程手动更新了pytorch，完成了代码的适配。在运行过程中调优超参数我采用同时搜索多个超参数的方式解决，最终结果又上了一个dB。</p>
<p>&emsp;&emsp;这之中实际遇到的问题很多，包括最终的实验也遇到了很多问题，除此之外我又参考了很多篇文献，最终形成了终稿，<a href="https://bnucsy.gitee.io/2022/05/02/represent-light-field-with-a-single-image/">这里</a>是对终稿的简单概括。本文最终没有采用可变形卷积方法，得到了一个还可以的结果，创新主要集中于思想和任务，预计在2023年初投稿ICPR。</p>
<p>主要参考文献：</p>
<ol>
<li><a href="https://arxiv.org/abs/2103.11314">A Learned Compact and Editable Light Field Representation</a> （一种紧凑的可编辑光场表示）</li>
<li><a href="https://arxiv.org/pdf/1708.03292.pdf">Learning to Synthesize a 4D RGBD Light Field from a Single Image</a>（使用单个RGB图像重建光场）</li>
<li><a href="https://arxiv.org/abs/2007.03535">Light Field Image Super-Resolution Using<br>Deformable Convolution</a>（光场超分辨率重建）</li>
<li><a href="https://arxiv.org/pdf/1505.04597.pdf">U-Net: Convolutional Networks for Biomedical<br>Image Segmentation</a>（经典论文：Unet网络）</li>
</ol>
]]></content>
      <categories>
        <category>paper</category>
        <category>mypaper</category>
      </categories>
  </entry>
  <entry>
    <title>MAE_源码级精读</title>
    <url>/MAE/</url>
    <content><![CDATA[<h2 id="Masked-Autoencoders-Are-Scalable-Vision-Learners-【图像分类、语义分割、目标检测】-CVPR2022"><a href="#Masked-Autoencoders-Are-Scalable-Vision-Learners-【图像分类、语义分割、目标检测】-CVPR2022" class="headerlink" title="Masked Autoencoders Are Scalable Vision Learners 【图像分类、语义分割、目标检测】 CVPR2022"></a><a href="url">Masked Autoencoders Are Scalable Vision Learners</a> 【图像分类、语义分割、目标检测】 CVPR2022</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;在 transformer 成功迁移到 CV 之后（VIT，swinTransformer），bert 等自监督训练的模型的迁移也开始被关注，虽然在 VIT 论文的末尾作者已经讨论了相关的可能性，但是 VIT 当时并不看好这种自监督的训练。这篇 MAE 大胆地进行了自监督训练图像的尝试，MAE 以一种非常简单的策略：遮挡某些 patch 后输入 VIT 得到隐空间表征，再通过另一个 VIT 解码全部图像，从而使 AE 学会表征语义信息。这是非常常见的 AE 训练策略，但是 MAE 做了 75% 以上的遮挡，构建了非常困难的任务场景，却得到了远超预期的结果。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/MAE/image-20220927100006046.png" alt="image-20220927100006046"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>借鉴 bert，采用自监督的训练方式训练 transformer</li>
<li>对于 encoder，被遮挡的 patch 并不输入，这保证了在复杂度一定的情况下 encoder 可以更深</li>
<li>对于隐空间的向量，没有 mask 的向量将由同一个 0 向量表示，该向量可以训练</li>
<li>对于 encoder 和 decoder，两次加入位置编码（这里虽然和其他 transformer 不一样，但是没做消融实验）</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/MAE/image-20220927095410783.png" alt="image-20220927095410783" style="zoom:80%;"></p>
<p>&emsp;&emsp;这篇论文的具体网络架构实在没啥好说的，就连代码也就 200 多行，不过由于这种简单的架构，我们在记录时基本可以把训练时的所有 trick 都试着看一看。这并非是不重要的，由于 VIT 在结尾也尝试了这种自监督的训练却没有好的结果，这种 bert 的迁移模型也有 BEiT 等工作，但偏偏 MAE 以非常简单的策略比其他的效果都要好，我认为一些超参数的设定，包括位置编码、某些变量的随机初始化分布，encoder 和 decoder 的层深，loss 的计算等细节部分可能对文章贡献更大。<a href="https://github.dev/facebookresearch/mae/blob/main/models_mae.py">源代码见此</a>。</p>
<hr>
<h4 id="初始化网络结构及参数初始化"><a href="#初始化网络结构及参数初始化" class="headerlink" title="初始化网络结构及参数初始化"></a>初始化网络结构及参数初始化</h4><p>&emsp;&emsp;这一部分关注与 encoder/decoder 及 encoder 之前的预处理部分，decoder 之前的预处理部分，包括如何打 patch，如何进行 random mask，以及 encoder/decoder 的一些参数。这里不多讨论具体 AE 做了什么，主要还是探讨一些初始化层。</p>
<p>&emsp;&emsp;首先是打 patch，这一部分和 VIT 完全没有任何区别，就是 VIT-base 的翻版，甚至直接 import 了 VIT 的实现，将 224x224 的图像打成多个 16x16 的 patch。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)</span><br><span class="line">num_patches = self.patch_embed.num_patches</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;然后 MAE 做了一个 mask 操作，具体的操作之后再说，总之在 mask 之后，原本 196x1024 的输入现在变成了 49x1024。接下来 MAE 完全按照 bert 来，也加了一个 CLS，然后加入 position embedding。具体来说，embedding 之后的维度为 1024，同时 pos 并非可学习的，而是直接采用 consin 编码方式，这一点和原始 transformer 一致。CLS 并非全零 tensor，而是服从 (0,0.2) 的正态分布。对于 decoder，也是同样的位置编码方式和反 embedding。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#———————————————————————————encoder————————————————————————————</span></span><br><span class="line">self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim))</span><br><span class="line">self.pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, embed_dim), requires_grad=<span class="literal">False</span>)  <span class="comment"># fixed sin-cos embedding</span></span><br><span class="line">pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-<span class="number">1</span>], <span class="built_in">int</span>(self.patch_embed.num_patches**<span class="number">.5</span>), cls_token=<span class="literal">True</span>)</span><br><span class="line">self.pos_embed.data.copy_(torch.from_numpy(pos_embed).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>))</span><br><span class="line"><span class="comment">#———————————————————————————decoder————————————————————————————</span></span><br><span class="line">decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-<span class="number">1</span>], <span class="built_in">int</span>(self.patch_embed.num_patches**<span class="number">.5</span>), cls_token=<span class="literal">True</span>)</span><br><span class="line">self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在这之后， MAE 选择了和 VIT 一致的 encoder，深度为 24，自注意力头数为 16，MLP 隐层维度为 4x1024，接受 token 维度为 1024。这样就可以输入 encoder 然后得到 50x1024 的输出了。对于和 VIT encoder 结构一致的 decoder，深度为 8，自注意力头数为 16，MLP 隐层维度为 4x512，接受输入的 token 维度为 512。【论文里多次强调，这是一个非对称结构的自编码器，然而实际上这里的非对称并非体现在框架算法上，更多的是 transformer 的层数和维度的大小】</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># MAE encoder specifics</span></span><br><span class="line">self.blocks = nn.ModuleList([</span><br><span class="line">        Block(embed_dim, num_heads, mlp_ratio, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, norm_layer=norm_layer)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line"><span class="comment"># MAE decoder specifics</span></span><br><span class="line">self.mask_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, decoder_embed_dim))</span><br><span class="line">self.decoder_blocks = nn.ModuleList([</span><br><span class="line">        Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, norm_layer=norm_layer)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(decoder_depth)])</span><br></pre></td></tr></table></figure>
<h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>&emsp;&emsp;在上面已经讨论过的一些模块的基础上，我们可以快速梳理 encoder 的前向过程，具体的过程直接在代码中注释说明</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_encoder</span>(<span class="params">self, x, mask_ratio</span>):</span><br><span class="line">    <span class="comment"># embed patches</span></span><br><span class="line">    x = self.patch_embed(x)	<span class="comment">#将图像切分成多个小 patch【196x256】</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># add pos embed w/o cls token</span></span><br><span class="line">    x = x + self.pos_embed[:, <span class="number">1</span>:, :]	<span class="comment">#在得到的 patch 中加入 cos-sin 位置编码【196x256】</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># masking: length -&gt; length * mask_ratio</span></span><br><span class="line">    x, mask, ids_restore = self.random_masking(x, mask_ratio)	<span class="comment">#进行 mask，实际上是进行 shuffle 之后取前 75%，返回 mask 和下标【49x256】</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># append cls token</span></span><br><span class="line">    cls_token = self.cls_token + self.pos_embed[:, :<span class="number">1</span>, :]	<span class="comment">#加入 CLS，并和 x 连接起来【50x256】</span></span><br><span class="line">    cls_tokens = cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">    x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply Transformer blocks</span></span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:	<span class="comment">#进入 encoder，经过 embedding 和 transformer block 等过程【50x1024】</span></span><br><span class="line">        x = blk(x)</span><br><span class="line">    x = self.norm(x)	<span class="comment">#标准LayerNorm</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在上述代码中我们仅有一处一笔带过，即 mask，接下来我们详细分析 mask 函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">random_masking</span>(<span class="params">self, x, mask_ratio</span>):</span><br><span class="line">    <span class="comment"># mask_ratio 即为遮挡的比例，默认值 0.75，即剩余 1/4</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Perform per-sample random masking by per-sample shuffling.</span></span><br><span class="line"><span class="string">    Per-sample shuffling is done by argsort random noise.</span></span><br><span class="line"><span class="string">    x: [N, L, D], sequence</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N, L, D = x.shape  <span class="comment"># batch, length, dim</span></span><br><span class="line">    len_keep = <span class="built_in">int</span>(L * (<span class="number">1</span> - mask_ratio))	<span class="comment">#mask 之后的长度，对于原本的输入 224，得到的是 224 * (1 - 0.75) = 49</span></span><br><span class="line">    </span><br><span class="line">    noise = torch.rand(N, L, device=x.device)  <span class="comment"># noise in [0, 1] 生成一定的噪音</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># sort noise for each sample</span></span><br><span class="line">    ids_shuffle = torch.argsort(noise, dim=<span class="number">1</span>)  <span class="comment"># ascend: small is keep, large is remove 排序噪声，这里的 noise 只作为排序的 shuffle 依据，没有实际含义</span></span><br><span class="line">    ids_restore = torch.argsort(ids_shuffle, dim=<span class="number">1</span>)	<span class="comment">#存储最终留下的 idx</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep the first subset</span></span><br><span class="line">    ids_keep = ids_shuffle[:, :len_keep]</span><br><span class="line">    x_masked = torch.gather(x, dim=<span class="number">1</span>, index=ids_keep.unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, D))	<span class="comment">#得到 mask 之后的 x，即【49x256】</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate the binary mask: 0 is keep, 1 is remove</span></span><br><span class="line">    mask = torch.ones([N, L], device=x.device)	<span class="comment">#所谓的 mask 矩阵，即在保留的位置赋 0，其余赋 1，在 decoder 中是不需要的，返回这个值是为了后续 loss 的计算</span></span><br><span class="line">    mask[:, :len_keep] = <span class="number">0</span></span><br><span class="line">    <span class="comment"># unshuffle to get the binary mask</span></span><br><span class="line">    mask = torch.gather(mask, dim=<span class="number">1</span>, index=ids_restore)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_masked, mask, ids_restore</span><br></pre></td></tr></table></figure>
<h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><p>&emsp;&emsp;这里的大部分操作是非常简单的，简单讲一下 decoder 的过程：首先复原和原本一样多的 patch ，复原方法是将其余被遮住的 patch 用同一个向量表示，这个向量进入计算图。然后加入位置编码，进入 transformer，出来之后过一个 Norm 和线性投射层，移除 CLS 后返回。</p>
<p>&emsp;&emsp;需要注意的是，在 李沐 的视频中，他说 decoder 是可以看到未经过 encoder 也未经过 mask 的原 patch，但实际上通过读源码我们发现并不能看到原图。这是因为在 李沐 出视频的时候 KaiMing 还没有放出来源码。同样的，李沐 在视频中抛出疑问：decoder 中是否所有部分都要加位置编码？有这个疑问是因为在没有被 mask 的 patch 中，其实本来就是有位置编码的，这样相当于加了两次。但实际上就是加了两次，又由于没有消融实验，我们无法判断两次加入位置编码是否对这项工作起了关键作用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_decoder</span>(<span class="params">self, x, ids_restore</span>):</span><br><span class="line">    <span class="comment"># embed tokens</span></span><br><span class="line">    x = self.decoder_embed(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># append mask tokens to sequence</span></span><br><span class="line">    mask_tokens = self.mask_token.repeat(x.shape[<span class="number">0</span>], ids_restore.shape[<span class="number">1</span>] + <span class="number">1</span> - x.shape[<span class="number">1</span>], <span class="number">1</span>)</span><br><span class="line">    x_ = torch.cat([x[:, <span class="number">1</span>:, :], mask_tokens], dim=<span class="number">1</span>)  <span class="comment"># no cls token</span></span><br><span class="line">    x_ = torch.gather(x_, dim=<span class="number">1</span>, index=ids_restore.unsqueeze(-<span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, x.shape[<span class="number">2</span>]))  <span class="comment"># unshuffle</span></span><br><span class="line">    x = torch.cat([x[:, :<span class="number">1</span>, :], x_], dim=<span class="number">1</span>)  <span class="comment"># append cls token</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># add pos embed</span></span><br><span class="line">    x = x + self.decoder_pos_embed</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply Transformer blocks</span></span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> self.decoder_blocks:</span><br><span class="line">        x = blk(x)</span><br><span class="line">    x = self.decoder_norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predictor projection</span></span><br><span class="line">    x = self.decoder_pred(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># remove cls token</span></span><br><span class="line">    x = x[:, <span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h4><p>&emsp;&emsp;网络的 loss 比较普通，选用常见任务对应的损失，这里因为是自监督的训练，选了个 MSE，也是自监督非常常见的损失选择，不过是最后加了一个针对 mask 的处理，并不复杂。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_loss</span>(<span class="params">self, imgs, pred, mask</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    imgs: [N, 3, H, W]</span></span><br><span class="line"><span class="string">    pred: [N, L, p*p*3]</span></span><br><span class="line"><span class="string">    mask: [N, L], 0 is keep, 1 is remove, </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    target = self.patchify(imgs)</span><br><span class="line">    <span class="keyword">if</span> self.norm_pix_loss:</span><br><span class="line">        mean = target.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        var = target.var(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        target = (target - mean) / (var + <span class="number">1.e-6</span>)**<span class="number">.5</span></span><br><span class="line"></span><br><span class="line">    loss = (pred - target) ** <span class="number">2</span></span><br><span class="line">    loss = loss.mean(dim=-<span class="number">1</span>)  <span class="comment"># [N, L], mean loss per patch</span></span><br><span class="line"></span><br><span class="line">    loss = (loss * mask).<span class="built_in">sum</span>() / mask.<span class="built_in">sum</span>()  <span class="comment"># mean loss on removed patches</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;结果是这项工作最重要的部分了，因为纵观这篇 paper，10 页之中只有 1 页讲了具体做了什么，其他全部在说我们为什么这么做，我们做出来有多么好，尤其是我们做出来多么好的结果这方面，真的占据了很大的篇幅，并且这结果确实让人直呼震惊，叹为观止。</p>
<p><img src="/MAE/image-20220927235213707.png" alt="image-20220927235213707"></p>
<p><img src="/MAE/image-20220927235234672.png" alt="image-20220927235234672"></p>
<hr>
<h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3><p>&emsp;&emsp;自从 transformer 出现之后，基于此产生的工作层出不穷，首先在 NLP 领域，transformer 被加入了自监督的训练方式，造就了 NLP 领域通用的预训练 baseline，如 bert，GPT 等。自从 2021 年之后，transformer 在 CV 领域也开始逐渐大放异彩，在诸多迁移的文章中，VIT 是首个将原生 transformer 用于 CV 领域的成功研究，这项研究让人们认识到，就算不针对图像做特殊处理，transformer 也可以和 CV 领域的传统老大哥 CNN 掰手腕，VIT 的同年，MSRA 提出了 swinTransformer，在多领域完成了对 VIT 的扩展，并几乎取得了所有的 SOTA。在这之外，针对 bert 等自监督训练模型向 CV 领域的迁移也开始进行，其中最成功的，甚至成功地有点过分的，就是这篇 MAE，即带掩码的自编码器。神奇的是，这个结果好得出奇甚至有点玄学的工作，在国外没有引起太大的反响，或许是因为网络设计真的太简单，但是单从结果上看，这项工作给人带来的震惊不逊于 KaiMing 的另一个工作 ResNet，嗷现在也有 ResNext 了。这俩大道至简了嗷属于是。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>MAGVIT</title>
    <url>/MAGVIT/</url>
    <content><![CDATA[<h2><center> MAGVIT: Masked Generative Video Transformer </center></h2>

<p>【视频生成】【arxiv】【<a href="http://arxiv.org/abs/2212.05199">paper</a>】【<a href="https://github.com/MAGVIT/magvit">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种基于 3DCNN 的 VQGAN 和多任务优化的视频生成器。其中 3DCNN 的网络由图像预训练的 2D 网络中心膨胀而来。所谓多任务优化，即通过不同的 mask 模拟不同的<strong>条件任务</strong>（包括帧插值、帧预测等十种），另外加上<strong>自重建任务</strong>进行联合优化。其中<strong>条件任务</strong>又分为：1.预测完全不存在的帧，2.预测部分存在的帧。因此第二部分可以描述为使用多任务构造损失函数联合优化 VQGAN 的下标重建 transformer。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/MAGVIT/image-20230417203930207.png" alt="image-20230417203930207"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>先训练 2D-VQGAN 再膨胀为 3D-VQGAN 的策略</li>
<li>对视频进行多任务 mask 联合优化的策略</li>
</ul>
<h4 id="tricks"><a href="#tricks" class="headerlink" title="tricks"></a>tricks</h4><ul>
<li>使用中心膨胀获得 3D 预训练权重</li>
<li>对视频的每一帧计算感知损失</li>
<li>在 GAN 的损失函数上加入 LeCam 正则化</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/MAGVIT/image-20230417203944064.png" alt="image-20230417203944064"></p>
<p>&emsp;&emsp;上图中，3D-VQ Encoder 将视频量化为离散的标记，而 3D-VQ  Decoder 将它们映射回像素空间。在每个训练步骤中对任务 $\rho$ 进行采样（共十种任务），并通过裁剪和填充原始视频来构建其条件输入 $c$，其中绿色表示 GT 像素，白色表示填充，橙色表示 GT tokens。使用用 3D-VQ Encoder 量化条件输入，并选择非填充部分（橙色）作为条件标记。掩码 token 序列（即图中 COMMIT Masking）结合了条件 tokens、[MASK] tokens 和 GT tokens，并以任务提示符和类令牌作为前缀（$[\rho,c]$，直接 Concat）。transformer 通过三个目标学习预测可视标记:细化条件标记、预测屏蔽标记和重构输入标记。</p>
<p>&emsp;&emsp;本文提出的 MAGVIT 是一个两阶段的模型，首先训练一个 3DCNN 的 VQGAN，第二阶段再通过多任务 mask 学习一个 transformer 预测 token。</p>
<p>&emsp;&emsp;第一阶段将编码器和解码器分别描述为 $f_\tau:V\to z,\ f_{\tau^{-1}}:z\to V$。第一阶段训练的 VQGAN 由 2DCNN 的预训练模型膨胀得到，在图像上训练的 2D-VQVAE 遵循如下优化模式： </p>
<ul>
<li>$I$ 表示图像，$z$ 表示图像经过编码器 VQ 之后的下标序列，长度为 N</li>
<li>$m$ 为 mask，是一个二元向量，对于 $m$ 的每一个值为 $x\to x/[mask]$，经过 mask 的 $m(z)$ 记作 $\bar z$</li>
<li>训练时，$m$ 从先验分布 $p_\mu$ 之中采样，即其中 $x\to[mask]$ 占比遵循余弦调度函数 $\gamma(\cdot)$<ul>
<li>采样的具体操作为：</li>
<li>从 $U(0,1)$ 中均值采样 mask 分数 $s$ 和一个中间变量 $r$，使用 $r$ 计算 $\lceil \gamma(r)N\rceil$，这个值表明小于阈值 $s^<em>$ 的 $s_i\in s$ 的个数，以此确定阈值 $s^</em>$</li>
<li>得到 $m_i(x)=[mask]\ \ \ \ if\ \ \ \ s_i\leq s^*\ \ \ \ else\ \ \ \ x$</li>
</ul>
</li>
<li>优化目标为：$L_{mask}(z; θ) = E_{m∼p_\mu} [∑_{\bar z_i =[MASK]} − \log p_θ(z_i | [c, \bar z]) ]$</li>
</ul>
<p>&emsp;&emsp;3D-VQVAE 采用中心膨胀的方式获得预训练的权重，<strong>这样可以在 GAN 的训练中快速达到稳定状态</strong>。同时在视频训练时<strong>将 VGGLoss 应用于每一帧的图像</strong>上，在 GANLoss 中<strong>加入了 <a href="https://arxiv.org/pdf/2104.03310.pdf">LeCam 正则化（CVPR2021）</a></strong> 用以稳定对抗训练。</p>
<p>&emsp;&emsp;第二阶段的训练采用多任务 mask 的方法进行建模，原文称之为：内部令牌的条件屏蔽建模（<strong>CO</strong>nditional <strong>M</strong>asked <strong>M</strong>odeling by <strong>I</strong>nterior <strong>T</strong>okens ）简记为 COMMIT 方法。在训练时从十种不同的条件生成任务中进行采样，具体来说任务包括：帧预测(FP)、帧插值(FI)、中央外绘(OPC)、垂直外绘(OPV)、水平外绘(OPH)、动态外绘(OPD)、中央补绘(IPC)和动态补绘(IPD)、类条件生成(CG)、类条件帧预测(CFP)。其中每个任务的具体定义见原文 P12 附录 B.1。</p>
<p>&emsp;&emsp;在第二阶段的训练过程中，其整体逻辑与第一阶段训练图像 2D-VQ 一致，但在细节上略有改动：</p>
<ul>
<li>首先采样原视频 $V$，采样一个任务提示符 $\rho$，并根据 $\rho$ 生成对应的 mask 任务视频 $\tilde V$</li>
<li>记 $z=f_\tau(V),\ \tilde z = f_\tau (\tilde V)$，得到对应 tokens 的 mask 结果 $m_i$：<img src="/MAGVIT/image-20230418164802591.png" alt="image-20230418164802591" style="zoom:40%;"><ul>
<li>其中 ispad(·) 表示是否为全填充，对应至上图 backbone 之中，表示为全白色的 token </li>
</ul>
</li>
<li>一阶段图像的loss描述为：$L_{mask}(z; θ) = E_{m∼p_\mu} [∑_{\bar z_i =[MASK]} − \log p_θ(z_i | [c, \bar z]) ]$，因此据此写出视频的描述应为：$L(V; θ) = E_{\rho,\tilde V}[ E_{m∼p_M} [∑_i − \log p_θ(z_i | [\rho,c, \bar z]) ]]$<ul>
<li>其中用 $p_M$ 而非 $p_\mu$ 是因为在当前的 $m_i$ 获取方式下，其分布已经和图像层面不一致了，故用一个新的符号</li>
<li>此式按照 $m_i$ 的三种不同取值可以拆分为三个部分：<img src="/MAGVIT/image-20230418165556463.png" alt="image-20230418165556463" style="zoom:40%;"></li>
<li>这三部分分别表示了：1.条件生成（10种具体任务的某些帧）2.完全预测（10种具体任务的某些帧）3.视频重建（单独的任务+10种具体任务的某些帧）</li>
</ul>
</li>
</ul>
<h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><p><img src="/MAGVIT/image-20230418165945029.png" alt="image-20230418165945029" style="zoom:50%;"></p>
<p>&emsp;&emsp;推理的过程中采用上图的推理方式，具体来说是：COMMIT 解码从 $m_i$ 入手，在 $m_i$ 和对应的 $s_i$ 的指导下，通过在每一步替换一部分新生成的 token 来执行向输出 token 的条件转换过程（具体的替换个数由更新 $s^*$ 的值确定，而这个值由 $t$ 确定，$t$ 表示第几步，总共 $K$ 步，$K$ 手动选择），并最终预测所有 token。</p>
<p>&emsp;&emsp;下图展示另一个边界预测任务下的 7 步推理可视化过程：</p>
<p><img src="/MAGVIT/image-20230418170349596.png" alt="image-20230418170349596" style="zoom: 50%;"></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><ul>
<li>在视频生成上针对 UCF-101 的 FVD</li>
</ul>
<p><img src="/MAGVIT/image-20230418170646469.png" alt="image-20230418170646469" style="zoom:50%;"></p>
<ul>
<li>在帧预测任务上针对 K600 和 BAIR 的 FVD</li>
</ul>
<p><img src="/MAGVIT/image-20230418170727129.png" alt="image-20230418170727129" style="zoom:50%;"></p>
<p>&emsp;&emsp;更多的结果可视化见网站：<a href="https://magvit.cs.cmu.edu">https://magvit.cs.cmu.edu</a></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>Meta-rPPG</title>
    <url>/Meta-rppg/</url>
    <content><![CDATA[<h2><center> Meta-rPPG: Remote Heart Rate Estimation Using a Transductive Meta-Learner </center></h2>

<p>【心率监测】【ECCV2020】【<a href="https://arxiv.org/pdf/2007.06786.pdf">paper</a>】【<a href="https://github.com/eugenelet/Meta-rPPG">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文采用了元学习的方式探索了在跨数据集上的适应性问题，通过将每个 clip 的预测看做单个任务的方式构造多任务，并将每个 clip 区分为互不相交的 Q, S 进行 few-shot 学习。得到的网络结构理论上可以在预训练的基础上支持 zero-shot 学习，通过梯度合成生成器快速学习无标签数据。缝合元学习的基础上在 MAHNOB-HCI 取得了 SOTA，在 UBFC 上效果普通。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Meta-rppg/image-20221115121637500.png" alt="image-20221115121637500" style="zoom:90%;"></p>
<span id="more"></span>
<hr>
<h3 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h3><h4 id="元学习概念"><a href="#元学习概念" class="headerlink" title="元学习概念"></a>元学习概念</h4><ul>
<li><p>元学习、自监督学习、对比学习、FSL</p>
<ul>
<li>自监督学习的常用方法包括对比学习</li>
<li>few-shot learning（小样本学习）是自监督学习的下游任务之一<ul>
<li>FSL 目的是通过不足以训练网络的样本量进行学习和预测</li>
</ul>
</li>
<li>FSL 通常使用迁移学习完成<ul>
<li>迁移学习着眼于：1.利用先验知识快速学习新的数据集，2.在学习新数据集的基础上不忘记旧数据集</li>
</ul>
</li>
<li>元学习是迁移学习的一种</li>
</ul>
</li>
<li><p>元学习的含义</p>
<ul>
<li><p>元学习原本的含义为 “learn to learn”，即通过学习超参数从而生产网络（有点像 NAS）</p>
<ul>
<li><p>但是在本文中并没有学习超参数</p>
</li>
<li><p>事实上在当前一些带有 “Meta” 的论文其中的本质是对比学习</p>
</li>
</ul>
</li>
<li><p>元学习着眼于：在同分布的任务上通过先验网络快速学习新数据集</p>
<ul>
<li>获取灵活的先验网络的方式：在多个任务上进行训练</li>
</ul>
</li>
<li><p>元学习最常用的任务为分类任务，其最初架构的提出与改进都在分类任务上进行测试</p>
<ul>
<li>但是在本文中的任务为回归任务（创新点之一，但创新得很牵强）</li>
</ul>
</li>
<li>由于常在 few-shot 任务上学习，元学习的网络结构通常不深，为了避免过拟合</li>
</ul>
</li>
<li><p>元学习中的定义</p>
<ul>
<li>$\mathcal {Q, S}$：Query，Support，即为了区别于元学习架构中真正的测试集、训练集，重新命名在迁移的单任务中的测试集（Query Set）、训练集（Support Set）。  </li>
<li>N-ways，K-shot：N-在 $\mathcal S$ 中的类别数目，K-在 $\mathcal S$ 中每个类别的样本数目</li>
</ul>
</li>
</ul>
<h4 id="一个元学习的实例"><a href="#一个元学习的实例" class="headerlink" title="一个元学习的实例"></a>一个元学习的实例</h4><p>&emsp;&emsp;对于图像分类任务，有网络 $f_\theta$，输入图像 $x$，训练数据集 $\mathcal T$。其中训练集如下图所示：</p>
<p><img src="/Meta-rppg/image-20221115130706698.png" alt="image-20221115130706698" style="zoom:50%;"></p>
<p>&emsp;&emsp;由于元学习需要在多任务上进行训练，一个简单的想法就是把 $\cal T$ 分成多个任务，多次训练之后可以得到初始化的 $f_\theta$，输入图像 $x$ 之后可以输出在多个类别上的概率分布。</p>
<p>&emsp;&emsp;此时给出在某个小任务（训练集分出来的或者新遇到的），对于此任务，其训练集（Support Set）和测试集（Query Set）都很小，一个可能的例子如下图所示，在此任务中，由于总共有 6 个类别，每个类别只有一个样本，因此称之为 6-ways，1-shot 任务。</p>
<p><img src="/Meta-rppg/image-20221115131338143.png" alt="image-20221115131338143" style="zoom:50%;"></p>
<p>&emsp;&emsp;以训练好的 $f_\theta$ 作为初始化网络开始学习新的小数据，可以在很快的时间内学会此分类任务。</p>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>一种可以用于心率监测的元学习框架</li>
<li>使用合成梯度生成器和分布最小化损失快速学习无标签数据</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/Meta-rppg/image-20221115121637500.png" alt="image-20221115121637500" style="zoom:70%;"></p>
<h4 id="网络组成"><a href="#网络组成" class="headerlink" title="网络组成"></a>网络组成</h4><p>&emsp;&emsp;整个网络分为三个部分：特征提取器、rPPG 估计器、合成梯度生成器。前两部分首先训练，就是普通的 rPPG 估计器训练方式，分别由 Conv 和 LSTM 组成。合成梯度生成器由 Conv 组成，仅在直推学习期间使用。三个组成部分的具体网络组成如下图所示，其中 Conv2DBlocks 由Conv2D、Batchnorm、average pooling 和 ReLU 组成。Conv1D 模块由 Conv1 d、Batchnorm 和 ReLU 组成。</p>
<p><img src="/Meta-rppg/image-20221115152101503.png" alt="image-20221115152101503" style="zoom:80%;"></p>
<h4 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h4><p>&emsp;&emsp;对于输入 $\cal T$，首先的预处理是标定其 landmark，切割出人脸部分（这里使用的是 dlib，肯定耗时很长，又不能叫端到端啦 😅）结果记为 $x^{train}$，从训练集中取出 N 个独立的 clip，这些 clip 每一个的 ppg 信号估计都代表了一个任务。接着对每一个子任务，将 clip 分为 $V,\ W$ 帧的子 clip，分别记为 $\cal {S,\ Q}$。这样就完成了元学习的基本设定。</p>
<p>&emsp;&emsp;在预训练阶段，对 $x^{train}$ 采样 T 个连续帧 $x$，将其输入特征提取器，得到 $f_\theta(x)$，其中 $\theta$ 是 $f$ 的参数。这里的 $f_\theta(x)$ 是一个具有 T 长度的 ppg 信号，记为 $z$。对于 $z$，将其输入 rPPG 估计器得到 $y=h_\phi(z)$，将预测的 ppg 信号 $y$ 和 $t(y_{GT})$ 进行比较，计算 $\mathcal L_{ORD}(y,t)$后梯度回传，是一个普通的训练过程。</p>
<p>&emsp;&emsp;在直推式学习阶段，需要限制两个新的学习：1.梯度生成器的输出应当和  $\mathcal L_{ORD}$ 回传给 $z$ 的梯度一致，以便接下来的迁移学习，2.通过 $x\in\cal S$ 特征提取器生产的特征值 $z$ 应当和 $x^{train}$ 生产的 $z^{proto}$ 具备一致的分布，这是因为之前的研究已经证明：必须在同分布的数据中元学习的方式才更有效。</p>
<p>&emsp;&emsp;因此在直推式学习阶段生成 $z$ 之后，$z$ 有两个去向，除了输入进入 $h_\phi$，还会进入 $g_\psi$，对于 $g_\psi(z)$，其期望的输出应当和 $\nabla_z\mathcal L_{ORD}(y,t)$ 一致，为了限制这种一致性，计算 $\mathcal L_{SYN}=||g_\psi(z)-\nabla_z\mathcal L_{ORD}(y,t)||_2^2$，同时在直推式学习之中更新 $\theta$ 的梯度，即 $g$ 也负责将 $\mathcal L_{ORD}$ 的梯度传给 $f$。通过这种方式就限制了合成梯度生成器的学习，而在之后的无标签学习中，就可以通过 $g$ 生成的梯度来辅助更新 $f$。</p>
<p>&emsp;&emsp;同时为了保证 $\cal S$ 生成特征的分布一致性，记 $\hat z = \frac 1 T \sum\limits_{t=1}^Tz_t$，为了在统计 $z^{proto}$ 在整个数据集上的分布特性，通过迭代的方式随着预训练统计，其具体更新公式为：</p>
<script type="math/tex; mode=display">
z^{proto}=\gamma z^{proto}+(1-\gamma) \mathbb E_{x^{(i)}\sim p(\mathcal T)}\frac 1 T\sum\limits_{t=1}^Tf_\theta(x_t^{(i)})</script><p>&emsp;&emsp;因此计算分布一致性损失 $\mathcal L_{PROTO}=\mathbb E_{x^{(i)}\sim p(\mathcal T)}\frac 1 T\sum\limits_{t=1}^T||f_\theta(x_t^{(i)})-z^{proto}||_2^2$，通过最小化这个损失训练特征提取器，强制其生成的图像位于同一分布。</p>
<p>&emsp;&emsp;更具体的，文章给出了前向过程的伪代码如下图。对于训练，</p>
<p><img src="/Meta-rppg/image-20221115151949132.png" alt="image-20221115151949132" style="zoom:80%;"></p>
<h4 id="快速适应性"><a href="#快速适应性" class="headerlink" title="快速适应性"></a>快速适应性</h4><p>&emsp;&emsp;对于学习得到的网络结构，在面对新的 $\cal Q,\ S$ 时，只需要按照同样的方式对 $\cal S$ 进行采样，输入已经初始化的 $f_\theta$，得到特征 $z$，对于特征 $z$ 输入已经预训练好的梯度生成器 $g_\psi$ 得到需要回传的梯度之一 $g_\psi(z)$，之二为分布损失回传的梯度，即 $\nabla_\theta\mathcal L_{proto}(\hat z,\hat z^{proto})$，用他们之和作为新的梯度来更新 $\theta$，这样就可以快速地适应新的数据集。同时对于 rPPG 估计器，其梯度不会更新，值也不会变化。</p>
<p>&emsp;&emsp;对于上述过程，其有效的假设在于：对于 ppg 信号的估计，其特征提取部分和从特征生成信号的部分是解耦合的。也就是说对于已经训练好的模型，面对新数据集时只需要修改特征提取器就可以使最终的结果正确。</p>
<p>&emsp;&emsp;对于学习新的 $\cal Q,\ S$，在学习过程中的伪代码如下图所示。对于适应性学习，训练的轮数为 10。</p>
<p><img src="/Meta-rppg/image-20221115153627351.png" alt="image-20221115153627351" style="zoom:80%;"></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;本文在 MAHNOB-HCI 和 UBFC 上测试了结果，使用含有 18 个长度为 120 秒的视频进行了预训练（没有给出预训练选择的数据集），和消融实验一起做成了一张表格如下图，这是 MAHNOB-HCI 的结果，是非常好的，在 physformer 中也没有明显的落后，并且其训练集小、适应性高。</p>
<p><img src="/Meta-rppg/image-20221115154720069.png" alt="image-20221115154720069" style="zoom: 67%;"></p>
<p>&emsp;&emsp;而在 UBFC 上的结果并不好，给个 The Way To My Heart 的结果可见一斑</p>
<p><img src="/Meta-rppg/image-20221115155102267.png" alt="image-20221115155102267" style="zoom:80%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>MoCo_V1_V2_V3</title>
    <url>/MoCo/</url>
    <content><![CDATA[<h2 id="Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning-【目标检测、图像分割】-CVPR2020"><a href="#Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning-【目标检测、图像分割】-CVPR2020" class="headerlink" title="Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning 【目标检测、图像分割】 CVPR2020"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf">Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning</a> 【目标检测、图像分割】 CVPR2020</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;这篇文章实际上发布了有相当一段时间了，在两年之后的今天会看这项工作，很难不折服于 MoCo 的框架通用性、泛化性以及作者高度的前瞻性。随后作者又提出了 MoCoV2，MoCoV3，这两项工作在一定程度上就是单纯的对 MoCo 的适应性改编，尤其是 MoCoV2，几乎没有什么更新的想法，而 MoCoV3 至少发现了一些一直被人们所忽略的 VIT 的训练特性。作为相当一段时间内在对比学习领域和 SimCLR 齐名的前二名，MoCo 对对比学习问题进行了高度抽象，提出了简单又高效的改革。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/MoCo/image-20220928213908179.png" alt="image-20220928213908179" style="zoom:50%;"></p>
<span id="more"></span>
<hr>
<h3 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h3><p>&emsp;&emsp;在讲述 MoCo 发展史之前，首先需要了解何为对比学习，以及以往的对比学习都采取了哪样的策略，这些工作尽管很出名，但是为什么被 MoCo 的作者说成”不可避免地受限“。</p>
<h4 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h4><p>&emsp;&emsp;首先，对比学习并非某种网络结构，更多地，这是像 AE、VAE、GAN 类似的无监督学习的一种学习方式。在对比学习之中，我们需要做两个关键的事情，1.构造正负例样本的向量表征，2.选择合适的损失函数训练分类/预测问题。</p>
<p>&emsp;&emsp;我们将举一个非常简单的例子描述对比学习，这种方式实际上出现在 SimCLR 之中。</p>
<p><img src="/MoCo/image-20220928224432063.png" alt="image-20220928224432063" style="zoom:40%;"></p>
<p>&emsp;&emsp;简单来说，通过对同一张图像进行数据增强，得到的两个新图像互为正样本，同样的，由两个不同的图像进行数据增强之后的结果就是负样本，因此对于某个图像，对他来说是正样本的只有 1 个，负样本有很多个，准确来说应该是 (batchSize-1)x2 个，因此问题转化为一个多分类问题。将所有的正负样本输入一个编码器，得到向量表征，这里的向量表征就是我们在最后想要得到的具备强语义信息。对向量继续通过一个 MLP 进行分类，并对分类之后的结果求 NCELoss，梯度回传并更新编码器，我们其实只想要编码器，这里训练用的 MLP 在推理的时候并不使用，而是到时候再训练一个新的解码器。</p>
<h4 id="代理任务"><a href="#代理任务" class="headerlink" title="代理任务"></a>代理任务</h4><p>&emsp;&emsp;所谓代理任务，即构造正负样本的方法，其中最出名的是 个体判别 方法，该方法将每个图像数据增强后的结果看做一个类，任何两个不同的图像都不属于一个类 ，这种简单粗暴的方式反而具备很好的效果，一直被 SimCLR，MoCo 等对比学习经典作品采用。</p>
<p>&emsp;&emsp;当然，还有其他的代理任务，正是代理任务的灵活性给了对比学习很高的进步空间，比如将一张图像打成多个 patch，选择中心 patch 作为基准，对其他 8 个 patch 进行标号，随机选取一个 patch，然后预测其相对于基准的标号。再比如在视频流中，每个帧和下一帧互为正样本，其他的随机抽取均为负样本。再比如在所有时序数据中，选取前四帧分别经过编码器和 MLP 整合成一个语义表征，则下四帧经过同样的编码器后的表征就是正样本，任意其他四帧经过编码器之后的就是负样本，这也就是 CPC 的策略，简单泛化效果好。</p>
<h4 id="对比学习中的困难点"><a href="#对比学习中的困难点" class="headerlink" title="对比学习中的困难点"></a>对比学习中的困难点</h4><p>&emsp;&emsp;当然，对比学习很棒，自监督意味着不需要额外的标号。但是正如 MoCo 所说，大多数工作都主要致力于解决这两个问题：1.负样本的一致性问题，2.负样本的数量问题。简单来说，问题 2 意味着负样本太少会让编码器学习到一些捷径，从而难以学到有力的表征，问题 1 意味着不同的负样本表征可能是由不同参数的编码器得到的，这加重了模型的训练难度，可能使其学到额外的知识并难以收敛。</p>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>MoCo<ul>
<li>提出了一种抽象的 q-k 描述来总结以往的对比学习优化目标和局限性</li>
<li>提出使用 queue 的数据结构将 batchSize 和 dictSize 分开，保证了 key 数据多样性</li>
<li>提出了动量更新的策略，保证了多个 key 之间的数据一致性</li>
</ul>
</li>
<li>MoCoV2<ul>
<li>对 ResNet50 之后的 MLP 层进行了加深</li>
<li>采用了一种更新的数据增强策略</li>
</ul>
</li>
<li>MoCoV3<ul>
<li>将骨干网络换成了 VIT，微调了 loss，取得了相对于 MoCo，MoCoV2 稍好一点点的结果</li>
<li>发现了在 VIT 训练过程中 patch projection 会导致训练精度突变的问题，并给出了解决方案</li>
</ul>
</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p>&emsp;&emsp;MoCo 实际上只是提出了一种网络架构，并没有规定在这个架构中具体的网络是什么，在 MoCo 刚提出的时候还是 CNN 时代，因此最初选取的骨干网络是 ResNet50，之后 MoCoV2 对 ResNet50 进行了一点小改动，在 MoCoV3 的时候已经是 transformer 盛行时代了，因此 MoCoV3 把骨干网络换成了 VIT，但是在实际操作的过程中发现了效果并没有太好，因此作者认真探究了为何会变差，得到了很有意思的结论。</p>
<h4 id="MoCo"><a href="#MoCo" class="headerlink" title="MoCo"></a>MoCo</h4><p><img src="/MoCo/image-20220929180009533.png" alt="image-20220929180009533" style="zoom:50%;"></p>
<p>&emsp;&emsp;对于 MoCo，所有的问题被重新表述了，在选用个体判别的代理任务的情况下，现在我们认为同一个 batch 内的所有图像中，由某一个图像数据增强得到的两个图像其中一个为 $x^q$，其他的都是 $x^k$，在所有的 $x^k$ 之中，只有 $x^{k_0}$ 的正样本，其他的都是负样本。对于 $x^q$ 和 $x^k$ 做转置乘，得到他们的相似性，这就相当于构造了一个字典，所有的 $x^k$ 就是字典中的键，原本的两个困难就可以被重新描述为 1. 字典的键要尽可能多，2. 字典的键要尽可能一致。为了解决这两个问题，MoCo 提出了两个方案：1.使用队列存储字典，2.不训练 key 的编码器</p>
<h5 id="队列和动量更新"><a href="#队列和动量更新" class="headerlink" title="队列和动量更新"></a>队列和动量更新</h5><p>&emsp;&emsp;对于刚刚提出的两个改进策略，1.将每次 batch 得到的所有 $x^k$ 都存储在一个队列里面，当队列满了之后就每次剔除队列头的元素，通过这种方式就可以保证负样本的数量。2.不训练 key 的编码器，而是每次直接从 query 的编码器中 copy 参数过来，通过一个公式进行动量更新，在保证 key 编码器在逐渐变化的情况下不失去 key 的一致性。</p>
<p>&emsp;&emsp;在每个 batch 得到 query 和 key 之后，key 进入队列，此时如果队列满了，就将队头移出。对于很多的 key，这之中有的是正样本，有的是负样本，总共约有 6W 多个，将这些 key 和 query 计算 loss，具体的 loss 这里选择 $\rm InfoNCELoss$，其具体计算公式如下：</p>
<script type="math/tex; mode=display">
\zeta_q=-\log\frac{\exp(q·k_+/\tau)}{\sum_{i=0}^{K}\exp(q·k_i/\tau)}</script><p>&emsp;&emsp;这里其实和 NCELoss 几乎没区别，加了个 $\tau$，目的是使模型更加好训练，据说是因为要控制 $q·k_+$ 的分布，咱也不太懂原理是啥。</p>
<p>&emsp;&emsp;总之，现在我们计算出来了 loss，接下来就可以进行反向传播了，但是要注意的是，这里的反向传播只能传播到 query 的编码器之中，我们切断了向 key 的梯度。然后更新 query 的编码器，更新完 query 的编码器之后我们同样要更新 key 的编码器，我们记 query 的编码器参数为 $\theta_q$，key 前一时刻编码器参数为 $\theta_k^{i-1}$，则当前时刻的 key 编码器参数由如下公式更新：</p>
<script type="math/tex; mode=display">
\theta_k^i=m\times\theta_k^{i-1}+(1-m)\times\theta_q</script><p>&emsp;&emsp;这个式子也就是所谓的动量更新，这里的 $m$ 就是动量，$m$ 越大代表 key 编码器更新越慢，而事实上 $m$ 在 MoCo 中选择非常之大，为 0.999，也就是每次 query 的编码器参数对 key 编码器参数的影响只占 $0.1\%$。</p>
<p>&emsp;&emsp;据此策略，MoCo 就可以展开训练了，作者选取 ResNet50 作为骨干网络，取得了很好的结果，并且给出了训练时的 pytorch 风格的伪代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># f_q, f_k: encoder networks for query and key</span></span><br><span class="line"><span class="comment"># queue: dictionary as a queue of K keys (CxK)</span></span><br><span class="line"><span class="comment"># m: momentum</span></span><br><span class="line"><span class="comment"># t: temperature</span></span><br><span class="line">f_k.params = f_q.params <span class="comment"># initialize</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> loader: <span class="comment"># load a minibatch x with N samples</span></span><br><span class="line">    x_q = aug(x) <span class="comment"># a randomly augmented version</span></span><br><span class="line">    x_k = aug(x) <span class="comment"># another randomly augmented version</span></span><br><span class="line">    q = f_q.forward(x_q) <span class="comment"># queries: NxC</span></span><br><span class="line">    k = f_k.forward(x_k) <span class="comment"># keys: NxC</span></span><br><span class="line">    k = k.detach() <span class="comment"># no gradient to keys</span></span><br><span class="line">    <span class="comment"># positive logits: Nx1</span></span><br><span class="line">    l_pos = bmm(q.view(N,<span class="number">1</span>,C), k.view(N,C,<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># negative logits: NxK</span></span><br><span class="line">    l_neg = mm(q.view(N,C), queue.view(C,K))</span><br><span class="line">    <span class="comment"># logits: Nx(1+K)</span></span><br><span class="line">    logits = cat([l_pos, l_neg], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># contrastive loss, Eqn.(1)</span></span><br><span class="line">    labels = zeros(N) <span class="comment"># positives are the 0-th</span></span><br><span class="line">    loss = CrossEntropyLoss(logits/t, labels)</span><br><span class="line">    <span class="comment"># SGD update: query network</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    update(f_q.params)</span><br><span class="line">    <span class="comment"># momentum update: key network</span></span><br><span class="line">    f_k.params = m*f_k.params+(<span class="number">1</span>-m)*f_q.params</span><br><span class="line">    <span class="comment"># update dictionary</span></span><br><span class="line">    enqueue(queue, k) <span class="comment"># enqueue the current minibatch</span></span><br><span class="line">    dequeue(queue) <span class="comment"># dequeue the earliest minibatch</span></span><br></pre></td></tr></table></figure>
<h5 id="MoCo和其他对比学习框架"><a href="#MoCo和其他对比学习框架" class="headerlink" title="MoCo和其他对比学习框架"></a>MoCo和其他对比学习框架</h5><p>&emsp;&emsp;MoCo 在之前提到，以往的工作往往受限于字典大小和表征一致性，那么具体是如何受限的呢，MoCo 给出了对比，在 MoCo 的动量更新、队列存储之前，对比学习往往才有两种训练方式，一种是端到端的训练，另一种是使用 memory bank 存储较多 key 的训练。</p>
<p><img src="/MoCo/image-20220929185622211.png" alt="image-20220929185622211"></p>
<p>&emsp;&emsp;对于端到端的训练，我们将 query 和 key 分别输入同样的 encoder，并且得到的 loss 直接反向传播到两个编码器，这两个编码器既可以共享参数也可以不共享参数，这样做的好处是每次我们得到的 $k$ 都是通过同一个编码器得到的，缺点就是每次一个 batch 得到的 $k$ 的个数太少了。</p>
<p>&emsp;&emsp;对于基于 memory bank 的方法，为了解决单个 batch 得到的 $k$ 的个数太少的问题，将预先将很多负样本通过一个 encoder 存储起来，这个就叫做 memory bank，每次计算时，从 memory bank 里面随机抽取几个负样本进行计算，并梯度回传更新 query 的 encoder，此时将我们抽取出来的负样本重新走一遍更新之后的 encoder，并更新这些样本然后再放回 memory bank。这样做的问题就是，memory bank 里面的 $k$ 不够一致，他们是由不同的 encoder 得到的，虽然为了解决这个问题，memory bank 方法采用了动量更新的方式更新改动的负样本，但是仍然无法从根本上解决一致性问题。</p>
<h4 id="MoCoV2"><a href="#MoCoV2" class="headerlink" title="MoCoV2"></a>MoCoV2</h4><p><a href="https://arxiv.org/pdf/2003.04297.pdf">Improved Baselines with Momentum Contrastive Learning</a></p>
<p>&emsp;&emsp;在 MoCo 发表的同一年，SimCLR 也发表了出来，因此 FAIR 团队把 SimCLR 的技术关键点移植到了 MoCo，提出了 MoCoV2，在指标上超越了 SimCLR，但是因为实在没有什么创新，因此也没有发表，作者写了个三页的小报告挂在了 arxiv 上，具体来说，改动分为这两个方面。</p>
<p>&emsp;&emsp;1.加入一个预测头，也就是解码器，这个解码器由后接 RELU 的两层 MLP 组成，这个解码器只在训练的时候使用，而在训练完成之后将这个 linear projection 扔掉，这实际上是 SimCLRV1 的策略。2.加入了一个更加有效的数据增强方式，blur augmentation，即模糊图像。之后可以专门写一篇总结基于 torchvision.transforms 的数据增强 API。</p>
<h4 id="MoCoV3"><a href="#MoCoV3" class="headerlink" title="MoCoV3"></a>MoCoV3</h4><p><a href="https://arxiv.org/pdf/2104.02057.pdf">An Empirical Study of Training Self-Supervised Vision Transformers</a></p>
<p>&emsp;&emsp;MoCoV3 是 VIT 提出之后作者将骨干网络换成 VIT 之后的续写，是在 MoCoV2 的改进基础上的续写，实际上如果仅仅如此，有可能 MoCoV3 也变成一篇技术报告，但是作者在替换骨干网络之后发现了奇怪的事情。</p>
<p>&emsp;&emsp;作者发现在将骨干网络替换为 VIT 之后出现了训练不稳定的问题，具体的不稳定体现在训练时的准确率会在某些时刻突然下降又升高。这和很多因素有关，作者做了一些实验，包括 batchsize，学习率，优化器。</p>
<p><img src="/MoCo/image-20220929230910284.png" alt="image-20220929230910284" style="zoom:57%;"></p>
<p><img src="/MoCo/image-20220929231009289.png" alt="image-20220929231009289" style="zoom:64%;"></p>
<p><img src="/MoCo/image-20220929231210763.png" alt="image-20220929231210763" style="zoom:100%;"></p>
<p>&emsp;&emsp;通过上面三张图像，我们可以发现，batchsize 和学习率都属于适中最好。这没啥说的，但问题就在于无论是选择了不太好的 batchsize 还是不太好的学习率，这些不稳定是有规律可循的。另外就是优化器，作者默认AdamW 作为优化器（常用于 ViT model），而另一方面，最近的工作经常使用 LARS 优化器处理大的 batchsize，作者对 LAMB 优化器进行实验，其是 LARS 中 AdamW 对应版本。这些优化器说实话不太理解，只知道最基础的 SGD、Adam 之类的，之后可以写一篇总结各种优化器及其优劣。总之，作者通过测试，发现 LAMB 优化器不太好，不如 AdamW。</p>
<p>&emsp;&emsp;在发现有规律可循之后，作者尝试探索了一下，打印出了网络的第一层和最后一层的参数梯度变化图像如下</p>
<p><img src="/MoCo/image-20220929231717516.png" alt="image-20220929231717516" style="zoom:80%;"></p>
<p>&emsp;&emsp;作者认为，这些突起的梯度，是因为在 MoCoV2 中加入的 linear projection 的训练问题，因此作者提出了一种策略：不训练 linear projection，仅做随机初始化。这是很离谱的想法，这样就让人很难理解为什么这样一个随机初始化又不更新的线性层对原本的 MoCo 有辅助作用，但实际上是有的，并且使用不训练策略之后梯度变化真的稳定了，而且在不同的学习率之下都是更稳定了。</p>
<p><img src="/MoCo/image-20220929232042931.png" alt="image-20220929232042931" style="zoom:80%;"></p>
<p>&emsp;&emsp;甚至不仅如此，作者还贴心地测试了 SimCLR 和 BYOL，将 SimCLR 和 BYOL 的线性层也改成随机初始化不训练的方式，神奇的是得到的训练结果真的上了几个点，并且没有了准确率的突变。</p>
<p><img src="/MoCo/image-20220929232245194.png" alt="image-20220929232245194" style="zoom:70%;"></p>
<p>&emsp;&emsp;总之，这就是 MoCoV3 的贡献，其不仅成功将 VIT 应用于 MoCo 的骨干网络，并且提升了结果，最关键的是其找到了新对比学习框架中常见的准确率突变问题的解决方案，并且在多个框架中验证了其有效性。同样地，这是 MoCoV3 的伪代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># f_q: encoder: backbone + proj mlp + pred mlp</span></span><br><span class="line"><span class="comment"># f_k: momentum encoder: backbone + proj mlp</span></span><br><span class="line"><span class="comment"># m: momentum coefficient</span></span><br><span class="line"><span class="comment"># tau: temperature</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> loader: <span class="comment"># load a minibatch x with N samples</span></span><br><span class="line">    x1, x2 = aug(x), aug(x) <span class="comment"># augmentation</span></span><br><span class="line">    q1, q2 = f_q(x1), f_q(x2) <span class="comment"># queries: [N, C] each</span></span><br><span class="line">    k1, k2 = f_k(x1), f_k(x2) <span class="comment"># keys: [N, C] each</span></span><br><span class="line">    loss = ctr(q1, k2) + ctr(q2, k1) <span class="comment"># symmetrized</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    update(f_q) <span class="comment"># optimizer update: f_q</span></span><br><span class="line">    f_k = m*f_k + (<span class="number">1</span>-m)*f_q <span class="comment"># momentum update: f_k</span></span><br><span class="line"><span class="comment"># contrastive loss</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ctr</span>(<span class="params">q, k</span>):</span><br><span class="line">    logits = mm(q, k.t()) <span class="comment"># [N, N] pairs</span></span><br><span class="line">    labels = <span class="built_in">range</span>(N) <span class="comment"># positives are in diagonal</span></span><br><span class="line">    loss = CrossEntropyLoss(logits/tau, labels)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * tau * loss</span><br></pre></td></tr></table></figure>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>&emsp;&emsp;$\rm InfoNCELoss$ 已经说过，不再赘述，之后可以专门写一篇总结$\rm MSE, RMSE, NCE,InfoNCE,BCE,CE$ 等的区别和联系。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>MoCoGAN_HD</title>
    <url>/MoCoGAN_HD/</url>
    <content><![CDATA[<h2><center> A GOOD IMAGE GENERATOR IS WHAT YOU NEED FOR HIGH-RESOLUTION VIDEO SYNTHESIS </center></h2>

<p>【视频生成】【ICLR2021】【<a href="https://arxiv.org/abs/2104.15069">paper</a>】【<a href="https://github.com/snap-research/MoCoGAN-HD">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;类似于 styleVideoGAN，本文也提出了一种使用预训练的 GAN 逆映射进行视频生成，区别于 styleVideoGAN，本文：1. 一种对动作隐式建模的网络结构，2. 在视频域和图像域进行判别器训练，3. 加入了对比学习的损失。总体来说，本文堆叠了很多 loss，在多个方面对网络进行了限制，取得了不错的结果。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/MoCoGAN_HD/image-20230414195218786.png" alt="image-20230414195218786"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>使用基于 LSTM 的结构组建了一个运动生成器，用以隐式地建模运动（表示为隐编码残差）</li>
<li>显式约束运动生成器对运动进行多样性建模的损失函数</li>
<li>对图像和视频分别使用判别器</li>
<li>使用对比学习额外训练视频生成器</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/MoCoGAN_HD/image-20230414201233911.png" alt="image-20230414201233911"></p>
<p>&emsp;&emsp;上图中左半部分描述了主要的 backbone，输入包括隐编码 $z_1$（从 mapping 之后的空间采样），运动轨迹噪声 $\epsilon_t$（从正态分布采样），PCA 基向量 $V$，输出为第二帧以及后续帧的隐编码 $z_t$，并通过图像生成器 $G_I$ 生成对应的视频帧 $\{\tilde x_t\}$，将其输入判别器 $D_V$ 进行对抗学习。</p>
<p>&emsp;&emsp;右上部分表示对判别器对抗部分，除了用 $D_V$ 直接判别整个视频  $\{\tilde x_t\}$ 之外，还需要将每一个 $\tilde x_t$ 单独输入图像判别器 $D_I$，右下部分表示对 $G_I$ 的监督除了对抗性监督之外还有一个对比监督，代理任务为个体判别。其中 $F$ 为和 $\rm LSTM_{enc}$ 结构基本一致的编码器。</p>
<p>&emsp;&emsp;具体来说，其前向过程如下：</p>
<ul>
<li><p>初始化</p>
<ul>
<li><p>从预训练的 $\mathcal W+$ 空间（或其他分布）中采样第一帧隐编码 $z_1$</p>
</li>
<li><p>从正态分布中采样 $t-1$ 个 $\epsilon_t$，用以生成多样性的运动</p>
</li>
<li>对  $\mathcal W+$ 空间进行多次采样，计算多次采样的 PCA 基向量 $V$</li>
</ul>
</li>
<li><p>运动生成器</p>
<ul>
<li>首先将 $z_1$ 输入 $\rm LSTM_{enc}$，用以初始化整个 LSTM，得到 $(h_1,c_1)$，其中 $h$ 表示隐状态，$c$ 表示单元格状态</li>
<li>递归生成所有的隐状态和单元格状态：$h_t,c_t=\mathrm{LSTM_{dec}}(\epsilon_t,(h_{t-1},c_{t-1}))$</li>
<li>对于每一个帧的隐状态，使用残差的方式建模运动（这种方式在理论上可以将运动和外观进行解耦），具体来说每一帧的隐编码 $z_t=z_{t-1}+\lambda\cdot h_t\cdot V$，其中 $\lambda$ 为超参数</li>
<li><strong>实验过程中发现很多时候运动生成器完全忽略了 $\epsilon_t$ 的作用，因此计算互信息（相似度） loss：$L_m=\frac1{n-1}\sum^n_{t=2}sim(H(h_t),\epsilon_t)$，其中 $sim(u,v)=\frac{u^Tv}{||u||\cdot||v||}$，$H$ 是一个两层 MLP</strong></li>
</ul>
</li>
<li><p>视频生成器</p>
<ul>
<li>图中显示得很清晰，$G_I$ 通过作用于每一帧 $z_t$ 从而生成最终的视频 $v$</li>
<li>其中，$G_I$ 的生产能力来自于三方面的监督：<ul>
<li><ol>
<li>视频对抗 loss：$L_{D_V}$<img src="/MoCoGAN_HD/image-20230414203649284.png" alt="image-20230414203649284" style="zoom:50%;"></li>
<li>图像对抗 loss：$L_{D_I}$<img src="/MoCoGAN_HD/image-20230414203701343.png" alt="image-20230414203701343" style="zoom:50%;"></li>
<li>对比学习 loss：$L_{contr}$<img src="/MoCoGAN_HD/image-20230414203716850.png" alt="image-20230414203716850" style="zoom:50%;"></li>
</ol>
</li>
<li>其中对抗损失都是普通的 GANLoss，对比学习损失意味着：将某帧生成的图像进行不同的数据增强得到正样本 / 将某两个视频的生成图像进行相同（或不同）的数据增强为负样本</li>
</ul>
</li>
</ul>
</li>
<li><p>总体的 loss 在上述四个 loss 之外，参考 <a href="https://arxiv.org/abs/1711.11585">pixel2pixelHD</a> 设计了生成的第一帧和其他帧之间的特征匹配损失 $L_f$，用以显示视频的语义一致性</p>
</li>
</ul>
<p>&emsp;&emsp;整体的优化 loss 为：</p>
<p><img src="/MoCoGAN_HD/image-20230414204214342.png" alt="image-20230414204214342" style="zoom:50%;"></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><ul>
<li>视频生成指标，其中 ACD(Average Content Distance) 指标继承自 MoCoGAN，用以度量一个视频中的人脸是否 ID 保持一致</li>
</ul>
<p><img src="/MoCoGAN_HD/image-20230414204246665.png" alt="image-20230414204246665"></p>
<ul>
<li>视频预测指标</li>
</ul>
<p><img src="/MoCoGAN_HD/image-20230414204516449.png" alt="image-20230414204516449"></p>
<ul>
<li>更进一步，本文实现了跨域的视频生成，可以看出在人脸上训练的网络对于类人脸的动物或者动漫都有一定的迁移能力，同时能看出具备一定的多样性</li>
</ul>
<p><img src="/MoCoGAN_HD/image-20230414204645741.png" alt="image-20230414204645741"></p>
<ul>
<li>在不同数据集上的消融实验</li>
</ul>
<p><img src="/MoCoGAN_HD/image-20230414204715371.png" alt="image-20230414204715371"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>NUWA</title>
    <url>/NUWA/</url>
    <content><![CDATA[<h2><center> NUWA: Visual Synthesis Pre-training for Neural visUal World creAtion </center></h2>

<p>【图/视频生成】【ECCV2022】【<a href="http://arxiv.org/abs/2111.12417">paper</a>】【<a href="https://github.com/microsoft/NUWA">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种统一的文本图像视频表示方法，基于此实现了全模态统一的视频/图像条件生成，值得一提的是，在本文之后，MSRA 又推出了能生成 38912 x 2048 分辨率图像的 NUWA-Infinity，以及 23 年 4 月推出了能生成 11 mins 动画的 NUWA-XL。网络使用 VQGAN 的结构，搭建了基于 transformer 的编解码器，为了解决复杂度的问题设计了 3DNA 的近邻结构。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/NUWA/image-20230412094014616.png" alt="image-20230412094014616"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>一种全统一（文本+图像+视频）的多模态生成模型</li>
<li>一种基于近邻的注意力计算方法 3DNA（3D Nearby Attention），有效减少了视频 transformer 复杂度</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/NUWA/image-20230412102033102.png" alt="image-20230412102033102"></p>
<p>&emsp;&emsp;为了能够将文本和图像视频一起表示，本文采用 $X\in\mathbb R^{h\times w\times s\times d}$ 的方式表示输入，其中 $h,w$ 表示图像的 shape，$s$ 表示文本视频长度，$d$  表示 token 维度。即文本 $\mathbb R^{1\times 1\times s\times d}$，图像 $\mathbb R^{h\times w\times 1\times d}$，视频 $\mathbb R^{h\times w\times t\times d}$。</p>
<p>&emsp;&emsp;上图中显示的是整体的网络 IO，其中的编解码器结构没有显示，具体在编解码器上的 transformer 结构也没有显示，对于网络结构，具体来说：本文采用基于 transformer 的 VQGAN 架构，对于视频数据的 transformer 结构，为了降低复杂度设计了 3DNA，其做法如下：</p>
<ul>
<li>3DNA 的 IO 表达式为：$Y=3DNA(X,C;W)$，其中 $Y$ 为输出，$X$ 为输入，$C$ 为条件，$W$ 为参数<ul>
<li>其中输入的 shape 为：$X\in\mathbb R^{h\times w\times s\times d_{in}},\ C\in\mathbb R^{h’\times w’\times s’\times d_{in}}$，都打过 patch 了</li>
</ul>
</li>
<li>对于 $X$ 的下标 $(i,j,k)$，首先寻找 $C$ 中对应的下标 $(i’,j’,k’)=([i\frac {h’}h],[j\frac {w’}w],[k\frac {s’}s])$</li>
<li>寻找 $C$ 中和 $(i’,j’,k’)$ 临近的 patch idx $N\to(e^h,e^w,e^s,d_{in})$，仅对这些做 attn，其中 $e^h,e^w,e^s$ 在图像下为 3,3,1，在视频下为 3,3,3</li>
</ul>
<p><img src="/NUWA/image-20230412110441508.png" alt="image-20230412110441508" style="zoom:40%;"></p>
<ul>
<li>接下来计算 attn ，对于 $Y$ 的 $(i,j,k)$ 下标，结果为：$Y_{(i,j,k)}=softmax(\frac{QK^T}{d_{in}})V\to(h,w,s,d_{out})$，再通过 MLP 即可。其中<ul>
<li>$Q=X\times W^Q\to(h,w,s,d_{out})$</li>
<li>$K=N\times W^K\to(e^h,e^w,e^s,d_{out})$</li>
<li>$V=N\times W^V\to(e^h,e^w,e^s,d_{out})$</li>
</ul>
</li>
<li>该 attn 计算需要经过共  $l$ 层，对 $C$ 来说也需要同步逐层更新：$C^l=3DNA(C^{l-1}, C^{l-1})$</li>
<li>在计算 attn 的过程中的位置编码为可学习的相对位置编码，在 $h,w,s$ 三个维度分开学习</li>
<li>使用 441x256 的codebook dim，codebook size 为 12288，进行 VQ</li>
<li>使用完全反向的解码器进行解码，loss 采用 VQGAN 的loss</li>
<li>对于第二阶段，由于本文做了视频文本图像三个任务（文本到图像，视频预测和文本到视频），因此第二阶段采用 GPT 类型的三个损失函数</li>
</ul>
<p><img src="/NUWA/image-20230412112545692.png" alt="image-20230412112545692" style="zoom: 40%;"></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;本文的实验十分翔实充分，在补充材料里给了将近 20 页的各种实验，无论是不同任务上的结果还是消融实验都很多，这里只列一下主任务结果和视频预测 FVD。</p>
<ul>
<li>和其他模型在文转图的视觉结果对比</li>
</ul>
<p><img src="/NUWA/image-20230412112837766.png" alt="image-20230412112837766"></p>
<ul>
<li>和其他模型在文转视频视觉效果对比</li>
</ul>
<p><img src="/NUWA/image-20230412112916381.png" alt="image-20230412112916381"></p>
<ul>
<li>视频预测结果（FVD）</li>
</ul>
<p><img src="/NUWA/image-20230412113018082.png" alt="image-20230412113018082" style="zoom:40%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>Orthogonal_VQGAN</title>
    <url>/Orthogonal_VQGAN/</url>
    <content><![CDATA[<h2><center> Exploration into Translation-Equivariant Image Quantization </center></h2>

<p>【VQGAN探索】【ICASSP<strong>(CCF_B)</strong>】【<a href="arXiv:2112.00384">paper</a>】【<a href>code</a>未开源】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了在 VQ 的时候由于码本中不同编码的堆叠，可能导致性能的下降以及平移等变性的缺失，因此以一种很简单的方式消除了堆叠，具体来说，即加入正交化损失，通过强制 codebook 正交化减少堆叠。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Orthogonal_VQGAN/image-20230330092831822.png" alt="image-20230330092831822" style="zoom:50%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>正交化损失，用于减少堆叠以及保持平移等变性</li>
<li>正交初始化，用于减少堆叠（以本文为经验可以得出的结果，具备明显的合理性）</li>
</ul>
<h3 id="为什么要正交？"><a href="#为什么要正交？" class="headerlink" title="为什么要正交？"></a>为什么要正交？</h3><p>&emsp;&emsp;图像经过 CNN 之后的平移等变性缺失是老生常谈的话题，以可视化的例子如下所示，下图中随着图像主体的平移，普通的 CNN 结构（baseline）的置信度变化方差很大，而抗堆叠的方法会好很多。</p>
<p><img src="/Orthogonal_VQGAN/video_00810.gif" alt></p>
<p>&emsp;&emsp;所谓平移等变，事实上在分类任务中要求的应该是平移不变性，就是平移之后输出不变，对应在分割等任务上应该是平移等变，就是平移之后输出的 box 等跟随平移。但是我觉得在分类任务其实也得要求平移等变，并非以结果和输入等变，而是以特征图和输入等变，如下图所示。而平移不变性则是在等变的特征图的基础上后续预测网络的要求。</p>
<p><img src="/Orthogonal_VQGAN/v2-73708d7f00e07e836bd63a96baad638e_1440w.webp" alt="img" style="zoom:50%;"></p>
<p>&emsp;&emsp;之所以会缺失平移等变性，主要是因为在降采样的过程中，无论是卷积降采样还是池化降采样，都会损失相对位置信息，以 MaxPooling 为例，第一步取 max 实际上只是在一定程度上做了平均，算是一种信息的压缩，但是这里的压缩损失的只是细节信息，但是第二步的池化操作会把原本离得或近或远的 max 的 index 放在相邻位置。这种降采样导致在周期性平移（平移长度为步长整数倍）时可以获得平移等变的结果，而除此之外就会产生较大的误差。因此可以看出 baseline 并不是一直都差，而是周期性地好坏。</p>
<h3 id="如何保证码本正交？"><a href="#如何保证码本正交？" class="headerlink" title="如何保证码本正交？"></a>如何保证码本正交？</h3><p>&emsp;&emsp;码本的正交可以最大限度地消除码本的冗余性，从而在特征向量少量移动时保证量化之后的向量尽可能不变。</p>
<p>&emsp;&emsp;至于做法包含两个层面：</p>
<ul>
<li>初始化的时候将随机初始化的变量进行正交化处理</li>
<li>使用正交化损失进行强制 codebook 正交化，orthogonality loss 写作：</li>
</ul>
<script type="math/tex; mode=display">
\mathcal L_{REG}(e)=\lambda\frac{1}{K^2}||l_2(e)^Tl_2(e)-I_K||_2^2</script><p>&emsp;&emsp;即，对码表的编码进行 l2_normlize 之后计算正交矩阵，使其和单位矩阵尽可能接近，$K^2$ 用以消除尺度误差，$\lambda$ 用以平衡权重。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>PGGAN</title>
    <url>/PG_GAN/</url>
    <content><![CDATA[<h2><center> Progressive Growing of GANs for Improved Quality, Stability, and Variation </center></h2>

<p>【图像生成】【ICLR2018】【<a href="http://arxiv.org/abs/1710.10196">paper</a>】【<a href="https://github.com/tkarras/progressive_growing_of_gans">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文使用逐步调大分辨率的生成方式成功生成了在当时还不错的高分辨率图像。具体来说是进行分阶段的训练，阶段按照分辨率进行分界。并且每个新的阶段要加入新的卷积层，为了防止没有经过训练的层回传梯度时影响训练好的层，本文还使用了一种平滑的融合策略。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/PG_GAN/image-20230105204451398.png" alt="image-20230105204451398"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>逐步并行地向判别器和生成器之中加入新的卷积层从而最终生成高分辨率图像，这使得训练更稳定、迅速</li>
<li>使用类似于“动量更新”的方式加入新的卷积层，防止其刚开始训练时回传的粗梯度影响之前的结果</li>
<li>在判别器的最后使用 <code>minibatch</code> 的方式分组无参计算每个 <code>batch</code> 内的分布加强判别器的能力</li>
<li>在训练时使用多通道特征训练，在需要可视化的时候加入一个 <code>to_rgb</code> 层</li>
<li>使用 $\mathcal N(0,1)$ 初始化权重，并且在运行的过程中动态缩放权重</li>
<li>提出了一种基于 <code>MS-SSIM</code> 的生成图像评价指标，可以对生成的图像在不同尺度上的细节分布进行检测</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><h4 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a>backbone</h4><p><img src="/PG_GAN/image-20230104212316211.png" alt="image-20230104212316211"></p>
<p>&emsp;&emsp;整个网络的生成器和判别器由两个对称的 CNN 网络组成，不同的是在判别器的最后一层 CNN 时加入了 <code>Minibatch stddev</code> 操作，用以计算某个 <code>batch</code> 内分组的分布，作为额外的特征通道加入 <code>C</code> 维度。</p>
<p>&emsp;&emsp;网络的训练逐步增加生成的图像分辨率，方式是逐步添加新的卷积层，上图中的 $4\times 4$ 表示针对 <code>NxCx4x4</code> 的特征的卷积操作，具体来说其中不包括上下采样，只包括多个卷积层和 <code>leaky_relu</code> 激活函数。上下采样没有在上图中显示，其存在于卷积层之间。</p>
<p>&emsp;&emsp;生成器逐步生成更高 <code>HxW</code> 的特征，最终得到 <code>NxCx1024x1024</code> 的特征，并由一个 <code>1x1Conv</code> 的 <code>to_rgb</code> 层整形成 <code>rgb</code> 通道的图像。该图像输入至判别器中由一个和生成器镜像对称的结构判断 <code>T/F</code>。</p>
<p><img src="/PG_GAN/image-20230105094832867.png" alt="image-20230105094832867"></p>
<p>&emsp;&emsp;加入新的卷积层的方式如上图所示，其展示了从 <code>16x16</code> 分辨率的网络加入新的卷积层变为 <code>32x32</code> 分辨率网络的过程。$(a)$ 表示 <code>16x16</code> 对应的 $G,D$，$(c)$ 表示一种朴素的加入方式，其中 <code>2x,0.5x</code> 表示上下采样。这种方式在实现之后会产生对训练好的网络的冲击，这种冲击来自于新加入的层最初训练时回传的梯度并没有太大的意义，而训练好的层接受到这种梯度并更新之后，原有的参数会被破坏。因此本文采用了 $(b)$ 表示的结构，使用 $\alpha\in[0,1]$ 作为平衡新层旧层权重的量，这种处理方式和“动量更新”或者迁移学习都很近似。</p>
<p>&emsp;&emsp;最终得到的生成 $1024^2$ 分辨率的网络结构参数如下。</p>
<p><img src="/PG_GAN/image-20230105091756017.png" alt="image-20230105091756017"></p>
<p>&emsp;&emsp;实际处理中整体上 $G,D$ 是镜像结构，但是 $D$ 中多了 <code>Minibatch stddev</code> 操作，其具体流程为：</p>
<ul>
<li>按照 <code>batch_size=N</code> 分解为 <code>N=GxM</code>，其中 <code>G</code> 为分组大小，<code>M=N/G</code>，此时特征变为 <code>GMCHW</code></li>
<li>在 <code>G</code> 维度上计算标准差，得到 <code>MCHW</code></li>
<li>在 <code>C,H,W</code> 维度上再计算平均值，得到 <code>M111</code></li>
<li>将上一步的结果在 <code>M</code> 维度上复制 <code>G</code> 次将 <code>M</code> 变成 <code>N</code>，在后两维度上各复制 <code>H,W</code> 次，得到 <code>N1HW</code></li>
<li>将上一步的结果和原始的输入进行拼接，得到 <code>N(C+1)HW</code></li>
</ul>
<p>&emsp;&emsp;其具体实现代码为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Minibatch standard deviation.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">minibatch_stddev_layer</span>(<span class="params">x, group_size=<span class="number">4</span></span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;MinibatchStddev&#x27;</span>):</span><br><span class="line">        group_size = tf.minimum(group_size, tf.shape(x)[<span class="number">0</span>])     <span class="comment"># Minibatch must be divisible by (or smaller than) group_size.</span></span><br><span class="line">        s = x.shape                                             <span class="comment"># [NCHW]  Input shape.</span></span><br><span class="line">        y = tf.reshape(x, [group_size, -<span class="number">1</span>, s[<span class="number">1</span>], s[<span class="number">2</span>], s[<span class="number">3</span>]])   <span class="comment"># [GMCHW] Split minibatch into M groups of size G.</span></span><br><span class="line">        y = tf.cast(y, tf.float32)                              <span class="comment"># [GMCHW] Cast to FP32.</span></span><br><span class="line">        y -= tf.reduce_mean(y, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)           <span class="comment"># [GMCHW] Subtract mean over group.</span></span><br><span class="line">        y = tf.reduce_mean(tf.square(y), axis=<span class="number">0</span>)                <span class="comment"># [MCHW]  Calc variance over group.</span></span><br><span class="line">        y = tf.sqrt(y + <span class="number">1e-8</span>)                                   <span class="comment"># [MCHW]  Calc stddev over group.</span></span><br><span class="line">        y = tf.reduce_mean(y, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], keepdims=<span class="literal">True</span>)      <span class="comment"># [M111]  Take average over fmaps and pixels.</span></span><br><span class="line">        y = tf.cast(y, x.dtype)                                 <span class="comment"># [M111]  Cast back to original data type.</span></span><br><span class="line">        y = tf.tile(y, [group_size, <span class="number">1</span>, s[<span class="number">2</span>], s[<span class="number">3</span>]])             <span class="comment"># [N1HW]  Replicate over group and pixels.</span></span><br><span class="line">        <span class="keyword">return</span> tf.concat([x, y], axis=<span class="number">1</span>)                        <span class="comment"># [NCHW]  Append as new fmap.</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;最终采用的 <code>loss</code> 为 <code>improved Wasserstein loss</code>。</p>
<h4 id="MS-SSIM"><a href="#MS-SSIM" class="headerlink" title="MS-SSIM"></a>MS-SSIM</h4><p><img src="/PG_GAN/image-20230105101714321.png" alt="image-20230105101714321"></p>
<p>&emsp;&emsp;本文提出现有的 <code>MS-SSIM</code> 方法只能对大尺度的模式崩溃进行响应，但不能对较小的影响做出反应，如颜色或纹理变化的损失，而且也不能从与训练集的相似性方面直接评估图像质量。上图展示了现有的 <code>MS-SSIM</code> 方法的流程，其中 $c,s$ 表示特征，$L$ 表示低通滤波，$2\downarrow$ 表示降采样。</p>
<p>&emsp;&emsp;本文基于同样的多尺度策略提出了另一种思路，也就是对于多张图像，每张图像逐步使用拉普拉斯金字塔进行多尺度滤波，对每一层的图像提取 <code>128</code> 个特征描述子，每个描述子由 <code>7x7</code> 个邻居的 <code>3</code> 个通道组成，对于提取出的描述子，首先标准化每个颜色通道的均值和标准差，然后通过计算它们的切片 <code>Wasserstein</code> 距离作为相似度。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/PG_GAN/image-20230105114256331.png" alt="image-20230105114256331"></p>
<p>&emsp;&emsp;该表使用了 <code>WGAN</code> 的 <code>baseline</code>，逐步地添加各个改进策略，用以进行消融实验验证策略有效性，其中 <code>Sliced Wasserstein distance</code> 下的 <code>128,64,32,16,Avg</code> 表示在不同尺度下的 <code>WSD</code>，其最终得到的图像如下图所示。</p>
<p><img src="/PG_GAN/image-20230105115119346.png" alt="image-20230105115119346"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>Privacy-Phys</title>
    <url>/Privacy_Phys/</url>
    <content><![CDATA[<h2><center> Privacy-Phys: Facial Video-based Physiological Modification for Privacy Protection </center></h2>

<p>【心率监测】【SPL2022】【<a href="https://ieeexplore.ieee.org/document/9806161/">paper</a>】【<a href>code未开源</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种基于预训练的 3D-CNN 网络结构用于在保持视觉不变性的基础上修改视频中人脸的 rppg 信号，可以用于 IDEA 中的正样本生成模块之中。另外，本文讲故事的方式是：rppg 信号中具备人的生理和疾病等隐私信息，这些信息不希望被他人窃取，因此使用 PrivacyPhys 方法可以快速地修改 rppg 信号以干扰窃取，可以用于视频软件之中。</p>
<p>&emsp;&emsp;一个有意思的点在于：本文所谓的预训练的网络其实完全不参与微调，梯度也不用来更新网络参数，而是直接更新结果。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Privacy_Phys/image-20221124091934773.png" alt="image-20221124091934773" style="zoom:80%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>一种新的用于修改人脸视频中 rppg 信号的 3DCNN 网络</li>
<li>该网络修改之后的视频可以混淆当前检测的 SOTA 方法（有点像 deepfake 和 detection 了，好怪)</li>
<li>相比于之前的大作 PulseEdit，这个方法推理更加迅速</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/Privacy_Phys/image-20221124092312238.png" alt="image-20221124092312238" style="zoom:67%;"></p>
<p>&emsp;&emsp;本文的前向过程非常简单，首先对于输入的视频 $z$，copy 一份同样的视频 $x$（$x$ 最后就是输出），同时根据人脸分割生成皮肤掩码 $S$，生成 $S$ 的目的是：心率编辑不应当修改除了人脸范围以外的其他像素。</p>
<p>&emsp;&emsp;接着读入目标心率 $y$，通过一个训练好并且固定权重的 3DCNN，这里是 STN，但也可以使任何可以快速计算的网络，记为 $G_\theta$ ，对 $x$ 进行 rppg 信号提取，提取得到的信号记为 $G_\theta(x)$，然后开始计算 loss。</p>
<p>&emsp;&emsp;loss 的计算分为两个部分：1.用于保证原始视频和新视频视觉效果一致的损失 $L_{f}$，2.用于保证修改之后的视频提取出的 rppg 信号和目标信号一致的损失 $L_{rppg}$，具体来说：</p>
<script type="math/tex; mode=display">
\rm
\begin{aligned}
&L_{f}(x,z)=\frac1{T\cdot C\cdot H\cdot W}||x-z||_2^2\\
&L_{rppg}(G_{\theta^*(x)},y)=L_{rppg}(\hat y,y)=\frac{-cov(\hat y, y)}{\sigma_{\hat y}\cdot \sigma_y}\\
where\ \ \ \ &\sigma_y=\sqrt{\frac 1 T \sum_{t=1}^Ty(t)^2-(\frac 1 T \sum_{t=1}^Ty(t))^2}\\
and\ \ \ \ &cov(\hat y,y)=\frac 1 T \sum_{t=1}^T\hat y(t)y(t)-\frac 1 T \sum_{t=1}^T\hat y(t)\cdot\frac 1 T \sum_{t=1}^Ty(t)
\end{aligned}</script><p>&emsp;&emsp;简单概括来说就是：使用 MSELoss 限制视频的视觉一致性，使用 NPC 限制 rppg 信号的一致性。</p>
<p>&emsp;&emsp;获得两个损失之后求和得到最终的损失：$L=L_{rppg}+\lambda L_f$，$\lambda$ 用于控制视频和 rppg 信号的重要程度。通过 $L$ 向输出 $x$ 求导得到 $\nabla_xL$，并使用此梯度更新 $x$，但是这里注意更新的其实是 $S\bigotimes x$，也就是只更新人脸 mask 之内的部分。具体实现中直接全部更新，更新之后再做掩码即可。</p>
<p>&emsp;&emsp;整体来说，本文的优化目标可以看做：</p>
<script type="math/tex; mode=display">
x=arg\min_{x\in R^{T\times C\times H\times W}} L_{rppg}(G_\theta(x),y)+\lambda L_f(x,z)\\
s.t.\ \ S\bigotimes x=S\bigotimes z</script><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;本文的创新性比较高，整体的实现有点天马行空，消融实验只能做一下 $\lambda$，其他都是对比实验。</p>
<ul>
<li>$\lambda$ 的选择</li>
</ul>
<p>&emsp;&emsp;这部分选择了 7 种预训练的模型作为 $G$，分别探索了在不同 $\lambda$ 的选择下对于视频 PSNR 和 rppg 信号 MAE 的平衡性，最终选择了 $\lambda=0.1$（其实我感觉有点怪，这个 PSNR 都最低 51 了 $\dots\dots$ 正常来说 30+ 就是高质量图像了，35+ 都肉眼无法区别了，这意思感觉像是 $\lambda$ 可以不要一样，后面就知道为啥明明都 50+ 了还要选“平衡”）</p>
<p><img src="/Privacy_Phys/image-20221124101025603.png" alt="image-20221124101025603" style="zoom:80%;"></p>
<ul>
<li>和 PulseEdit 的对比</li>
</ul>
<p>&emsp;&emsp;同样是之前选择的预训练模型，这里对比了在 PURE 和 MMSE 中和 PulseEdit 的结果，可以看出基本上在正常的 PSNR 下都是 PrivacyPhys 更好一些（就是为了全面超 PulseEdit 才选 0.1 的 $\lambda$ 以提高 PSNR）</p>
<p><img src="/Privacy_Phys/image-20221124102201504.png" alt="image-20221124102201504"></p>
<p><img src="/Privacy_Phys/image-20221124101931885.png" alt="image-20221124101931885"></p>
<ul>
<li>注意区域的对比</li>
</ul>
<p>&emsp;&emsp;可以看出相对来说，PrivacyPhys 还是更加关注了正确的区域，但是总觉得也就那样，和其他的注意力图展示出来差不少，整体来说 rppg fake 或许都不太好。</p>
<p><img src="/Privacy_Phys/image-20221124102235096.png" alt="image-20221124102235096"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>PulseEdit</title>
    <url>/PulseEdit/</url>
    <content><![CDATA[<h2><center> PulseEdit: Editing Physiological Signal in
Facial Videos for Privacy Protection </center></h2>

<p>【心率检测】【TIFS2022】【<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9680677">paper</a>】【<a href>code未开源</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种在不改变外观特征的情况下进行心率信号改变的方法，具体来说，使用一系列的图像处理步骤得到逐帧的像素更改区域以及更改量，直接更改 RGB 区域从而混淆估计器。区别于 privacy-phys，本文梯度更新的是扰动信号 $\delta$，而 privacy-phys 梯度直接更新结果，一致的是两篇文章都没有参数。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/PulseEdit/image-20221207155000703.png" alt="image-20221207155000703" style="zoom:50%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>一种新的用于更改或者抹除 rppg 信号的算法</li>
<li>结合 pulseEdit 和 deepfake 技术探索能否绕过活体检测或者 deepfake detection</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p>&emsp;&emsp;本文没有实际上的网络，其实更加是一种算法，由于梯度更新的只是扰动量，因此本文实际上把 rppg 信号的编辑抹除过程看做优化问题。而区别编辑和抹除，在优化目标为抹除的时候，直接以高斯噪声为编辑目标即可。</p>
<p><img src="/PulseEdit/image-20221207155241895.png" alt="image-20221207155241895"></p>
<p>&emsp;&emsp;在前向过程中，大体分为三个部分：首先是 rppg 信号提取部分，目的是从背景中选取 ROI 区域，做法是采用高斯皮肤色彩模型；然后是 rppg 信号编辑部分，目的是给出扰动量的表达式从而写出优化表征；最后是更改部分，使用前一部分的结果直接编辑视频即可。</p>
<ul>
<li>rppg 信号提取部分</li>
</ul>
<p>&emsp;&emsp;首先使用 Dlib 库函数提取出 landmark，但是没有开源我也不太确定这里是每帧都做还是只做第一帧。</p>
<p>&emsp;&emsp;然后使用高斯皮肤色彩模型将 ROI 转化到颜色空间，描述为：</p>
<script type="math/tex; mode=display">
p(x)=\exp(-\frac12(x-m)^T\Sigma^{-1}(x-m))\begin{matrix}skin\\\to\\non-skin\end{matrix}p_t</script><p>&emsp;&emsp;通过上述模型可以判断出一个像素点是否属于皮肤，接着从每个 ROI 区域之中选取 M 个子区域（本文的设置为 M=6x6），每个子区域含有 RGB 三通道，同时含有 N 帧，因此记选出的 M 个 ROI 区域为 $R\in\mathbb R^{M\times3\times N}$</p>
<ul>
<li>rppg 信号编辑部分</li>
</ul>
<p>&emsp;&emsp;对于提取出的多个 ROI $R$，进行去势处理，假设去势之后的信号为 $S$，这里的 $S$ 加入扰动后和 rppg 信号 $T$ 需要对齐。去势阶段对 $S$ 的要求包括：$S$ 的信号本身峰值不能太高，$S$ 的局部变化应该和 $R$ 一致，这主要是为了消除环境光照的影响（亮度不变性）</p>
<p>&emsp;&emsp;因此描述为：</p>
<script type="math/tex; mode=display">
\min_{S_{i,c}}\ \frac 1 2||S_{i,c}||_2^2+\mu||D(R_{i,c}-S_{i,c}) ||_1,\ \forall \ i,c</script><p>&emsp;&emsp;其中 $S\in\mathbb R^{M\times 3\times N}$，$S_{i,c}\in\mathbb R^{N}$，$D\in\mathbb R^{(N-2)\times N}$，$D$ 是一个差分矩阵，其值为：</p>
<script type="math/tex; mode=display">
D = \begin{bmatrix}
-1&2&-1\\
&-1&2&-1\\
&&\ddots&\ddots&\ddots&\\
&&&-1&2&-1\\
&&&&-1&2&-1
\end{bmatrix}</script><p>&emsp;&emsp;因此 $D$ 和 $(R-S)$ 的乘积代表着在某个固定的 ROI 的某个固定的通道内，差值每个帧与前后两帧之间的差值 $2x_i-x_{i-1}-x_{i+1}$ 应当接近 0，也即 $R$ 和 $S$ 趋势尽可能接近。</p>
<p>&emsp;&emsp;对于提取出的信号 $S$ ，其本质上就是 $R$ 的亮度不变性版本，和 rppg 信号并不对得上，因此在其上加入扰动 $\delta\in \mathbb R^{3\times N}$，将 $\delta$ 重复之后加到 $S$ 上就得到了 $\hat S$，$\hat S=S_{i,c}+\delta_c$，其中 $\delta_c$ 表示在某个通道下的扰动。</p>
<p>&emsp;&emsp;对于 $\hat S$，其应当和 $T$ 尽可能接近，此时使用负皮尔森相关性 NPC 度量，记为：</p>
<script type="math/tex; mode=display">
\min_{\delta} -\frac 1 M \sum_{i,c}\rho(\hat S_{i,c},T_c)=-\frac 1 M \sum_{i,c}\rho( S_{i,c}+\delta_c,T_c)</script><p>&emsp;&emsp;其中 $T\in\mathbb R^{3\times N}$，由于没有开源，因此不知道是怎么从 $N$ 扩展到 $3\times N$ 的，猜测应该是直接重复填充的。除了上述要求之外，还需要使得扰动足够小，因此描述为：</p>
<script type="math/tex; mode=display">
\frac 1N ||\delta||_2^2</script><p>&emsp;&emsp;综上，$\delta$ 的求解可以看做最小化问题：</p>
<script type="math/tex; mode=display">
\min_\delta-\frac 1 M \sum_{i,c}\rho( S_{i,c}+\delta_c,T_c)+\lambda\frac1N||\delta||_2^2</script><p>&emsp;&emsp;说白了接下来就以这个作为 loss，用 Adam 优化器以 lr=0.1 迭代 200 个 epoch，鉴于这篇文章最初都不提自己的 efficient，因此确实比较慢，后面结果有分析</p>
<ul>
<li>皮肤信号修改</li>
</ul>
<p>&emsp;&emsp;在这部分，我们希望的是能够通过扰动 $\delta$ 改变图像视觉效果，需要注意的是对于是否含有人脸的部分调整是不同的，非人脸区域不应该做调整，即使在人脸区域，扰动的调整也具备可调节性。而本文选择在不同位置随机给 $\delta $ 加扰动，没有说明为啥一定要取这个随机。</p>
<p>&emsp;&emsp;具体来说，我们希望以 $p$ 的概率取 $\delta$ 的下整，除此之外取上整，又为了让整体的 $\delta $ 分布尽可能一致，因此有：</p>
<script type="math/tex; mode=display">
\delta_c(n)=\lfloor\delta_c(n)\rfloor p+\lceil\delta_c(n)\rceil(1-p)</script><p>&emsp;&emsp;上式化简之后可以得到：</p>
<script type="math/tex; mode=display">
p=\lceil\delta_c(n)\rceil-\delta_c(n)</script><p>&emsp;&emsp;故而在所有的人脸区域范围内，随机取 $p\in(0,1)$，判断 $p$ 和上述式子的阈值大小关系，从而决定使用 $\delta$ 的上整还是下整，其伪代码如下：</p>
<p><img src="/PulseEdit/image-20221207174237612.png" alt="image-20221207174237612" style="zoom:50%;"></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;由于本文实际上是针对 rppg 信号编辑的算法，因此没有和 SOTA 的对标，首先做了有效性的可视化实验，选取了三个传统方法在不同的 $\lambda$ 下进行实验，从结论上看 $\lambda$ 不能太大。可视化效果中确实还行。同时还做了在 MPEG-4 压缩之后的视频中提取修改的 rppg 信号的实验，结果大差不差。</p>
<p><img src="/PulseEdit/image-20221207174532830.png" alt="image-20221207174532830" style="zoom:80%;"></p>
<p>&emsp;&emsp;接着是这类工作最重要的实验：分析是否确实在保证了视觉不变性的情况下改变了信号的数据测试：</p>
<p><img src="/PulseEdit/image-20221207174838630.png" alt="image-20221207174838630"></p>
<p>&emsp;&emsp;其实还有很多的实验，主要是测试是否能够结合 deepfake 的，和我不太相关，因此不再记录。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>PulseGAN</title>
    <url>/PulseGAN/</url>
    <content><![CDATA[<h2><center> PulseGAN: Learning to generate realistic pulse
waveforms in remote photoplethysmography </center></h2>

<p>【心率检测】【BHI2021】【<a href="https://arxiv.53yu.com/pdf/2006.02699">paper</a>】【<a href="https://github.com/miki998/PulseGAN.git">code非官方</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文引入了一种使用 GAN 训练的网络，网络并不直接从原始视频中获得 rppg 信号，而是使用其他方法已经估计出的粗糙 rppg 信号进行细化，BHI 是生物信息期刊，这篇文章相对其他论文具备更多的专业性指标，但是实验做得很少，没有在其他数据集内验证有效性。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/PulseGAN/image-20221207193415813.png" alt="image-20221207193415813"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>针对已有的粗糙的 rppg 信号估计更加精确的信号</li>
<li>结合了 GAN 和 Conv（可惜这里 GAN 并非生成式任务，不然可以试着用 DM 代替）</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/PulseGAN/image-20221208122816922.png" alt="image-20221208122816922"></p>
<p>&emsp;&emsp;这篇文章的网络结构很简单，主要分为两个部分：1. 已有的方法估计出的粗糙的 rppg 信号，2. 使用 GAN 进行对抗式训练。</p>
<p>&emsp;&emsp;首先将输入 $V\in\mathbb R^{T\times C\times H\times W}$ 输入某个方法用于提取粗糙信号 $X\in\mathbb R^{T}$，为了快速训练和对运动鲁棒，本文选择的方法是 CHROM（又或许是因为 SOTA 很难再改进了），实际上这种方式在 Dual-GAN 里面也有运用，使用这种类似 CGAN 结构训练的话，一个直观的想法是直接从原视频中生成 rppg 信号。</p>
<p>&emsp;&emsp;但是无论是这篇文章还是 Dual-GAN 都没有这么做的原因应该是原视频包含了太多的噪音。因此 CHROM 的预处理相当于剔除环境噪音，而 Dual-GAN 则采用 STMap 进行输入，在某种程度上也是去除环境噪音。</p>
<p>&emsp;&emsp;将处理之后的粗糙信号 $X$ 输入 U-Net 形状的生成器 $G$，得到整形之后的输出 $G(X)\in\mathbb R^{T}$，接着借用 CGAN 的方式，将 $G(X)$ 和 $X$ 组合（cat）之后输入判别器 $D$ 并判断概率。</p>
<p>&emsp;&emsp;判别器 $D$ 总共接受两个输入：1. $(G(X),X)$，2. $(X_C,X)$，其中 $X_C$ 为 GT。判别器期望对输入 1 判别为假，对输入 2 判别为真，生成器则相反。判别器估计之后的结果计算 loss 之后回传至生成器。</p>
<p>&emsp;&emsp;其中，生成器 $G$ 的网路结构如图所示，整个网络是 U-Net 形状的网络，包含 6 个降采样和升采样模块，每个模块由 1d 卷积和 PReLU（在 ReLU 的基础上的改进，用于防止过拟合）组成，解码器则是对应的 1d 转置卷积。</p>
<p><img src="/PulseGAN/image-20221208124435592.png" alt="image-20221208124435592" style="zoom:50%;"></p>
<p>&emsp;&emsp;同上，判别器 $D$ 的网络结构如下：</p>
<p><img src="/PulseGAN/image-20221208124701079.png" alt="image-20221208124701079" style="zoom:50%;"></p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>&emsp;&emsp;对抗训练的损失可以写为：</p>
<script type="math/tex; mode=display">
\begin{align}
&L_G=\frac12(D(G(X),X)-1)^2+\lambda||X_c-G(X)||_1+\beta||X_{cf}-G_f(X)||_1\\
&L_D=\frac12(D(G(X),X))^2+\frac12(D(X_c,X)-1)^2
\end{align}</script><p>&emsp;&emsp;其中， $X_{cf}$ 和 $G_f$ 分别表示对 $X_c,\ G(X)$ 进行傅里叶变换向频率域的映射后的信号值，生成器期望：1. $G(X)$ 难以分辨真假，2. $G(x)$ 在时间域上接近 GT，3. $G(X)$ 在频率域上接近 GT；判别器期望：能够分辨 $G(X)$ 和 $X_c$。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;本文整体来说实验较少，主要实验是在 UBFC 上做了数据集内测试如下所所示，其中的 $HR_{mer}$ 表示平均错误率百分比，至少在 MAE 和 RMSE 上达到的结果只能说还 ok，毕竟在 21 年的时候 Siamese rppg 也发表了。</p>
<p><img src="/PulseGAN/image-20221208130646082.png" alt="image-20221208130646082" style="zoom:67%;"></p>
<p>&emsp;&emsp;接着是跨数据集测试，PURE → UBFC。可以看出指标下滑较多，因为只选了 DAE 做对比因此看上去还行，整体上不如 Siamese rppg，不过跨数据集只做了这一个略显寒酸了。</p>
<p><img src="/PulseGAN/image-20221208131055463.png" alt="image-20221208131055463" style="zoom: 67%;"></p>
<p>&emsp;&emsp;其他的实验测量了包括 AVNN，SDNN、IBI 等其他方法不测试的指标，因为没得比，因此主要是做离群值分析，或许是因为发在生物信息的期刊，对这方面的指标比较看重吧。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>SiNC</title>
    <url>/SiNC/</url>
    <content><![CDATA[<h2><center> Non-Contrastive Unsupervised Learning of Physiological Signals from Video </center></h2>

<p>【rPPG】【CVPR2023】【<a href="http://arxiv.org/abs/2303.07944">paper</a>】【<a href="https://github.com/CVRL/SiNC-rPPG">code</a>】</p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>&emsp;&emsp;本文基于 PPG 信号的三个特征，提出了一种非对比学习的自监督方法用于 rPPG 测量，虽然作者强调了没有针对 rPPG 进行独特的网络设计，但是三种特征均为 PPG 信号强相关的，并且这项工作实际上可以看做“基于负样本和先验约束的对比学习”。需要指出：作者认为在同 batch 内的 PSD 之和应当分布均匀，这是显然不成立的，即使考虑 batch size 可以很大并且视频经过数据增强，在统计学意义上 PSD 之和仍应该服从正态分布。</p>
<h3 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h3><p><img src="/SiNC/image-20231109105443684.png" alt="image-20231109105443684"></p>
<span id="more"></span>
<hr>
<h3 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h3><ul>
<li>“非对比学习”的无监督结构（事实上可以看做是对比学习的变体）</li>
<li>三个针对“频率任务”进行优化的损失函数：sparsely，bandlimits，variance</li>
</ul>
<h3 id="formulation"><a href="#formulation" class="headerlink" title="formulation"></a>formulation</h3><p>&emsp;&emsp;输入 video $x_i\in\mathbb{R}^{T\times W\times H\times C}$；数据增强的操作集合 $\Phi$；来自 SOTA（physnet）的模型 $f$；波形 $y_i=f(x_i)$；波形频率 PSD $F=\mathrm{FFT}(y)$；$F_i$ 表示 $F$ 在频率为 $i$ 的分量；正常人的 PPG 信号频率在 $a=0.66\mathrm{Hz}\rightarrow b=3\mathrm{Hz}$ 之间变化。</p>
<h3 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h3><p>&emsp;&emsp;本文的关键不在于网络而是损失函数的设计，在模型上选取了简单的 physnet。</p>
<ul>
<li>Sparsity Loss：此损失防止频率的带宽过分散</li>
</ul>
<script type="math/tex; mode=display">
L_s=\frac{\sum\limits_{i=a}^{\arg max(F)-\Delta F}F_i+\sum\limits_{i=\arg max(F)+\Delta F}^{b}F_i}{\sum\limits_{i=a}^{b}F_i}</script><p>&emsp;&emsp;其中，$\arg max(F)$ 即为心率，而 $\Delta F$ 表示 $\pm0.1\mathrm{Hz}$ 范围内的频率。也就是说，这个损失函数的意义为：在 $\arg max(F)\pm0.1\mathrm{Hz}$ 之外的频率图面积和总频率图面积之比，优化方向为：使 PSD 更集中在 $0.1\mathrm{Hz}$ 之内。</p>
<ul>
<li>Bandwidth Loss：此损失防止带宽离开心率频率范围之外</li>
</ul>
<script type="math/tex; mode=display">
L_b=\frac{\sum\limits_{i=-\infty}^{a}F_i+\sum\limits_{i=b}^{\infty}F_i}{\sum\limits_{i=-\infty}^{\infty}F_i}</script><p>&emsp;&emsp;这个损失函数的意义为：小于 $a$ 或大于 $b$ 的频率图面积和所有可能的频率面积之比。优化方向为：使 PSD 集中在 $[a,b]$ 之内（事实上就是 IPR 越小越好）</p>
<ul>
<li>Variance Loss：此损失防止模型 collapse</li>
</ul>
<p>&emsp;&emsp;在只有前面两个损失函数的情况下，仅仅能限制模型输出“正确”的信号而非“对应”的信号，<em>本文通过将同一个 batch 内输出的 PSD 方差增大期望模型学到正确信号</em>。</p>
<script type="math/tex; mode=display">
L_v=\frac1d\sum\limits_{i=1}^{d}(\mathrm{CDF}_i(Q)-\mathrm{CDF}_i(P))^2</script><p>&emsp;&emsp;其中 CDF 是累计分布函数，$\mathrm{CDF}_i$ 就表示 Q 分布在第 i 个频率的分量，这里 Q 和 P 都是 (1, d=140) 的 tensor。可以这样理解：Q 表示对一个 batch 内所有的 PSD 直接求和的结果，而 P 则是单纯的均匀分布。使 Q 靠近 P 即为：在同一个 batch 内，PSD 的峰值应当分散一些，从而让整个 batch 的 PSD 求和等于均匀分布。</p>
<h3 id="experiment"><a href="#experiment" class="headerlink" title="experiment"></a>experiment</h3><p>&emsp;&emsp;intra- 中规中矩，cross- 效果比较差，但是本文列出的其他 SOTA 实验结果也差，也就是说实际上本文重新跑了其他 SOTA 的跨数据集。</p>
<p><img src="/SiNC/image-20231109131002347.png" alt="image-20231109131002347"></p>
<p><img src="/SiNC/image-20231109131243482.png" alt="image-20231109131243482" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>Siamese-rPPG</title>
    <url>/Siamese-rPPG/</url>
    <content><![CDATA[<h2><center> Siamese-rPPG network: remote photoplethysmography signal estimation from face videos </center></h2>

<p>【心率监测】【SAC2020】【<a href="https://dl.acm.org/doi/10.1145/3341105.3373905">paper</a>】【<a href>code未开源</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种基于孪生网络的 rppg 估计网络，这种网络结构以一种十分简单的思路对人脸进行分割，通过简单的 3DCNN 取得了很好的结果，这篇文章当初觉得太简单没有看，但看数据居然和 Dual-GAN 差不多。这篇文章整体来说比较容易，但是一些处理能够引发思考：心率估计是否不太需要复杂的非线性计算能力？</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Siamese-rPPG/image-20221201152359197.png" alt="image-20221201152359197" style="zoom:50%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>使用孪生网络针对不同 ROI 进行提取，这应该是最早的对不同 ROI 能够提取同样心率的假设</li>
<li>假设同一个 clip 的 ROI 区域关键点很接近（在新的 VIPL 等数据集上似乎不太适用）</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p>&emsp;&emsp;对于同一个视频，首先将其裁剪成 clip $v$，这里在实现的时候选取的帧数较高，$v\in R^{600\times W\times H\times C}$，接着对于每一个 clip 的第一帧使用 dlib 库函数标定人脸关键点，并且通过该信息区分出额头区域和脸颊区域的大致 crop 范围，接下来对剩余视频的每一帧不再执行人脸关键点标定，而是直接使用第一帧的结果。这种方式是极其简单的偷懒方式，但也是有原因的，rppg 估计的网络大多没有大幅度的动作变化，并且简单裁剪之后就可以得到粗略的人脸对齐结果。</p>
<p>&emsp;&emsp;对于获得的额头区域和脸颊区域，分别取出 $v_{forehead},\ v_{check}$，将其输入一个孪生网络，该孪生网络具备两个相同的分支，具体来说每个分支的结构如下：</p>
<p><img src="/Siamese-rPPG/image-20221201152916471.png" alt="image-20221201152916471" style="zoom: 33%;"></p>
<p>&emsp;&emsp;通过网络之后将会分别得到两个预测结果，分别记为 $x_{forehead},\ x_{check}$，接下来是一个 late fusion，这里的 fusion 策略也很关键，本文最初尝试了 Conv+pooling 结构，如下图所示：</p>
<p><img src="/Siamese-rPPG/image-20221202102636322.png" alt="image-20221202102636322" style="zoom:50%;"></p>
<p>&emsp;&emsp;但是这样的结果很差，因此最终本文选择了直接使用两个信号的加法，即 $x=x_{forehead}+x_{check}$，这里又是一点，在某个程度上证明了对于 rppg 估计任务也许简单的策略才是好的。对于得到的结果，本文选择 NPCLoss，离谱的是居然还说明了一下为啥选择 NPC，或许是因为文章发得比较早？现在早已是固定的选择了。计算出 loss 之后进行梯度回传，这里是我比较疑惑的点，因为参数共享的网络梯度回传有不同的做法，不说清楚的话不知道是怎么做的，猜想应该是先锁住一个，回传并更新另一个的梯度，再锁住另一个并更新之前被锁住分支的参数，然后更新之前锁住的梯度。</p>
<p>&emsp;&emsp;接着为了评估网络效果，对 HR 的计算也很简单，不使用 PSD，而是直接用 fft，然后去除心率 &lt; 40 或者 &gt; 180 的，再在剩余的频率中取最大值，其实和 PSD 差不多，毕竟 PSD 也是根据 fft 来的，不过或许和现在的方法比起来稍显落后。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;本文的结果在 UBFC 和 PURE 上做了测试，这两个数据集都属于比较简单的，结果相对来说比较容易达到好的效果，本文这种用简单方法做出来的结果反而好得很，在更复杂的数据集上或许会变差一些。</p>
<p><img src="/Siamese-rPPG/image-20221202105220447.png" alt="image-20221202105220447" style="zoom:40%;"></p>
<p><img src="/Siamese-rPPG/image-20221202105209149.png" alt="image-20221202105209149" style="zoom:50%;"></p>
<p>&emsp;&emsp;同时还有跨数据集的测试，效果比较差一些，应该是简单的策略匹配简单的数据集和简单的任务吧，可以看到在 PURE 训练的结果在 UBFC 上迁移效果比较差，反过来则不是这样。这或许也是因为 UBFC 相对于 PURE 更加有利于模型的训练。</p>
<p><img src="/Siamese-rPPG/image-20221202105303143.png" alt="image-20221202105303143" style="zoom:67%;"></p>
<hr>
<h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3><p>&emsp;&emsp;本文总共采用了三个简单化处理的策略：1. 使用每个 clip 的第一帧标定 ROI，其余帧不再检测，2. 使用加法做 late fusion，而不是基于学习的方法，3. 使用 ftt 提取 HR，而不是用 PSD。有意思的是在这种设置下效果反而不错，有这么一种可能，虽然心率监测的任务看起来玄学一些，但是对于稀疏编码了生理信号的视频，也许提取 ppg 信号不太需要模型具备复杂的高维映射能力。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>SimPer</title>
    <url>/SimPER/</url>
    <content><![CDATA[<h2><center> SIMPER: SIMPLE SELF-SUPERVISED LEARNING OF PERIODIC TARGETS </center></h2>

<p>【周期性信号对比学习】【arxiv】【<a href="https://arxiv.org/pdf/2210.03115.pdf">paper</a>】【<a href="https://github.com/YyzHarry/SimPer">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文探索了旧有对比学习方法在周期性任务学习中的局限性，提出了一种新的周期性对比学习的代理任务，并且改进了 InfoNCE，测试了六个周期性变化的对比学习任务，和 MoCo，SimCLR，BYOL 等对比学习方法相比取得了较大的进步。（其实是个实验详实的水文，对比的这些 SOTA 都是学 ID 或者 ACTION 语义的，和 HR 等目标完全不一样）</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/SimPER/image-20221104103618517.png" alt="image-20221104103618517" style="zoom:80%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>提出周期性任务对比学习的内在特性</li>
<li>提出了一种周期性任务对比学习的框架 SimPer</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/SimPER/image-20221104105331714.png" alt="image-20221104105331714" style="zoom:67%;"></p>
<p>&emsp;&emsp;对于网络结构，参考 <a href="https://bnucsy.github.io/HR_CL_the_way_to_my_heart/">The Way to My Heart</a> 是一目了然的，也就是提了一些更加准确性的定义：</p>
<ul>
<li>$\tau$：周期改变的数据增强</li>
<li>$\sigma$：周期不变的数据增强</li>
</ul>
<p>&emsp;&emsp;因此对于输入 $x$ 首先进行周期改变的数据增强，得到 $\tau_i(x),i\in [1,k]$，对于得到的 $k$ 个视频，将其逐个进行周期不变的增强，每个视频进行两次增强作为 pos 对，得到 $2k$ 个样本 $\sigma_i(\tau_j(x)),i\in [1,2],j\in[1,k]$。对于所有的样本通过 backbone $f$，$\tau $ 下标一致的接近，不一致的远离。</p>
<p>&emsp;&emsp;其中 backbone 针对不同的六个任务选取不同，RotatingDigits 和 SCAMPS 上，采用了 CNN 的简单 3D 变体。采用 TS-CAN 模型的变体对 UBFC 和 PURE 进行实验。最后，在 Countix 和 LST 上，使用ResNet-3D-18。</p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>&emsp;&emsp;对于损失函数，为了度量周期性信号的相似度，不可以使用普通的 cos 方式测量，而是需要能够测量周期的损失，绝对误差和周期误差的区别如下图所示。</p>
<p><img src="/SimPER/image-20221104115127418.png" alt="image-20221104115127418" style="zoom:80%;"></p>
<p>&emsp;&emsp;因此本文尝试了 MXCorr（最大互相关）和 nPSD（归一化功率谱密度）对周期性信号进行建模，并且提出了一种基于 InfoNCE 的广义 InfoNCE。</p>
<ul>
<li>InfoNCE</li>
</ul>
<script type="math/tex; mode=display">
\rm \mathcal L_{InfoNCE}=-log\frac{exp(sim(z,\hat z)/\nu)}{\sum_{z'\in \mathcal Z/\{z\}}exp(sim(z,z')/\nu)}</script><ul>
<li>Generalized InfoNCE</li>
</ul>
<script type="math/tex; mode=display">
\rm\mathcal L_{SimPer } = \mathop\sum\limits_i \mathcal l_{SimPer}^i</script><script type="math/tex; mode=display">
\rm \mathcal l_{SimPer}^i=\mathop\sum\limits_i-\mathop\sum\limits_{j=1}^M\frac{exp(w_{i,j})}{\sum_{k=1}^Mexp(w_{i,k})}log\frac{exp(sim(z_i,\hat z_j)/\nu)}{\sum_{k=1}^Mexp(sim(z_i,z_k')/\nu)}</script><p>&emsp;&emsp;综合来看，其本身没有和 InfoNCE 的太大区别，区别在于对于高频采样的信号，GInfoNCE 可以将其视为连续信号计算误差，这样在实验中可以得到更好的优化。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;结果相对于 SIMCLR、MoCo，BYOL，CVRL 等都好很多，但是和正儿八经做周期性对比学习的方法差很多，以心率检测为例：其中 FFT 和 1-NN 是分别以傅立叶变换和最近邻分类器直接评估学习到的特征。虽然这样做由于没有进行特殊设计相比于 The Way to My Heart 略有劣势，但结果也差了不少，和 physnet 都差很多，更不用提 physformer 和 Dual-GAN 了。</p>
<p><img src="/SimPER/image-20221104133719823.png" alt="image-20221104133719823"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>TATS</title>
    <url>/TATS/</url>
    <content><![CDATA[<h2><center> Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer </center></h2>

<p>【长视频生成】【ECCV2022】【<a href="http://arxiv.org/abs/2204.03638">paper</a>】【<a href="https://songweige.github.io/projects/tats">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文提出了一种遵循 VQGAN+transformer 架构的长视频生成方法，通过对以往的方法在自回归生成长视频的探索，本文提出了两个见解：1.3D-VQGAN 在自编码时为了获得对应 shape 而进行的时间维度上的 zero padding 是长视频生成崩溃的原因，2.在一定的范围内（如生成 1k 帧），直接用自回归进行视频生成会产生很长的时间注意力（因为计算了 $p(z_{1000}|z_{1:999})$），这也可能是导致崩溃的原因。据此本文提出了一些改动，形成了对时间不敏感（<strong>T</strong>ime-<strong>A</strong>gnostic）的 VQGAN，和对时间敏感 （<strong>T</strong>ime-<strong>S</strong>ensitive）的 transformer。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/TATS/image-20230419101038411.png" alt="image-20230419101038411"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>发现 VQGAN 在时间维度上的 zero padding 是长视频生成的崩溃原因，使用复制进行 padding</li>
<li>提出了一种分层预测的 transformer，首先预测关键帧，再根据关键帧进行插值预测</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/TATS/image-20230419102056925.png" alt="image-20230419102056925"></p>
<p>&emsp;&emsp;本文的网络结构包含两个方面，一是可以在固定窗口大小下进行视频自编码的 3D-VQGAN，然后是基于 VQVAE 采样的文本或音频（也可以是条件视频生成的条件）进行引导生成。</p>
<p>&emsp;&emsp;对于 3D-VQGAN，其网络的设计简单地将标准 VQGAN 的 2D 卷积进行 3D 替换。但是在 padding 时采用复制的方法进行，具体来说在时间维度的 padding 可以从以下三个角度考虑，并且最终选定复制填充：</p>
<p><img src="/TATS/image-20230419105819340.png" alt="image-20230419105819340"></p>
<ul>
<li>zero padding，在时间卷积时使用 0 填充帧以获得对应的 shape，这样会导致长视频崩溃</li>
<li>使用真实帧填充，$(b)$ 中的 -1,-2 即倒数第 1,2 帧，也就是将视频看做循环播放，这样可以解决崩溃问题，但是在降采样率很大的时候会出现较大的复杂度</li>
<li>使用复制帧填充，即将视频开始帧之前和结束帧之后当成视频被“冻结”了，实现简单，并且效果与 $(b)$ 基本一致</li>
<li>🔥由于过短的视频可能要进行大量的 padding，因此在训练时将短视频直接剔除</li>
</ul>
<p>&emsp;&emsp;在确定网络结构之后，本文的损失函数创新性属于有但不多，具体来说包括：</p>
<ul>
<li>用于重建的 VQVAE loss：<img src="/TATS/image-20230419111332708.png" alt="image-20230419111332708" style="zoom: 40%;"><ul>
<li>在更新 $L_{codebook}$ 时采用 EMA</li>
</ul>
</li>
<li>对抗性损失 $L_{disc}$，其描述为：<ul>
<li>对于空间维度的判别器，度量每一帧的生成质量 $f_{D_s}$，对于时间维度的判别器，度量运动有效性的 $f_{D_t}$，真实 token $x$，生成的伪 token $\hat x$</li>
<li>$L_{disc}=\log f_{D_{s/t}}(x)+\log (1-f_{D_{s/t}}(\hat x))$</li>
</ul>
</li>
<li>特征匹配损失 $L_{match}$，其主要包含感知损失和判别器的每一层加权损失，其描述为：<ul>
<li><img src="/TATS/image-20230419111416986.png" alt="image-20230419111416986" style="zoom:40%;"></li>
<li>其中 $s/t/VGG$ 表示 $VGG$ 时为 L1 感知损失，$s/t$ 时才有对应的 $p_i$，$i$ 表示 $f_{D_{s/t}}$ 的第 $i$ 层的值，也就是参考 VGG 损失写的判别器在每一层的特征匹配，其中 $p_i$ 是常数</li>
</ul>
</li>
<li>最终优化目标结合上述三个损失，具体来说：<ul>
<li><img src="/TATS/image-20230419111728354.png" alt="image-20230419111728354" style="zoom:40%;"></li>
<li>其中所有的 $\lambda,\ \beta$ 都是超参数</li>
</ul>
</li>
</ul>
<p>&emsp;&emsp;对于第二阶段的 transformer，其每个 transformer block 包含两个部分：1.自回归生成的下一帧隐编码，2.插帧生成的中间帧隐编码<strong>（在没开源的情况下，这种描述是不精确的，具体来说并不确定：1.自回归生成稀疏编码，插帧生成中间编码，这个更合理；2.自回归生成所有编码，插帧细化中间编码，这个更符合网络图）</strong>。</p>
<p>&emsp;&emsp;对于第一个 block，其作用就是自回归进行下一个的生成，其优化目标为 $E_{z\sim p(z)}[-\log p(z_i|z_{0:i})]$。</p>
<p>&emsp;&emsp;对于第二个 block，相对于普通的 casual attn，插帧下的 attn 方式略有不同，本文做了如下三种尝试，最终选择了 $(c)$：</p>
<p><img src="/TATS/image-20230419112631114.png" alt="image-20230419112631114"></p>
<ul>
<li>$(a)$ 是原始的 causal attn，这里的每一帧都能看到前面所有帧的隐编码</li>
<li>$(b)$ 是一种变体，这里的每一帧可以看到前面所有帧的隐编码，同时包括插帧边界的隐编码，但是边界帧可以看到中间生成了哪些帧（这样会导致中间帧也跟随全部隐编码进行更新，就淡化了 causal</li>
<li>$(c)$ 是另一种变体，这里的每一帧（非边界）可以看到前面所有帧和边界帧，边界帧只能看到边界帧</li>
</ul>
<p>&emsp;&emsp;通过这种分层设计的模型，本文通过实验验证提升了模型的长期预测能力，能够保持视频从头到尾的一致主题。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><ul>
<li>在各个数据集上的 FVD，其中 KVD 来自：<a href="https://arxiv.org/abs/1812.01717">Towards accurate generative models of video: A new metric &amp; challenges</a></li>
</ul>
<p><img src="/TATS/image-20230419113145705.png" alt="image-20230419113145705" style="zoom:50%;"></p>
<ul>
<li>长视频生成，整体来说分辨率并不高，并且伪影也很多，但是可以看出整个视频是符合某一个主题的</li>
</ul>
<p><img src="/TATS/image-20230419113436249.png" alt="image-20230419113436249"></p>
<p>&emsp;&emsp;更多的结果（包括长视频游戏动画生成）见：<a href="https://songweige.github.io/projects/tats/">https://songweige.github.io/projects/tats/</a></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>Two_stream_CNN</title>
    <url>/Two_stream_CNN/</url>
    <content><![CDATA[<h2 id="Two-stream-convolutional-networks-for-action-recognition-in-videos-【视频动作识别】-NIPS2014"><a href="#Two-stream-convolutional-networks-for-action-recognition-in-videos-【视频动作识别】-NIPS2014" class="headerlink" title="Two-stream convolutional networks for action recognition in videos 【视频动作识别】 NIPS2014"></a><a href="https://proceedings.neurips.cc/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf">Two-stream convolutional networks for action recognition in videos</a> 【视频动作识别】 NIPS2014</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;双流网络是卷积神经网络在视频理解领域的第一个应用，在这之前的视频理解都是基于手工制作的特征，正是从这项工作开始，视频理解领域步入 CNN 时代。虽然早就有人尝试使用 CNN 做视频，但是效果很差，主要原因是 CNN 难以理解视频帧之间的时间变化。双流网络克服了这个问题，提出了使用光流信息代替时间变化的思路，将空间帧和光流帧按顺序输入两个不同的 CNN 网络，将得到的输出通过一个 SVM 分类器进行分类。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Two_stream_CNN/image-20221008092010923.png" alt="image-20221008092010923"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>使用双向光流图代表时间信息</li>
<li>对光流图的叠加方式进行了探究</li>
<li>使用了 JPEG 的方式存储光流图</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p>&emsp;&emsp;本文的网络正如概览，大致是个 AlexNet 变体，对于输入的 video，单帧输入空间流网络，同时抽取光流之后堆叠输入时间流网络，得到的结果通过 SVM 进行分类，两个网络联合训练，是经典且简单的 CNN。</p>
<h4 id="光流"><a href="#光流" class="headerlink" title="光流"></a>光流</h4><p><img src="/Two_stream_CNN/image-20221008094036062.png" alt="image-20221008094036062"></p>
<p>&emsp;&emsp;光流顾名思义表示了光的流动，在图像每一帧之间抽取光流，对于 (a)，(b) 两帧抽取光流，将会得到 (c)，(c) 是一个二维表示，每个点有一个梯度，梯度的方向和大小分别代表了点移动的方向和距离，二维表示中的二维分别是水平方向和垂直方向的变化。如果将水平方向和垂直方向的单一维度进行灰度图可视化，将会得到 (d)，(e)。</p>
<p>&emsp;&emsp;光流是一种直接性的时间变化信息，因此本文使用这种数据的堆叠输入 CNN，帮助 CNN 进行了难以理解的特征的抽取，得出了较好的结果。但是光流存在一些缺点，1.计算较为困难，2.密集表示的光流需要的空间太大。为了解决这些问题，本文提出了一种基于 GPU 方式计算光流的方法，平均计算一张光流图需要 0.06s，同时对于一张 HxWx2 的光流图，本文采用 JPEG 的方式压缩图像，在需要使用的时候再解压缩。这种方式能够在一定程度上解决前两个问题，但是以目前的眼光来看，对于大型数据集，光流预处理时间需要按周/月来计算，存储空间也是 TB 级别。更离谱的是，时至今日，仍然没有人提出更好的处理和存储方式，基本上光流已经被废弃了。</p>
<h4 id="光流叠加"><a href="#光流叠加" class="headerlink" title="光流叠加"></a>光流叠加</h4><p>&emsp;&emsp;在光流抽取之后，作者认为将光流逐帧输入 CNN 在一定程度上丧失了其时间上的语义信息，因此作者选取连续的 10 个帧，将其光流进行叠加。这里作者探讨了两种叠加方法：</p>
<p><img src="/Two_stream_CNN/image-20221008100702857.png" alt="image-20221008100702857"></p>
<p>&emsp;&emsp;图左表示直接进行叠加，也就是单纯地将计算得到的光流按帧排列，然后输入给网络，图右是一种更加利用光流位移信息的叠加方式，对于后一帧的图像，其每个点的梯度继承前一帧同一个点的梯度，两个梯度进行叠加，用于表示当前状态相对于初始状态下的位移情况。看上去第二种叠加更加有效，但实际上单纯的排列会得到更好的效果，作者表示了疑惑但没有解释原因。我猜这是因为对于 CNN 来说，从原始叠加学会梯度移动的叠加并不困难，但是返回来就会很难学习。当然，这也是结果导向的猜测，原始堆叠更好我也能编个理由出来。</p>
<h4 id="数据组织方式"><a href="#数据组织方式" class="headerlink" title="数据组织方式"></a>数据组织方式</h4><p>&emsp;&emsp;本文的输入数据采样固定数量的帧 ( 25 个) ,它们之间的时间间隔相等。从每一帧中，通过裁剪和翻转帧的四个角和中心，获得 10 个 ConvNet 输入，因此对每一组输入共有 250 维。对于光流图像一致。</p>
<p>&emsp;&emsp;同时，本文采用了双向光流，也即同时对这个视频倒着计算光流，分别进行输入和计算，和 bi-LSTM 一致。这种方式是稳定的涨点方法，基本等同于平时的金字塔操作。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>video</category>
      </categories>
  </entry>
  <entry>
    <title>AN IMAGE IS WORTH 16X16 WORDS</title>
    <url>/VIT/</url>
    <content><![CDATA[<h2 id="AN-IMAGE-IS-WORTH-16X16-WORDS-【图像分类】-ICLR"><a href="#AN-IMAGE-IS-WORTH-16X16-WORDS-【图像分类】-ICLR" class="headerlink" title="AN IMAGE IS WORTH 16X16 WORDS  【图像分类】 ICLR"></a><a href="https://arxiv.org/abs/2010.11929">AN IMAGE IS WORTH 16X16 WORDS</a>  【图像分类】 ICLR</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;VIT(vision transformer) 是第一个将NLP领域的颠覆性成果——transformer成功迁移到视觉领域的工作。VIT采用了非常简单的操作使图像可以像文字序列一样输入transformer架构之中。正如题目所说，VIT将图像分为许多16x16的patch，并将这些patch视为句子中的word，将图像视为句子，几乎完全使用transformer架构完成了对CNN的超越。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/VIT/image-20220925105818246.png" alt="image-20220925105818246"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>将图像打成patch，几乎使用标准transformer进行处理</li>
<li>借鉴bert，采用cls作为标志位进行分类</li>
</ul>
<h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><p>&emsp;&emsp;整体来说，该工作与普通的transformer差别不大，基本过程几乎没有差别，在这里的分析过程中不讲理由，只按照论文的前向过程走一遍，看一下VIT中的各个参数维度变化。</p>
<h4 id="patch-embedding"><a href="#patch-embedding" class="headerlink" title="patch embedding"></a>patch embedding</h4><p>&emsp;&emsp;初始输入的图像 image 具有 $H\times W\times C$ 的 shape，首先我们需要对每个 image 按照 $P\times P$ 切分成多个 patch，理论上共有 $N = \frac{HW}{P^2}$ 个patch。以 VIT-base 为例，$H = W = 256, P = 32$, 切分之后有8*8=64个patch， 每个patch经过拉平之后大小为32*32=1024维，此时输入序列为 $(N, P^2\times C)$，在这之后，为了对准标准 transformer，使用一个Linear层完成从 $P^2\times C \rightarrow D$ 的维度转化。因此输入的序列维度为 $(N,D)$。VIT-base 中 $D=1024$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">assert</span> image_size % patch_size == <span class="number">0</span>, <span class="string">&#x27;Image dimensions must be divisible by the patch size.&#x27;</span>  <span class="comment"># 保证一定能够完整切块</span></span><br><span class="line">num_patches = (image_size // patch_size) ** <span class="number">2</span>  <span class="comment"># 获取图像切块的个数</span></span><br><span class="line">patch_dim = channels * patch_size ** <span class="number">2</span>  <span class="comment"># 线性变换时的输入大小，即每一个图像宽、高、通道的乘积</span></span><br><span class="line"><span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;<span class="string">&#x27;cls&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>&#125;, <span class="string">&#x27;pool type must be either cls (cls token) or mean (mean pooling)&#x27;</span>  <span class="comment"># 池化方法必须为cls或者mean</span></span><br><span class="line"></span><br><span class="line">self.to_patch_embedding = nn.Sequential(</span><br><span class="line">    Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_size, p2 = patch_size),  <span class="comment"># 将批量为b通道为c高为h*p1宽为w*p2的图像转化为批量为b个数为h*w维度为p1*p2*c的图像块</span></span><br><span class="line">    <span class="comment"># 即，把b张c通道的图像分割成b*（h*w）张大小为P1*p2*c的图像块</span></span><br><span class="line">    <span class="comment"># 例如：patch_size为16  (8, 3, 48, 48)-&gt;(8, 9, 768)</span></span><br><span class="line">    nn.Linear(patch_dim, dim),  <span class="comment"># 对分割好的图像块进行线性处理（全连接），输入维度为每一个小块的所有像素个数，输出为dim（函数传入的参数）</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h4><p>&emsp;&emsp;VIT的位置编码不像普通transformer的编码直接指定，而是通过学习进行编码，VIT将会初始化一个size和输入序列一致的位置编码 $(N,D)$，然后和序列每个位置的编码直接相加，通过梯度下降学习。事实上，通过这种策略学习到的编码在变回2D之后，其大小分布基本表示了每个patch在原图像中的位置，如下图。</p>
<p><img src="/VIT/image-20220925151026352.png" alt="image-20220925151026352" style="zoom:33%;"></p>
<p>&emsp;&emsp;在加入位置编码的同时，VIT为了实现分类任务进行了对bert的借鉴，也就是在整个序列的首位加入了一个CLS标志位，直接使用CLS标志位的输出过一个MLP后进行图像分类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))  <span class="comment"># 位置编码，获取一组正态分布的数据用于训练</span></span><br><span class="line">self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))  <span class="comment"># 分类令牌，可训练</span></span><br></pre></td></tr></table></figure>
<hr>
<h4 id="transformer-encoder"><a href="#transformer-encoder" class="headerlink" title="transformer encoder"></a>transformer encoder</h4><p>&emsp;&emsp;这里的计算过程基本和transformer没有区别，详见<a href="https://bnucsy.github.io/Attention%20is%20all%20you%20need/">标准transformer</a>，这里主要说一些不同点，在VIT中，首先要进行LayerNorm再进行MHA，虽然事实上transformer在pytorch中的接口中也提供了这种先做norm的选项。这里没有 $A\&amp;N$ 部分，非要说的话可以说成 $N\&amp;A$，具体地，这 $L$ 次encoder中的变化过程大致是：</p>
<script type="math/tex; mode=display">
\rm A^i \rightarrow Z,Z=A + MHA(LayerNorm(A))\rightarrow A^{i+1},A^{i+1}=Z+MLP(LayerNorm(Z))</script><p>&emsp;&emsp;这里的 MLP 约等于标准 transformer 中的 FFN，都属于先放大再缩小的全连接层，VIT-base 中的隐藏层维度为 2048，具体到 transformer 中的内部参数，$W_{Q/K/V}\rightarrow (1024,64), headers=6$。另有一处区别于标准transformer，在 MLP 中，其过程中的 RELU 被替换成了 GELU，并且在多处加入了dropout，由于实在有很多地方都加入了dropout，具体哪些位置这里不做记录。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)  <span class="comment"># 正则化</span></span><br><span class="line">        self.fn = fn  <span class="comment"># 具体的操作</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fn(self.norm(x), **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="comment"># attention</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads = <span class="number">8</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head *  heads  <span class="comment"># 计算最终进行全连接操作时输入神经元的个数</span></span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dim_head == dim)  <span class="comment"># 多头注意力并且输入和输出维度相同时为True</span></span><br><span class="line"></span><br><span class="line">        self.heads = heads  <span class="comment"># 多头注意力中“头”的个数</span></span><br><span class="line">        self.scale = dim_head ** -<span class="number">0.5</span>  <span class="comment"># 缩放操作，论文 Attention is all you need 中有介绍</span></span><br><span class="line"></span><br><span class="line">        self.attend = nn.Softmax(dim = -<span class="number">1</span>)  <span class="comment"># 初始化一个Softmax操作</span></span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias = <span class="literal">False</span>)  <span class="comment"># 对Q、K、V三组向量先进性线性操作</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 线性全连接，如果不是多头或者输入输出维度不相等，进行空操作</span></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, n, _, h = *x.shape, self.heads  <span class="comment"># 获得输入x的维度和多头注意力的“头”数</span></span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim = -<span class="number">1</span>)  <span class="comment"># 先对Q、K、V进行线性操作，然后chunk乘三三份</span></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h = h), qkv)  <span class="comment"># 整理维度，获得Q、K、V</span></span><br><span class="line"></span><br><span class="line">        dots = einsum(<span class="string">&#x27;b h i d, b h j d -&gt; b h i j&#x27;</span>, q, k) * self.scale  <span class="comment"># Q, K 向量先做点乘，来计算相关性，然后除以缩放因子</span></span><br><span class="line"></span><br><span class="line">        attn = self.attend(dots)  <span class="comment"># 做Softmax运算</span></span><br><span class="line"></span><br><span class="line">        out = einsum(<span class="string">&#x27;b h i j, b h j d -&gt; b h i d&#x27;</span>, attn, v)  <span class="comment"># Softmax运算结果与Value向量相乘，得到最终结果</span></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)  <span class="comment"># 重新整理维度</span></span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)  <span class="comment"># 做线性的全连接操作或者空操作（空操作直接输出out）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([])  <span class="comment"># Transformer包含多个编码器的叠加</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            <span class="comment"># 编码器包含两大块：自注意力模块和前向传播模块</span></span><br><span class="line">            self.layers.append(nn.ModuleList([</span><br><span class="line">                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),  <span class="comment"># 多头自注意力模块</span></span><br><span class="line">                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))  <span class="comment"># 前向传播模块</span></span><br><span class="line">            ]))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># 自注意力模块和前向传播模块都使用了残差的模式</span></span><br><span class="line">            x = attn(x) + x</span><br><span class="line">            x = ff(x) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="MLP-header"><a href="#MLP-header" class="headerlink" title="MLP header"></a>MLP header</h4><p>&emsp;&emsp;最后的MLP就是一个普通的MLP，$\rm layernorm +Linear$就无了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.mlp_head = nn.Sequential(</span><br><span class="line">    nn.LayerNorm(dim),  <span class="comment"># 正则化</span></span><br><span class="line">    nn.Linear(dim, num_classes)  <span class="comment"># 线性输出</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>&emsp;&emsp;由于是分类问题，VIT使用最终输出的结果和真实label进行计算交叉熵损失。其SOTA的性能由JFT的大规模数据集预训练保证。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>Video-based_rppg_SSL</title>
    <url>/Video-based_rppg_SSL/</url>
    <content><![CDATA[<h2><center> Video-based Remote Physiological Measurement via Self-supervised Learning </center></h2>

<p>【心率监测】【arxiv2022】【<a href="http://arxiv.org/abs/2210.15401">paper</a>】【<a href="https://github.com/yuezijie/Video-based-Remote-Physiological-Measurement-via-Self-supervised-Learning">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文是基于对比学习进行 rppg 信号估计的又一 SOTA，在 UBFC、PURE、DEAP、MMVS 上展示了最好的性能。鉴于目前的 rppg 对比学习网络结构基本都基于孪生网络，在数据增强和代理任务上进行创新，本文主要采样的方法有：正样本：来自空间增强的 anchor $p_1$ 和 anchor 在同一个 video 的邻居 $p_2$，负样本：来自 LFA 模块 $n$，该模块可以在视觉不变的基础上进行频率重采样，代理任务：对比 $p_1$ 和 $p_2$ 拉进距离，对比 $p_1$ 和 $n$ 拉远距离，backbone：基于混合专家模型的 ResNet3D。并且对于多样的样本提出了多样的损失。</p>
<p>&emsp;&emsp;整体来说起作用的我认为最可能是 LFA，该模块可以生成大量的负样本，或许负样本的数量是 CL 的关键。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/Video-based_rppg_SSL/image-20221202123712050.png" alt="image-20221202123712050" style="zoom:80%;"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>一种新的正样本获取策略：时间上的邻居</li>
<li>一种新的负样本获取策略：基于学习的 LFA 模块，用于生成不同频率但人物一致的视频</li>
<li>一种基于混合专家模型的 backbone</li>
<li>对正样本间、正负样本间的不同损失函数 CFAL、FCL、FRCL</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/Video-based_rppg_SSL/image-20221202123712050.png" alt="image-20221202123712050" style="zoom:80%;"></p>
<h4 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h4><ul>
<li>对于输入的视频 $X$，首先将其裁剪为多个 clip，每个 $T$ 帧，然后随机选择其中一个 clip 作为主 clip $X^a$，而其他的 clip 作为邻居 $X^b$，此时的 $X^b$ 也是正样本的一员<ul>
<li>在实现的过程中，每个输入产生三个邻居，这些正样本之间互相不重合帧，记为 $X^b_1,\ X^b_2,\ X^b_3$</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_img</span>(<span class="params">filepath, scale, video_length</span>):</span><br><span class="line">    <span class="built_in">list</span> = os.listdir(filepath)</span><br><span class="line">    imglen = <span class="built_in">len</span>(<span class="built_in">list</span>)</span><br><span class="line">    begin = random.randrange(<span class="number">0</span>, <span class="built_in">int</span>(imglen-(video_length*<span class="number">4</span>)))</span><br><span class="line">    whole_video = []</span><br><span class="line">    count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(begin, begin + (video_length*<span class="number">4</span>)):</span><br><span class="line"></span><br><span class="line">        img = cv2.imread(os.path.join(filepath, <span class="string">&#x27;face_&#123;0&#125;.png&#x27;</span>.<span class="built_in">format</span>(i)))</span><br><span class="line">        img = cv2.resize(img, (scale, scale))</span><br><span class="line"></span><br><span class="line">        whole_video.append(img)</span><br><span class="line">    whole_video = np.array(whole_video)</span><br><span class="line"></span><br><span class="line">    whole_video = whole_video.reshape(</span><br><span class="line">        (<span class="number">4</span>, video_length, whole_video.shape[<span class="number">1</span>], whole_video.shape[<span class="number">2</span>], whole_video.shape[<span class="number">3</span>]))</span><br><span class="line">    a = random.choice(count)</span><br><span class="line">    <span class="built_in">input</span> = whole_video[a]</span><br><span class="line">    count.remove(a)</span><br><span class="line">    neighbor1 = whole_video[count[<span class="number">0</span>]]</span><br><span class="line">    neighbor2 = whole_video[count[<span class="number">1</span>]]</span><br><span class="line">    neighbor3 = whole_video[count[<span class="number">2</span>]]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">input</span>, neighbor1, neighbor2, neighbor3</span><br></pre></td></tr></table></figure>
<ul>
<li>同时输入的还有一系列的频率 $R$，这个频率数组控制了期望 LFA 模块后的 video 与原 clip 的频率比，$R$ 中的值随机在 $(0.3,0.8)\cup(1.2,1.7)$ 之中产生</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">frequency_ratio</span>():</span><br><span class="line">  ratio_interval1 = np.random.uniform(<span class="number">0.3</span>, <span class="number">0.8</span>)</span><br><span class="line">  ratio_interval2 = np.random.uniform(<span class="number">1.2</span>, <span class="number">1.7</span>)</span><br><span class="line">  full_interval = np.stack((ratio_interval1, ratio_interval2))</span><br><span class="line">  random_ratio = np.random.choice(full_interval)</span><br><span class="line">  <span class="keyword">return</span> random_ratio</span><br></pre></td></tr></table></figure>
<ul>
<li>对于 $X^a$，执行以下两种增强策略：<ul>
<li>将 $X^a$ 和 $R$ 输入 LFA 模块，得到负样本 $X^n$，这应当是本文的关键，即大量的有效负样本<ul>
<li>重建之后为了保证视觉一致性，需要通过重建损失 $L_{rec}=\mathrm {MSE}(X^a,X^n)$</li>
</ul>
</li>
<li>将 $X^a$ 进行空间上的数据增强，具体来说就只包括旋转和反转，得到正样本 $X^p$<ul>
<li>在实现的过程中，每个 $X^a$ 生成两个正样本，记为 $X^p_1,\ X^p_2$</li>
</ul>
</li>
</ul>
</li>
<li>对于之前得到的所有正负样本：$X^b,\ X^p,\ X^n$，将他们全部通过 rppg 估计网络 REA 模块，分别得到输出 $Y^b,\ Y^p,\ Y^n$</li>
<li>为了拉进正样本之间的距离以及拉远正负样本之间的距离，同时限制 LFA 的学习，需要过以下 3 个损失<ul>
<li>用于拉进同视频增强后的正样本距离，同时与负样本拉远距离，需要通过 $L_{FCL}=f_1(Y^p_1,\ Y^p_2,\ Y^n)$<ul>
<li>有一说一，其他的两个 loss 算是新的，这个 loss 最多算是重写了一下 InfoNCE</li>
</ul>
</li>
<li>用于拉进增强后正样本与邻域正样本的距离，需要通过 $L_{CFAL}=f_2(Y^p_1,\ Y^p_2,\ Y^b_1,\ Y^b_2,\ Y^b_3)$</li>
<li>用于限制正负样本对之间的信号比与 $R$ 一致，也就是为了 LFA 的学习，需要通过 $L_{FRCL}=f_3(R,\ Y^p_1,\ Y^p_2,\ Y^n)$</li>
</ul>
</li>
<li>将所有 loss 求和之后进行梯度回传，loss 之间不设权重调整项</li>
</ul>
<h4 id="LFA-模块"><a href="#LFA-模块" class="headerlink" title="LFA 模块"></a>LFA 模块</h4><p><img src="/Video-based_rppg_SSL/image-20221202124426153.png" alt="image-20221202124426153" style="zoom:50%;"></p>
<p>&emsp;&emsp;LFA 模块是生成负样本的关键，其正向过程如下：</p>
<ul>
<li>对于输入的 $X^a$，首先经过 3D 卷积和 Res3D，得到 $s^a_1$</li>
<li>再将 $s^a_1$ 进行降采样两次分别得到 $s^a_2,\ s^a_3$，所有的 $s_i^a$ 和从 $R$ 中 sample 出的 $r_i$ 共同输入 FMB 模块<ul>
<li>对于每个 FMB 模块，首先将 $s^a$ 进行全局池化和卷积得到 $z$，然后将 $r_i$ 填充至和 $z$ 等长（其实输入的时候就是等长）</li>
<li>然后 cat $r_i$ 和 $z$，并经过一个 Res 块和 LSTM，输出之后和之前的输入 $s^a$ 做按位乘法后输出</li>
<li>FMB 的伪代码如下：</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FMB</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, s_a, ratio, video_length</span>):</span><br><span class="line">        <span class="built_in">super</span>(FMB, self).__init__()</span><br><span class="line">        self.mod_gap = nn.AdaptiveAvgPool3d((video_length, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.mod_conv1d = nn.Conv1d(</span><br><span class="line">            in_channels=base_filter, out_channels=<span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.mod_res1d = ResnetBlock1D(</span><br><span class="line">            <span class="number">2</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>, activation=<span class="string">&#x27;relu&#x27;</span>, norm=<span class="literal">None</span>)</span><br><span class="line">        self.mod_lstm = nn.LSTM(<span class="number">2</span>, <span class="number">1</span>, num_layers=<span class="number">1</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, ratio</span>):</span><br><span class="line">        s_a = self.mod_gap(s_a)</span><br><span class="line">        s_a = s_a.squeeze(<span class="number">3</span>)</span><br><span class="line">        s_a = s_a.squeeze(<span class="number">3</span>)</span><br><span class="line">        z = self.mod_conv1d(s_a)</span><br><span class="line">        r_i = ratio.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        fuse = torch.cat((z, r_i), <span class="number">1</span>)</span><br><span class="line">        m_i = self.mod_res1d(fuse)</span><br><span class="line">        m_i = m_i.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        m_i = self.mod_lstm(m_i)[<span class="number">0</span>]</span><br><span class="line">        m_i = m_i.view(B, <span class="number">1</span>, T, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        s_n = torch.mul(s_a, m_i)</span><br><span class="line">        <span class="keyword">return</span> s_n</span><br></pre></td></tr></table></figure>
<ul>
<li>对于三个 FMB 模块的输出分别按照降采样的程度进行上采样，整体上算是个类 U-net 的结构</li>
</ul>
<h4 id="REA-模块"><a href="#REA-模块" class="headerlink" title="REA 模块"></a>REA 模块</h4><p><img src="/Video-based_rppg_SSL/image-20221202124505577.png" alt="image-20221202124505577" style="zoom:80%;"></p>
<p>&emsp;&emsp;REA 模块是一个基于混合专家模型的 rppg 估计网络，所谓混合专家模型（MOE），本质上像是集成学习一样的策略，即对不同的子问题有不同的网络（专家模型）进行拟合，最终给各个网络（专家模型）赋值权重得到输出。</p>
<p>&emsp;&emsp;对于 REA 模块，其将输入的视频经过一个 resnet 之后进行分割，将一个 clip 按照空间区域划分为 L 个子空间（L=4），每个空间经过同样结构但不共享参数的局部专家模型得到对应空间的输出，并且给每个专家模型赋值不同的权重，这些权重来自于另一个模型 $\cal G$，整个模型的前向过程由上图给出，蛮清晰的，实现也比较简单。</p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>&emsp;&emsp;上述前向过程之中总共出现了四个损失函数 $\cal L_{vr},\ L_{fc},\ L_{fr},\ L_{fa}$，分别对应：Video reconstruction loss，Frequency contrastive loss，Frequency ratio consistency loss，Cross-video frequency agreement loss，他们的表达式分别如下：</p>
<script type="math/tex; mode=display">
\begin{align}
&\rm \mathcal L_{vr}=MSE(X^n, X^a)\tag 1\\
&\rm \mathcal L_{fc}=\log (1+\frac{\exp(d(y_1^p,y_2^p)/\tau)}{\sum_{i=1}^k(\exp(d(y_1^p,y_i^n)/\tau)+\exp(d(y_2^p,y_i^n)/\tau))})\tag 2\\
&\rm \mathcal L_{fr}=\frac1{2k}\sum_{i=1}^k\left|\frac{P(y_i^n)}{P(y_1^p)}-r_i \right|+\left|\frac{P(y_i^n)}{P(y_2^p)}-r_i \right|\tag 3\\
&\rm\mathcal L_{fa} =\frac 1 {2J} ∑^J_{j=1} d(y^p_1 , y^b_ j ) + d(y^p_ 2 , y^b_ j )\tag 4
\end{align}</script><p>&emsp;&emsp;其中，$(2)$ 是对 InfoNCE 损失的重写，基本的表示方式也参考了 InfoNCE（$-\log\frac{\exp(q·k_+/\tau)}{\sum_{i=0}^{K}\exp(q·k_i/\tau)}$），$(3)$ 是度量两个正样本和负样本的频率比是否和预设的 $R$ 一致，用来限制 LFA 的学习，$(4)$ 是用来拉进正样本之间距离的。</p>
<p>&emsp;&emsp;总体来说 $\mathcal L=\mathcal L_{vr} +\mathcal L_{fc}+\mathcal L_{fr}+\mathcal L_{fa}$，需要注意的是，这里的正样本包括增强样本和邻居样本，但是和负样本拉远距离的只包括增强样本，邻居样本不和负样本拉远距离，作者在文中提到如果都拉远的话效果会变差，结果上来说这有点反直觉，也难以解释。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FRCL</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, Fs, min_hr, max_hr</span>):</span><br><span class="line">        <span class="built_in">super</span>(FRCL, self).__init__()</span><br><span class="line">        self.Fs = Fs</span><br><span class="line">        self.min_hr = min_hr</span><br><span class="line">        self.max_hr = max_hr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, neg_rppgarr, pos_rppg1, pos_rppg2, ratio_array</span>):</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        poshr1 = predict_heart_rate(pos_rppg1[<span class="number">0</span>].detach(</span><br><span class="line">        ).cpu().numpy(), self.Fs, self.min_hr, self.max_hr)</span><br><span class="line"></span><br><span class="line">        poshr2 = predict_heart_rate(pos_rppg2[<span class="number">0</span>].detach(</span><br><span class="line">        ).cpu().numpy(), self.Fs, self.min_hr, self.max_hr)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(neg_rppgarr)):</span><br><span class="line">            neghr = predict_heart_rate(neg_rppgarr[i][<span class="number">0</span>].detach().cpu().numpy(), self.Fs, self.min_hr, self.max_hr)</span><br><span class="line"></span><br><span class="line">            loss += np.<span class="built_in">abs</span>(neghr/poshr1-ratio_array[<span class="number">0</span>][i][<span class="number">0</span>].detach().cpu().numpy()) +\</span><br><span class="line">                np.<span class="built_in">abs</span>(neghr/poshr2-ratio_array[<span class="number">0</span>][i][<span class="number">0</span>].detach().cpu().numpy())</span><br><span class="line">            count += <span class="number">2</span></span><br><span class="line">        loss = loss/count</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CFAL</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, Fs, high_pass=<span class="number">2.5</span>, low_pass=<span class="number">0.4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CFAL, self).__init__()</span><br><span class="line">        <span class="comment"># PSD_MSE</span></span><br><span class="line">        self.norm_psd = CalculateNormPSD(Fs, high_pass, low_pass)</span><br><span class="line">        self.distance_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pos_rppg1, pos_rppg2, neighbor_rppg1, neighbor_rppg2, neighbor_rppg3</span>):</span><br><span class="line">        posfre1 = self.norm_psd(pos_rppg1)</span><br><span class="line">        posfre2 = self.norm_psd(pos_rppg2)</span><br><span class="line">        neifre1 = self.norm_psd(neighbor_rppg1)</span><br><span class="line">        neifre2 = self.norm_psd(neighbor_rppg2)</span><br><span class="line">        neifre3 = self.norm_psd(neighbor_rppg3)</span><br><span class="line"></span><br><span class="line">        loss = self.distance_func(posfre1, neifre1)+self.distance_func(posfre1, neifre2)+self.distance_func(posfre1, neifre3)\</span><br><span class="line">            + self.distance_func(posfre2, neifre1)+self.distance_func(posfre2, neifre2)+self.distance_func(posfre2, neifre3)</span><br><span class="line">        loss = loss/<span class="number">6</span></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FCL</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, Fs, high_pass=<span class="number">2.5</span>, low_pass=<span class="number">0.4</span>, tau=<span class="number">0.08</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FCL, self).__init__()</span><br><span class="line">        <span class="comment"># PSD_MSE</span></span><br><span class="line">        self.norm_psd = CalculateNormPSD(Fs, high_pass, low_pass)</span><br><span class="line">        self.distance_func = nn.MSELoss()</span><br><span class="line">        self.tau = tau</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, neg_rppgarr, pos_rppg1, pos_rppg2</span>):</span><br><span class="line"></span><br><span class="line">        posfre1 = self.norm_psd(pos_rppg1)</span><br><span class="line">        posfre2 = self.norm_psd(pos_rppg2)</span><br><span class="line">        pos_dis = torch.exp(self.distance_func(posfre1, posfre2)/self.tau)</span><br><span class="line">        neg_dis_total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(neg_rppgarr)):</span><br><span class="line">            negfre = self.norm_psd(neg_rppgarr[i])</span><br><span class="line">            neg_dis = torch.exp(self.distance_func(posfre1, negfre) / self.tau)+torch.exp(self.distance_func(posfre2, negfre) / self.tau)</span><br><span class="line">            neg_dis_total += neg_dis</span><br><span class="line"></span><br><span class="line">        loss = torch.log10(pos_dis/neg_dis_total+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;本文总共在 UBFC，PURE，DEAP，MMVS 上测试了效果，结果如下图所示，可以看出相比于 the way to my heart 好了不少，现在比较有竞争力的 SSL 方法也就是这一篇以及 oulu 的一篇 contrast phys，可以看出慢慢地在追赶上有监督的方法了。</p>
<p><img src="/Video-based_rppg_SSL/image-20221203155035139.png" alt="image-20221203155035139"></p>
<p>&emsp;&emsp;除了心率实验之外，本文也延续在 UBFC 上测试了基于 rppg 信号的各个指标的估计值</p>
<p><img src="/Video-based_rppg_SSL/image-20221203155556125.png" alt="image-20221203155556125"></p>
<p>&emsp;&emsp;以及跨数据集测试</p>
<p><img src="/Video-based_rppg_SSL/image-20221203155536102.png" alt="image-20221203155536102"></p>
<p>&emsp;&emsp;总体来说，本文的效果非常地好，虽然还没发，不过应该很快就能见会，问题在于：不论是这一篇还是之前的对比学习任务，以及简单化处理的 Siamese rppg，似乎都偏爱于 UBFC、PURE 这一类简单的数据集，而这篇文章做出来的时候已经有更难的 VIPL 数据集了，但是却完全没有在 VIPL 上做测试，让人不得不怀疑 SSL 方法是否仅能适用于简单的数据集。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>hr</category>
      </categories>
  </entry>
  <entry>
    <title>transformer_implements</title>
    <url>/awesome_transformers/</url>
    <content><![CDATA[<h2><center> transformer_implements </center></h2>

<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;在 视频行为识别 / 视频实例分割 / 视频超分辨率 等领域具备一定效果的 transformer backbone</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><ul>
<li>TeViT</li>
<li>UniFormer</li>
</ul>
<span id="more"></span>
<hr>
<h4 id="TeViT-Temporally-Efficient-Vision-Transformer-for-Video-Instance-Segmentation"><a href="#TeViT-Temporally-Efficient-Vision-Transformer-for-Video-Instance-Segmentation" class="headerlink" title="TeViT : Temporally Efficient Vision Transformer for Video Instance Segmentation"></a>TeViT : Temporally Efficient Vision Transformer for Video Instance Segmentation</h4><p>CVPR2022Oral 视频语义分割 <a href="https://ieeexplore.ieee.org/document/9879803/">paper</a> <a href="https://github.com/hustvl/TeViT">code</a></p>
<h5 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h5><ul>
<li>一种信使 token+移位的时间维度特征融合方法</li>
<li>多尺度的金字塔结构，生成多个不同尺度的特征图用于下一步分割头</li>
</ul>
<h5 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h5><p><img src="/awesome_transformers/image-20230308095622857.png" alt="image-20230308095622857" style="zoom:50%;"></p>
<p>&emsp;&emsp;对于基于 transformer 的特征提取 backbone，串行地打四个不同 size 的 patch，将得到的 token 经过 embeding 输入网络，从而生成 4 个比原图像降采样 4,8,16,32 的特征图序列 $\{F_i^1,F_i^2,F_i^3,F_i^4\}_{i=1}^T$，每个 $F$ 都表示了 T 帧中不同感受野的特征，用于下一步的分类/回归任务。</p>
<p>&emsp;&emsp;对于每个尺度下的 backbone，其具体操作如下：</p>
<ul>
<li>对一个 T 帧的 token $x_i\to(\frac{HW}{P^2},C)$，配备 M 个信使 token $m_i\to(M,C)$，两者 concat 之后为 $\{x_i,m_i\}_{i=1}^T\to(T,\frac{HW}{P^2}+M,C)$</li>
<li>对于配备好信使 token 的新数据，在经过 backbone 的时候，每两个 MHSA+FFN 为一次循环，对每一次循环的操作如下：<ul>
<li><img src="/awesome_transformers/image-20230308103204639.png" alt="image-20230308103204639" style="zoom:33%;"></li>
<li>图中绿色部分代表信使 token，蓝色部分代表原始的 token，上下维度表示时间维度（图中显示为 5 帧），左右维度表示信使分组个数（图中显示为 4 组，注意，每组不见得只有一个信使 token），前后维度表示 channel</li>
<li>在每轮循环中，第一个 MHSA+FFN 直接在 $\{x_i,m_i\}_{i=1}^T$ 上进行，目的是使所有的 $m$ 都学到自己对应原始 token 的信息</li>
<li>接着进行 shift，位移的是信使 token，按组进行 shift，具体方式为逐组地上下位移，位移步长为 1122···</li>
<li>再进行第二个 MHSA+FFN，和上一步一样的目的，交换不同帧序列的信息 token，以交换帧间信息</li>
<li>接着进行反向 shift，即将之前 shift 的内容再换回去，该设计旨在随着网络的深入保持稳定的时间接受场</li>
</ul>
</li>
<li>将此时输出的原始 token 记为特征序列 $\{F_i\}\to(T,\frac{HW}{P^2},C)$，此时得到的特征还原至 $(T,C,\frac H P,\frac WP)$ 接着打 patch，得到更小尺度的特征，如此循环四次，最终得到 4,8,16,32 的降采样序列（即 patch size 分别取 4,2,2,2）</li>
<li>遵循 PVT，将四个特征序列 $\{F_i^{1,2,3,4}\}^T$ 输入 STQI（时空查询交互头，即上图的右半部分）这部分是任务导向的。</li>
</ul>
<h4 id="UniFormer-Unified-Transformer-for-Efficient-Spatiotemporal-Representation-Learning"><a href="#UniFormer-Unified-Transformer-for-Efficient-Spatiotemporal-Representation-Learning" class="headerlink" title="UniFormer : Unified Transformer for Efficient Spatiotemporal Representation Learning"></a>UniFormer : Unified Transformer for Efficient Spatiotemporal Representation Learning</h4><p>ICLR2022 视频动作识别 <a href="http://arxiv.org/abs/2201.04676">paper</a> <a href="https://github.com/Sense-X/UniFormer">code</a></p>
<h5 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h5><ul>
<li>提出一种基于关系聚合器的注意力结构，在形式上统一了 Conv 和 transformer，在取得大部分 transformer 架构结果的基础上加了 efficient</li>
<li>通过 4 stage 的设置使其具备了多尺度信息</li>
</ul>
<h5 id="网络-1"><a href="#网络-1" class="headerlink" title="网络"></a>网络</h5><p><img src="/awesome_transformers/image-20230314164352127.png" alt="image-20230314164352127" style="zoom:50%;"></p>
<p>&emsp;&emsp;具体来说，本文首先分析了 timesformer 在行为识别上的表现，展示了训练好的 timesformer 在第三层的输出，包括时间注意力和空间注意力，可以发现的结论是：对于第三层的输出，某个 patch 在时间上只对上下帧具备 att，在空间上只对临近位置有 att，从而得出一个结论：前期的浅层网络应当关注局部信息，而后期深层网络关注全局信息，使用纯 transformer 架构相当于前期用了较大的计算资源做了 Conv 的事情，故而应当对 transformer 的 att 计算方式进行额外的设计。</p>
<p><img src="/awesome_transformers/image-20230310160622692.png" alt="image-20230310160622692" style="zoom: 50%;"></p>
<p>&emsp;&emsp;因此本文设计了一种“统一化”的 transformer 注意力计算，其实大概就是用一种描述统一地描述了 conv 和标准 transformer，至于 DPE 和 FFN，实际上创新性不大，或许用处是有的。具体来说，在 4 个 stage 中，前两个做局部 transformer，后两个做全局 transformer。</p>
<ul>
<li>对于每一个 stage，输入的 $X_{in}$ 首先经过 PE 打 patch，目的是降采样，此时使用 DWConv，目的是尽可能减少计算量</li>
<li>从上计算出 $X$，$X=DPE(X_{in})+X_{in}$</li>
<li>将 $X$ 输入 MHRA（多头关系聚合器），具体来说，首先将 $X$ reshape 成 $(L=T\times H\times W,C)$，又分为 $N$ 个头，然后首先通过一个linear 进行等维度的映射，得到 $V_n(X)\to(L,\frac C N)$，目的是聚合上下文信息，同时这个值就可以看做 transformer 中的 $V$</li>
<li>接着需要将 $A_n$ 和 $V_n$ 做乘法，作为第 $n$ 个头的输出结果，$A_n$ 的计算方式与 global 或者 local 有关：<ul>
<li>对于 local，$A_n(X_i,X_j)=a_n^{i-j},\ j\in\Omega^{t\times h\times w}$，含义为在一个小的邻域范围 $\Omega$ 内对两个 token 之间的 att 矩阵计算为一个常数，这个常数基于学习得到，基本上实现了类似 Conv 的信息聚合方式</li>
<li>对于 global，$A_n(X_i,X_j)=\frac{e^{Q_n(X_i)K_n(X_j)^T}}{\Sigma_{j’\in\Omega_{T\times H\times W}}e^{Q_n(X_i)K_n(X_j)^T}}$，含义为在全部 token 范围内对两个 token 之间的 att 矩阵表示为 Q 与 K 之间的 $QK^T$ 与 Q 和所有 K 之间的 $\sum QK^T$ 比值，在计算复杂度上，事实上对于原本的 transformer，也需要计算任意 $i,j$ 之间的 $Q^TK$ 结果</li>
</ul>
</li>
<li>无论是对 local 还是 global，$A_n$ 都表示注意力矩阵 $QK^T$，只不过将 Conv 的计算逻辑使用注意力矩阵的方式重新表述了</li>
<li>对于多头的结果，$A_nV_n$ 其实就是 $QK^TV$，从代码来看，softmax 和 scale 一个不少，上述 global 的公式表述也有问题，其实就是标准 att 计算过程，将多头的结果 Concat，之后和 U 相乘，也是 transformer 的标准操作</li>
<li>输出的结果 $Y$ 记为 $Y=MHRA(Norm(X))+X$，并输入 FFN，FFN 如上图，很好理解</li>
</ul>
<p>&emsp;&emsp;由此得到的四个特征图用于下游任务，值得一提的是 efficient 的结果，看上去相对于 timesformer 取得基本一致的 acc 的时候算力小很多。</p>
<p><img src="/awesome_transformers/image-20230314175746208.png" alt="image-20230314175746208"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>docx and help</title>
    <url>/docxs/</url>
    <content><![CDATA[<h2 id="文档教程收集"><a href="#文档教程收集" class="headerlink" title="文档教程收集"></a>文档教程收集</h2><p>&emsp;速查包括【markdown、 cuda编程、系统指令、Python三方库】 等文档或教程</p>
<span id="more"></span>
<h3 id="markdown"><a href="#markdown" class="headerlink" title="markdown"></a>markdown</h3><ul>
<li><a href="https://markdown.com.cn/basic-syntax/">markdown基本语法</a></li>
<li><a href="https://www.cnblogs.com/jockming/p/14120987.html">markdown数学表达式</a></li>
<li><a href="https://emojipedia.org/">markdown_emoji</a></li>
</ul>
<hr>
<h3 id="cuda编程"><a href="#cuda编程" class="headerlink" title="cuda编程"></a>cuda编程</h3><ul>
<li><a href="https://face2ai.com/program-blog/#GPU%E7%BC%96%E7%A8%8B%EF%BC%88CUDA%EF%BC%89">谭升CUDA编程</a></li>
</ul>
<hr>
<h3 id="系统指令"><a href="#系统指令" class="headerlink" title="系统指令"></a>系统指令</h3><ul>
<li><a href="https://www.linuxcool.com/">linux</a></li>
<li><a href="https://gitee.com/all-about-git">git</a></li>
</ul>
<hr>
<h3 id="python-三方库"><a href="#python-三方库" class="headerlink" title="python 三方库"></a>python 三方库</h3><ul>
<li><a href="https://pytorch.org/docs/stable/index.html">pytorch-en</a></li>
<li><a href="https://pytorch-cn.readthedocs.io/zh/latest/">pytorch-zh</a></li>
<li><a href="https://www.kancloud.cn/aollo/aolloopencv/269602">opencv-python-zh</a></li>
</ul>
]]></content>
      <categories>
        <category>docx</category>
      </categories>
  </entry>
  <entry>
    <title>layered-neural-atlases</title>
    <url>/layered-neural-atlases/</url>
    <content><![CDATA[<h2><center> Layered Neural Atlases for Consistent Video Editing </center></h2>

<p>【视频编辑】【SIGA2021】【<a href="https://arxiv.org/pdf/2109.11418.pdf">paper</a>】【<a href="https://github.com/ykasten/layered-neural-atlases">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>ds；本文使用基于 NeRF 的进行了视频编辑的工作。得到的模型将输入视频分为前景和背景并分别映射到两个图层。允许对背景图层或者前景图层的编辑并且一致性地传播到整个视频之中从而达到编辑视频的效果。本文除了 PSNR 之外没有其他数据指标，多为定性研究与消融实验。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/layered-neural-atlases/image-20221212163203372.png" alt="image-20221212163203372"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><p>（视频编辑第一次接触，都挺新的）</p>
<ul>
<li>仅使用四个 MLP 作为整个网络的 backbone</li>
<li>对不同的数据来说，测试集和训练集一致，并且模型不可泛化</li>
<li>设计了多个损失函数，针对可能出现的不同问题进行特有的限制</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/layered-neural-atlases/image-20221214093448979.png" alt="image-20221214093448979" style="zoom:67%;"></p>
<p>ds；本文含参的网络模块只有四个，分别对应上图的：$\mathbb M_b, \mathbb M_f, \mathbb M_\alpha, \mathbb A$，这四个网络都是 MLP 结构，具体参数量不同</p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><script type="math/tex; mode=display">
math\_express</script><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><hr>
<h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3>]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>video</category>
      </categories>
  </entry>
  <entry>
    <title>LeetCode-11. 盛最多水的容器</title>
    <url>/leetcode_11%E7%9B%9B%E6%9C%80%E5%A4%9A%E6%B0%B4%E7%9A%84%E5%AE%B9%E5%99%A8/</url>
    <content><![CDATA[<h2 id="LeetCode-11-盛最多水的容器【M】"><a href="#LeetCode-11-盛最多水的容器【M】" class="headerlink" title="LeetCode-11. 盛最多水的容器【M】"></a>LeetCode-<a href="https://leetcode-cn.com/problems/container-with-most-water/">11. 盛最多水的容器</a>【M】</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。</p>
<p>找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。</p>
<p>返回容器可以储存的最大水量。</p>
<p>说明：你不能倾斜容器。</p>
<span id="more"></span>
<p>示例 1：</p>
<p><img src="/leetcode_11%E7%9B%9B%E6%9C%80%E5%A4%9A%E6%B0%B4%E7%9A%84%E5%AE%B9%E5%99%A8/question_11.jpg" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：[1,8,6,2,5,4,8,3,7]</span><br><span class="line">输出：49 </span><br><span class="line">解释：图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。</span><br></pre></td></tr></table></figure>
<p>示例 2：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：height = [1,1]</span><br><span class="line">输出：1</span><br></pre></td></tr></table></figure>
<p>提示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">n == height.length</span><br><span class="line">2 &lt;= n &lt;= 105</span><br><span class="line">0 &lt;= height[i] &lt;= 104</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><ul>
<li>双指针法</li>
</ul>
<p>&emsp;&emsp;设置两个指针 p , v 分别从头部和尾部向中间缩小，缩小策略为：①移动当前较小的指针，②当能使最大值更新的时候更新最大值。代码执行逻辑如下：</p>
<hr>
<ul>
<li>初始化最大值为两端为边的值</li>
<li>当左指针小于右指针时<ul>
<li>向中间移动更小的那个指针</li>
<li>如果可以更新最大值则更新最大值</li>
</ul>
</li>
<li>返回最大值</li>
</ul>
<hr>
<h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><p>&emsp;&emsp;这里需要证明的关键在于：在小的指针固定时，大的指针无论如何向中间移动都不可能得到更小的结果。</p>
<p>&emsp;&emsp;即：记求值函数为 $f(p,v)$ 指针 $x$ 对应的高度为 $h(x)$，若 $h(p)\leq h(v)$，则对 $\forall v’\leq v,f(p,v’)\leq f(p,v)$。这个定理的正确性保证了只移动值更小的指针就可以遍历所有可能的情况，从而得到最优解。</p>
<p>&emsp;&emsp;接下来证明该定理：</p>
<ul>
<li><p>由于 $h(p)\leq h(v)$，则 $f(p,v)=(v-p)\times h(p)$，对于 $p,v’$，有 $f(p,v’)=(v’-p)\times \min(h(p),h(v’))$</p>
</li>
<li><p>由于 $v’-p\leq v-p$， 且 $\min(h(p),h(v’))\leq h(p)$</p>
</li>
<li><p>从而 $f(p,v’)=(v’-p)\times \min(h(p),h(v’))\leq (v-p)\times h(p)=f(p,v)$</p>
</li>
<li>即得证</li>
</ul>
<p>该方法时间复杂度为 $O(n)$，空间复杂度为 $O(1)$</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxArea</span>(<span class="params">self, height: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        p,v = <span class="number">0</span>,<span class="built_in">len</span>(height)-<span class="number">1</span></span><br><span class="line">        maxs = (v-p)*<span class="built_in">min</span>(height[p], height[v])</span><br><span class="line">        <span class="keyword">while</span> p&lt;v:</span><br><span class="line">            <span class="keyword">if</span> height[p] &lt;= height[v]:</span><br><span class="line">                nmax = (v-p-<span class="number">1</span>)*<span class="built_in">min</span>(height[p+<span class="number">1</span>], height[v])</span><br><span class="line">                <span class="keyword">if</span> nmax &gt; maxs:</span><br><span class="line">                    maxs = nmax</span><br><span class="line">                p += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                nmax = (v-p-<span class="number">1</span>)*<span class="built_in">min</span>(height[p], height[v-<span class="number">1</span>])</span><br><span class="line">                <span class="keyword">if</span> nmax &gt; maxs:</span><br><span class="line">                    maxs = nmax</span><br><span class="line">                v -= <span class="number">1</span></span><br><span class="line">           </span><br><span class="line">        <span class="keyword">return</span> maxs</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>code</category>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title>LeetCode-229.求众数 II</title>
    <url>/leetcode_229/</url>
    <content><![CDATA[<h2 id="LeetCode-229-求众数-II【M】"><a href="#LeetCode-229-求众数-II【M】" class="headerlink" title="LeetCode-229. 求众数 II【M】"></a>LeetCode-<a href="https://leetcode-cn.com/problems/majority-element-ii/">229. 求众数 II</a>【M】</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定一个大小为 <em>n</em> 的整数数组，找出其中所有出现超过 <code>⌊ n/3 ⌋</code> 次的元素。</p>
<span id="more"></span>
<p>示例 1：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：[3,2,3]</span><br><span class="line">输出：[3]</span><br></pre></td></tr></table></figure>
<p>示例 2：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [1]</span><br><span class="line">输出：[1]</span><br></pre></td></tr></table></figure>
<p>示例 3：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：[1,1,1,3,3,2,2,2]</span><br><span class="line">输出：[1,2]</span><br></pre></td></tr></table></figure>
<p>提示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1 &lt;= nums.length &lt;= 5 * 104</span><br><span class="line">-109 &lt;= nums[i] &lt;= 109</span><br></pre></td></tr></table></figure>
<p>进阶：尝试设计时间复杂度为 O(n)、空间复杂度为 O(1)的算法解决此问题。</p>
<hr>
<h3 id="解题思路（摩尔投票法）"><a href="#解题思路（摩尔投票法）" class="headerlink" title="解题思路（摩尔投票法）"></a>解题思路（摩尔投票法）</h3><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>&emsp;&emsp;简单的思路很简单，使用哈希表存一下然后统计个数判断就行，此方法具备 $O(n)$ 的时间复杂度和 $O(n)$ 的空间复杂度。</p>
<p>&emsp;&emsp;但是有一种更加节省空间的方法，摩尔投票法。该方法是用于寻找众数的一种简单方法，首先对最基本的情况（即从所有数字中找出出现次数大于1/2的数字）进行描述。</p>
<p>&emsp;&emsp;简要描述摩尔投票法：该方法模拟了选举的过程，将不同的数字按他们的值分为不同的派别，每一个数字能且只能投一次票，且只能投给自己的派别支持，其余派别反对。候选数字按遍历顺序轮流更替，当当前候选人得票数为 0 的时候更换候选人。</p>
<p>&emsp;&emsp;为了实现摩尔投票法，需要预设两个变量：c 和 s，其中 c 代表当前候选数字，s 代表该候选人的得票。遍历整个数组：</p>
<hr>
<ul>
<li>判断 <code>s == 0</code> ？如果是，则更换候选数字为当前数字；否，则不操作</li>
<li>判断 <code>c == n</code> ？如果是，则当前候选数字票数加一；否，则减一</li>
</ul>
<p>&emsp;&emsp;遍历完成后返回当前候选数字即可。</p>
<hr>
<h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><p>&emsp;&emsp;考虑这种情况：对于含有 $n$ 个数字的数组求解 $c=f(n)$，前 $2k$ 个数字包含了 $k$ 个答案和 $k$ 个非答案，此时对于剩下的 $n-2k$ 个数字，$c$ 仍然是 $f(n-2k)$ 的解，如此递归，最终当 $n$ 是偶数的时候数组会剩下两个 $c$，否则将会剩下一个 $c$。</p>
<p>&emsp;&emsp;对于上述说明，仍需要证明：如何保证一定可以向下递归，即为什么一定会出现《前 $2k$ 个数字包含了 $k$ 个答案和 $k$ 个非答案》的情况。使用反证法，若不包含这样的情况，则对于前 $2\times [\frac n 2]$ 个数字必然是非答案数字个数多于答案数字个数【若答案更多，则该情况在更前方的地方出现】。而此结果与有解相违背，故一定可以递归。</p>
<p>&emsp;&emsp;此时的时间复杂度为 $O(n)$，空间复杂度为 $O(1)$。</p>
<h4 id="扩展m"><a href="#扩展m" class="headerlink" title="扩展m"></a>扩展m</h4><p>&emsp;&emsp;事实上，在本题中需要找到的是前 $m$ 个候选数字，即每个数字超出所有数字个数的 $[\frac n {m+1}]$，因此需要对摩尔投票法进行简单的扩展，具体来说需要维护一个长度为 $m$ 的字典，并遍历每个数字：</p>
<ul>
<li>判断 $m$ 个候选数字是否有空席 以及  $m$ 个候选数字是否有的得票数为零<ul>
<li>如果有，则将要被替换的候选数字席位置空，并将当前数字加入候选数字</li>
<li>否则，不操作</li>
</ul>
</li>
<li>判断当前 $m$ 个候选数字的派别，投出赞同票或反对票</li>
<li>统计当前 $m$ 个候选数字的个数，返回符合题意的数字列表</li>
</ul>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">majorityElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], m: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        m -= <span class="number">1</span></span><br><span class="line">        re = []</span><br><span class="line">        dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">list</span>(dic):</span><br><span class="line">                <span class="keyword">if</span> dic[k] == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">del</span> dic[k]</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(dic) &lt; m <span class="keyword">and</span> n <span class="keyword">not</span> <span class="keyword">in</span> dic:</span><br><span class="line">                dic[n] = <span class="number">0</span></span><br><span class="line">            devote = <span class="number">0</span> <span class="keyword">if</span> n <span class="keyword">in</span> dic <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> dic:</span><br><span class="line">                <span class="keyword">if</span> k == n:</span><br><span class="line">                    dic[k] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dic[k] -= devote</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> dic:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">sum</span>([<span class="number">1</span> <span class="keyword">if</span> i==k <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> nums]) &gt; <span class="built_in">len</span>(nums)//m:</span><br><span class="line">                re.append(k)</span><br><span class="line">        <span class="keyword">return</span> re</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>code</category>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title>LeetCode-5 最长回文子串</title>
    <url>/leetcode_5/</url>
    <content><![CDATA[<h2 id="LeetCode-5-最长回文子串【M】"><a href="#LeetCode-5-最长回文子串【M】" class="headerlink" title="LeetCode-5 最长回文子串【M】"></a><a href="https://leetcode-cn.com/problems/longest-palindromic-substring/">LeetCode-5 最长回文子串</a>【M】</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>给你一个字符串 s，找到 s 中最长的回文子串。</p>
<span id="more"></span>
<ul>
<li>示例 1：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：s = &quot;babad&quot;</span><br><span class="line">输出：&quot;bab&quot;</span><br><span class="line">解释：&quot;aba&quot; 同样是符合题意的答案。</span><br></pre></td></tr></table></figure>
<ul>
<li>示例 2：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：s = &quot;cbbd&quot;</span><br><span class="line">输出：&quot;bb&quot;</span><br></pre></td></tr></table></figure>
<p>提示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1 &lt;= s.length &lt;= 1000</span><br><span class="line">s 仅由数字和英文字母组成</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="解答"><a href="#解答" class="headerlink" title="解答"></a>解答</h3><h4 id="思路1"><a href="#思路1" class="headerlink" title="思路1"></a>思路1</h4><p>对于题目，很容易想到使用动态规划，更新规则应该是<code>dp[i][j] = dp[i+1][j-1] if s[i]==s[j]</code></p>
<p>应该注意的是：数据长度最大值为1000，即可以使用二维数组。</p>
<p>最终可以接受的答案包括两种情况：①关于某字符中心对称  ②关于对称轴对称</p>
<p>采用中心扩展方法：遍历所有可能的中心位置，分对中心对称和轴对称两种情况遍历扩展即可</p>
<ul>
<li>时间复杂度：$O(n^2)$</li>
<li>空间复杂度：$O(1)$</li>
</ul>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        l,r = <span class="number">0</span>,<span class="built_in">len</span>(s)-<span class="number">1</span></span><br><span class="line">        maxstr = s[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            p,v = i,i+<span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> p&gt;=l <span class="keyword">and</span> v&lt;=r <span class="keyword">and</span> s[p] == s[v]:</span><br><span class="line">                <span class="keyword">if</span> v-p+<span class="number">1</span>&gt;<span class="built_in">len</span>(maxstr):</span><br><span class="line">                    maxstr = s[p:v+<span class="number">1</span>]</span><br><span class="line">                p -= <span class="number">1</span></span><br><span class="line">                v += <span class="number">1</span></span><br><span class="line">            p = v = i</span><br><span class="line">            <span class="keyword">while</span> p&gt;=l <span class="keyword">and</span> v&lt;=r <span class="keyword">and</span> s[p] == s[v]:</span><br><span class="line">                <span class="keyword">if</span> v-p+<span class="number">1</span>&gt;<span class="built_in">len</span>(maxstr):</span><br><span class="line">                    maxstr = s[p:v+<span class="number">1</span>]</span><br><span class="line">                p -= <span class="number">1</span></span><br><span class="line">                v += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> maxstr</span><br></pre></td></tr></table></figure>
<h4 id="提交结果"><a href="#提交结果" class="headerlink" title="提交结果"></a>提交结果</h4><p>时间——$60.1\%$</p>
<p>空间——$98.9\%$</p>
<hr>
<h4 id="思路2"><a href="#思路2" class="headerlink" title="思路2"></a>思路2</h4><p><a href="https://www.jianshu.com/p/116aa58b7d81">Manacher 算法</a></p>
<p>好难</p>
<p>估计搞懂了也立刻忘记</p>
<ul>
<li>时间复杂度：$O(n)$</li>
<li>空间复杂度：$O(n)$</li>
</ul>
<hr>
]]></content>
      <categories>
        <category>code</category>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title>LeetCode-761. 特殊的二进制序列</title>
    <url>/leetcode_761%E7%89%B9%E6%AE%8A%E7%9A%84%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%BA%8F%E5%88%97/</url>
    <content><![CDATA[<h2 id="LeetCode-761-特殊的二进制序列"><a href="#LeetCode-761-特殊的二进制序列" class="headerlink" title="LeetCode-761. 特殊的二进制序列"></a>LeetCode-<a href="https://leetcode.cn/problems/special-binary-string/">761. 特殊的二进制序列</a></h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>特殊的二进制序列是具有以下两个性质的二进制序列：</p>
<p>0 的数量与 1 的数量相等。<br>二进制序列的每一个前缀码中 1 的数量要大于等于 0 的数量。<br>给定一个特殊的二进制序列 S，以字符串形式表示。定义一个操作 为首先选择 S 的两个连续且非空的特殊的子串，然后将它们交换。（两个子串为连续的当且仅当第一个子串的最后一个字符恰好为第二个子串的第一个字符的前一个字符。)</p>
<p>在任意次数的操作之后，交换后的字符串按照字典序排列的最大的结果是什么？</p>
<span id="more"></span>
<p>示例 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: S = &quot;11011000&quot;</span><br><span class="line">输出: &quot;11100100&quot;</span><br><span class="line"></span><br><span class="line">解释:</span><br><span class="line">将子串 &quot;10&quot; （在S[1]出现） 和 &quot;1100&quot; （在S[3]出现）进行交换。</span><br><span class="line">这是在进行若干次操作后按字典序排列最大的结果。</span><br></pre></td></tr></table></figure>
<p>说明:</p>
<ol>
<li>S 的长度不超过 50。</li>
<li>S 保证为一个满足上述定义的特殊 的二进制序列。</li>
</ol>
<hr>
<h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>&emsp;&emsp;本题采用分治法，对每一个01串进行如下操作：</p>
<ul>
<li>排除首位 1 和末位 0</li>
<li>拆分成多个 特殊01串 并暂存<ul>
<li>使用一个标记位，当遇到1就+1，遇到0就-1，当标记位为0时则得到一个 子特殊01串</li>
</ul>
</li>
<li>排序所有的 01串 并返回</li>
</ul>
<h4 id="关键结论"><a href="#关键结论" class="headerlink" title="关键结论"></a>关键结论</h4><ol>
<li>每个特殊01串必然以 1 开始， 以 0 结束</li>
<li>每个特殊01串若以首位1开始标记位拆分，则结果不一定是其本身</li>
<li>标记位方法所得的拆分是最小拆分</li>
</ol>
<p>时间复杂度：$O(n^2)$</p>
<p>空间复杂度：$O(n)$ </p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">makeLargestSpecial</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(s) &lt;= <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line">    </span><br><span class="line">        tmp = left = <span class="number">0</span></span><br><span class="line">        tmp_str = <span class="built_in">list</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(s):</span><br><span class="line">            tmp += <span class="number">1</span> <span class="keyword">if</span> ch == <span class="string">&#x27;1&#x27;</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> tmp == <span class="number">0</span>:</span><br><span class="line">                tmp_str.append(<span class="string">&#x27;1&#x27;</span> + self.makeLargestSpecial(s[left+<span class="number">1</span>:i]) + <span class="string">&#x27;0&#x27;</span>)</span><br><span class="line">                left = i + <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        tmp_str.sort(reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(tmp_str)</span><br></pre></td></tr></table></figure>
<h4 id="提交结果"><a href="#提交结果" class="headerlink" title="提交结果"></a>提交结果</h4><p>时间——$\%66.67$</p>
<p>空间——$\%36.36$</p>
<hr>
]]></content>
      <categories>
        <category>code</category>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title>LeetCode-XX XXXX</title>
    <url>/leetcode_template/</url>
    <content><![CDATA[<h2 id="LeetCode-XX-XXXX"><a href="#LeetCode-XX-XXXX" class="headerlink" title="LeetCode-XX XXXX"></a>LeetCode-XX XXXX</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><span id="more"></span>
<hr>
<h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>时间复杂度：</p>
<p>空间复杂度：</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="提交结果"><a href="#提交结果" class="headerlink" title="提交结果"></a>提交结果</h4><p>时间——$\%$</p>
<p>空间——$\%$</p>
<hr>
]]></content>
      <categories>
        <category>template</category>
      </categories>
  </entry>
  <entry>
    <title>XXXX</title>
    <url>/paper_template/</url>
    <content><![CDATA[<h2><center> paper_name </center></h2>

<p>【task】【conf_name】【<a href>paper</a>】【<a href>code</a>】</p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><h3 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h3><span id="more"></span>
<hr>
<h3 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h3><h3 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h3><script type="math/tex; mode=display">
math\_express</script><h3 id="experiment"><a href="#experiment" class="headerlink" title="experiment"></a>experiment</h3>]]></content>
      <categories>
        <category>template</category>
      </categories>
  </entry>
  <entry>
    <title>pctpose</title>
    <url>/pctpose/</url>
    <content><![CDATA[<h2><center> Human Pose as Compositional Tokens </center></h2>

<p>【姿态检测】【CVPR2023】【<a href="http://arxiv.org/abs/2303.11638">paper</a>】【<a href="https://sites.google.com/view/pctpose">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文观察到在存在遮挡情况下的姿态检测案例中会出现被遮挡部分的不合理现象，因此引入 VQVAE 框架以尽可能在遮挡情况下获得近真实的姿态。整体思路非常简单，关键在于其 VQVAE 部分实际上是对姿态坐标 $(17,2)$ 的重建，这样少量的信息能够被真实有效重建，其中必然包含了特殊设计（姿态转化为 M 个 token）以及漫长的调试。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/pctpose/image-20230504105458592.png" alt="image-20230504105458592"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>一种专门针对姿态检测的 VQVAE 编码器</li>
<li>使用 M 个解耦向量对姿态进行表述，每个向量有属于的对应类别，从此类别估计关键点</li>
</ul>
<h3 id="tricks"><a href="#tricks" class="headerlink" title="tricks"></a>tricks</h3><ul>
<li>使用 ema 更新 codebook，而非梯度更新</li>
<li>使用 MAE 的方式进行训练</li>
<li>不使用 index 计算 CE，而是使用对应的 logit</li>
<li>使用 soft 的方式计算 VQ</li>
<li>在第二阶段不训练编码器，锁住解码器和 codebook，将 backbone 内嵌编码器</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/pctpose/image-20230504140351195.png" alt="image-20230504140351195"></p>
<p>&emsp;&emsp;整体来说，整个前向过程比较 naive，使其有效的过程在我看来有两个关键：1.对于 VQ 的训练，编码器采用适度复杂的扩张网络，解码器和编码器对应，并且给 latent code 一定的含义；2.第二阶段相比于其他的 VQVAE 难训练得多，因为这里二阶段剔除了编码器，而是使用一个结构类似的“四层 MLP-Mixer”进行训练，这里是从零训练，会不会太难？是否可以考虑使用 label 进行深度监督？</p>
<p>&emsp;&emsp;其前向过程表示为：</p>
<p><strong>stage-1</strong></p>
<ul>
<li>将原始姿态数据表示为 $G\to(K,D)$，其中 $K$ 表示有多少关键点，本文选择 $K=17$，$D$ 表示每个关键点的位置表征，在 2D 的情况下，$D=2$</li>
<li>将 $G$ 输入编码器 $f_e$，得到一个包含 M 个向量的集合 $T=\{t_i\}_i^M$</li>
<li>对 $T$ 进行 VQ，VQ 过程使用 NN 方法，从而得到使用 codebook $C=(c_1,\dots,c_V)^T\to(V,N)$ 的表示：$\hat T=\{c_{q(t_i)}\}_i^M$</li>
<li>将 $\hat T$ 输入解码器 $f_d$，得到重建姿态 $\hat G$</li>
<li>计算 loss：$L_{pct}=L_1(G,\hat G)+\beta\sum\limits_{i=1}^M||t_i-sg[c_{q(t_i)}]||_2^2$<ul>
<li><strong>codebook 更新问题</strong>：这里只有对 codebook 的梯度裁剪，也就是说对于 codebook 没有采用梯度更新，文中阐述采用 ema 进行更新（V100:/root/code/PCT/models/pct_tokenizer.py:155）</li>
</ul>
</li>
</ul>
<p><strong>stage-2</strong></p>
<ul>
<li>原始输入图像为 $I$，使用对应的 backbone 提取特征为 $X$，$X$ 的 shape 不重要，因为接下来会使用一些 proj 改到固定 shape</li>
<li>将 $X$ 经过一个特征调制器 $\cal C$ 和一个线性投影头 $\cal L$ 之后得到：$X_f=\mathcal L(\mathrm{Flatten}(\mathcal C(X)))\to(M,N)$<ul>
<li>此时，$X_f$ 已经和 $T$ 的 shape 一致，但是为了保证具备 stage-1 编码器能力，额外加入一些 MLP</li>
</ul>
</li>
<li>由四层 MLP-Mixer（近似 stage-1 Encoder）进行编码，得到 $\hat L=\mathcal M(X_f)\to(M,V)$<ul>
<li>$\hat L$ 的每一行的 $V$ 列表示 $V$ 个 codebook index 的预测概率</li>
</ul>
</li>
<li>由此计算和 GT 的 CEloss $L_{cls}=CE(\hat L,L)$<ul>
<li>这里的 $L$ 来自于真正的 $G$ 在 stage-1 中的 VQ 结果，其 shape 为 $(M,1)$，由于来源于 NN，只包含最终的类别，计算 CE 时进行广播</li>
</ul>
</li>
<li>stage-2 中计算的 $\hat G=f_d(S)$，其中 $S=\hat L\times C$，$C$ 就是 codebook</li>
<li>计算重建损失 $L_1(\hat G,G)$，因此 stage-2 的 loss 最终为：$L=CE(\hat L,L)+L_1(\hat G,G)$ </li>
</ul>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><ul>
<li>关于 codebook size 和 M 的选择消融实验</li>
</ul>
<p><img src="/pctpose/image-20230504150017630.png" alt="image-20230504150017630" style="zoom: 50%;"></p>
<ul>
<li>可视化效果（绿框中是遮挡也测出来的，红框中是遮挡没测出来的）</li>
</ul>
<p><img src="/pctpose/image-20230504150205661.png" alt="image-20230504150205661" style="zoom:60%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>represent-light-field-with-a-single-image</title>
    <url>/represent-light-field-with-a-single-image/</url>
    <content><![CDATA[<h2 id="Represent-LightField-With-A-Single-Image【光场压缩】【not-published】"><a href="#Represent-LightField-With-A-Single-Image【光场压缩】【not-published】" class="headerlink" title="Represent LightField With A Single Image【光场压缩】【not published】"></a><a href="https://gitee.com/bnucsy/represent-light-field-with-a-single-rgb-image">Represent LightField With A Single Image</a>【光场压缩】【not published】</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><ul>
<li>提出了一种基于自编码器的光场压缩方法，构建了基于 U-Net 的编解码器使用单张RGB图进行光场压缩。</li>
<li>得到的 RGB 表征在 0.07bpp 下𝑃𝑆𝑁𝑅为 41.31dB，重建光场𝑃𝑆𝑁𝑅值 36.51dB， 𝑆𝑆𝐼𝑀值为 0.96。</li>
<li>在使用网络平台 JPEG 压缩后重建光场𝑃𝑆𝑁𝑅值为 35.48dB。</li>
<li>对重建后光场应用重聚焦和深度估计任务，均取得了较好的结果。</li>
</ul>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/represent-light-field-with-a-single-image/image-20220502140548979.png" alt="image-20220502140548979"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>限制使用单张 RGB 图像表征光场，并且该图像和光场中央视图视觉效果一致</li>
<li>得到的 RGB 表征对 JPEG 压缩具备鲁棒性</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><ul>
<li><p>网络概览</p>
<p><img src="/represent-light-field-with-a-single-image/image-20220502140608358.png" alt="image-20220502140608358"></p>
</li>
<li><p>$E、D$ 网络结构</p>
<p><img src="/represent-light-field-with-a-single-image/image-20220502140622337.png" alt="image-20220502140622337"></p>
</li>
<li><p>$J$ 网络结构</p>
<p><img src="/represent-light-field-with-a-single-image/image-20220502140637615.png" alt="image-20220502140637615"></p>
</li>
</ul>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>使用$MSELoss$，分别限制中央视图和光场，比率分别为$0.9$，$0.5$。</p>
<script type="math/tex; mode=display">
\rm arg \min\limits_{E,D,\alpha,\beta}:=\alpha\times||J(E(L))-L(\frac M 2,\frac N 2)||_2^2 + \beta\times ||D(J(E(L)))-L||_2^2</script><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><ul>
<li><p>和传统方法对比</p>
<p><img src="/represent-light-field-with-a-single-image/image-20220502140653242.png" alt="image-20220502140653242"></p>
</li>
<li><p>深度估计</p>
<p><img src="/represent-light-field-with-a-single-image/image-20220502141337322.png" alt="image-20220502141337322" style="zoom:80%;"></p>
</li>
<li><p>重聚焦</p>
<p><img src="/represent-light-field-with-a-single-image/image-20220502140504370.png" alt="image-20220502140504370"></p>
</li>
</ul>
<hr>
<h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3><p>&emsp;自己的文章，启发即为过程性总结，全在毕设记录之中了</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>mypaper</category>
      </categories>
  </entry>
  <entry>
    <title>styleGAN</title>
    <url>/styleGAN/</url>
    <content><![CDATA[<h2><center> A Style-Based Generator Architecture for Generative Adversarial Networks </center></h2>

<p>【图像生成】【CVPR2019】【<a href="http://arxiv.org/abs/1812.04948">paper</a>】【<a href="https://github.com/NVlabs/stylegan">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文基于 PGGAN 提出了一种按照尺度（非属性）解耦隐空间特征的 styleGAN 网络，这种尺度在 fine detail 上对于视觉的表现即为风格，因此除了生成和 PGGAN 一样的高分辨率图像之外，styleGAN 还可以对多个图像的不同尺度特征进行拼接，从而进行风格迁移。局限于这些特征都是 latent code，因此不能融合任意图像，但仍然挖下了解耦隐空间的坑。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/styleGAN/image-20230105204654476.png" alt="image-20230105204654476"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>基于 PGGAN 的插值、Norm 改进以及超参数调优</li>
<li>隐空间 $\cal W$ 的提出，使用 $f$ 在输入生成器 $g$ 之前将正态分布 $\cal Z$ 映射到 $\cal W$</li>
<li>使用 AdaIN 操作将采样出的 $w\in\cal W$ 逐级加入不同的分辨率</li>
<li>对于不同的尺度，加入随机噪声，从而保证图像的多样性（一般）</li>
<li>混合正则化，即生成图像时从不同的 $z$ 生成的不同 $w$ 中取不同的尺度进行组合，进一步解耦</li>
<li>两个新的度量生成图像的隐空间解耦程度的指标，<code>Perceptual path length(PPL)</code> 和 <code>Linear separability</code></li>
<li>不同尺度控制的风格不同，粗粒度的特征（前几层）控制姿态、头发、脸型等，中粒度的特征控制眼睛、脸部特征等，细粒度的特征控制风格、色调等。需要注意的是这些属性并没有限制，而是通过限制尺度自主学习到的属于不同尺度的属性。</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><h4 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a>backbone</h4><p><img src="/styleGAN/image-20230105231848102.png" alt="image-20230105231848102" style="zoom:67%;"></p>
<p>&emsp;&emsp;上图中左侧是 PGGAN 的 backbone， 在其中加入了 <code>PixelNorm</code>，整体上就是针对 PGGAN 的调优。右侧即为 styleGAN 的 backbone，可以看出对于 $g$ 基本没有变化，区别在于：</p>
<ul>
<li>多了一个由八个 MLP 组成的 Mapping Network $f$，用于将 $z\in\cal Z$ 的采样映射到 $w\in\cal W$</li>
<li>对于 $g$，其输入不同，PGGAN 中输入直接是 $z$，而 styleGAN 之中输入是一个 <code>const</code>，在前向的过程之中输入 <code>style</code> 和 <code>noise</code>，其中 <code>style</code> 来自于对 $w$ 进行可学习的仿射变换 $A$，<code>noise</code> 来自于对随机噪声进行通道加权填充 $B$</li>
<li>对于输入的 <code>style</code>，使用 $AdaIN$ 输入，$AdaIN$ 是其他工作验证有效的 <code>style</code> 嵌入方式</li>
</ul>
<p>&emsp;&emsp;对于上述 backbone 的改进，其具体操作和有效性证明如下：</p>
<ol>
<li><strong>$\cal W$ 空间映射的必要性</strong></li>
</ol>
<p><img src="/styleGAN/image-20230106102057695.png" alt="image-20230106102057695" style="zoom: 50%;"></p>
<p>&emsp;&emsp;之前的工作多为直接从 $\cal Z$ 即 $\mathcal N(0,1)$ 采样，通过这种方式采样的隐编码 $z$ 往往在线性插值之后失去其语义信息，上述图像是一个例子：</p>
<ul>
<li><p>$(a)$ 表示训练集的一个分布，可以简单理解为性别和头发长度的分布，缺少的一角表示在训练集中不存在长发男性，则理论上 $g$ 不可能生成长发男性而不被 $D$ 认出。</p>
</li>
<li><p>对于这种不规则的分布，如果希望 $\cal Z$ 中的采样能映射至 $(a)$，则 $\cal Z$ 会产生较大的 <code>warp</code>，如 $(b)$ 所示，这种 <code>warp</code> 会导致 $\cal Z$ 难以学习到插值有效的隐编码。</p>
</li>
<li><p>而允许 $\cal f(Z)\to W$  的变形则赋予了 $\cal W$ 更灵活的初始分布，如图 $(c)$ 所示，这样可以产生更少的 <code>warp</code>，也就更容易学习。</p>
</li>
</ul>
<ol>
<li><strong><code>style</code> 的语义以及 $AdaIN$ 的输入</strong></li>
</ol>
<p>&emsp;&emsp;对于采样出的 $z\to(512,1)$，经过映射之后 $w=f(z)\to(512,1)$，这里的 $w$ 就初步代表 <code>style</code>，而针对不同的尺度，学习不同的仿射变换 $A$ 将 $w$ 映射至 $(512,2)$，其中 <code>512</code> 表示通道的个数，<code>2</code> 表示针对每个通道有两个值分别表示缩放和平移。这里的缩放和平移就是最终针对不同尺度不同通道的 <code>style</code>。</p>
<p>&emsp;&emsp;对于 $AdaIN$，其输入包括 <code>feature map</code> $x$ 和 <code>style</code> $y$，公式描述为：</p>
<script type="math/tex; mode=display">
AdaIN(x_i,y)=y_{s,i}\frac{x_i-\mu(x_i)}{\sigma(x_i)}+y_{b,i}</script><p>&emsp;&emsp;其中的 $i\in0\to512$，含义上代表第几个通道，$s,\ b$ 分别代表 $y_i$ 的第 <code>0</code> 维和第 <code>1</code> 维，含义上分别代表缩放和平移，这么做的原因是：</p>
<ul>
<li>首先对 $x_i$ 进行归一化，防止 $x_i$ 的不同值的尺度产生的影响，也起到去除 $x_i$ 的风格的作用</li>
<li>接着对归一化的 $x_i$ 进行平移和缩放，加入新的风格</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Style modulation.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">style_mod</span>(<span class="params">x, dlatent, **kwargs</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;StyleMod&#x27;</span>):</span><br><span class="line">        style = apply_bias(dense(dlatent, fmaps=x.shape[<span class="number">1</span>]*<span class="number">2</span>, gain=<span class="number">1</span>, **kwargs))</span><br><span class="line">        style = tf.reshape(style, [-<span class="number">1</span>, <span class="number">2</span>, x.shape[<span class="number">1</span>]] + [<span class="number">1</span>] * (<span class="built_in">len</span>(x.shape) - <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">return</span> x * (style[:,<span class="number">0</span>] + <span class="number">1</span>) + style[:,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<ol>
<li><strong><code>noise</code> 的输入与作用</strong></li>
</ol>
<p>&emsp;&emsp;由不相关的噪声根据不同的尺度无参应用不同的比例因子，噪声直接来源于 $\mathcal N(0,1)$，并且直接广播至所有的特征通道。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Noise input.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_noise</span>(<span class="params">x, noise_var=<span class="literal">None</span>, randomize_noise=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(x.shape) == <span class="number">4</span> <span class="comment"># NCHW</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Noise&#x27;</span>):</span><br><span class="line">        <span class="keyword">if</span> noise_var <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> randomize_noise:</span><br><span class="line">            noise = tf.random_normal([tf.shape(x)[<span class="number">0</span>], <span class="number">1</span>, x.shape[<span class="number">2</span>], x.shape[<span class="number">3</span>]], dtype=x.dtype)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            noise = tf.cast(noise_var, x.dtype)</span><br><span class="line">        weight = tf.get_variable(<span class="string">&#x27;weight&#x27;</span>, shape=[x.shape[<span class="number">1</span>].value], initializer=tf.initializers.zeros())</span><br><span class="line">        <span class="keyword">return</span> x + noise * tf.reshape(tf.cast(weight, x.dtype), [<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>noise</code> 的应用可以让图像有更多的生成，原文提出细小的变化会增大图像的多样性同时不影响人眼的辨识度，下图给出了原图以及在不同噪音下生成的图像的标准差和随机变化，对于肉眼确实区别不大，但是这个多样性在现在的 DM 之前就差得太多了。</p>
<p><img src="/styleGAN/image-20230106144334983.png" alt="image-20230106144334983" style="zoom:50%;"></p>
<h4 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h4><p>&emsp;&emsp;训练过程中采样出的 $z\in\cal Z$ 经过映射网络 $f$ 得到 $w\in\cal W$，接着随机化一个常数输入进入生成器 $g$，生成器从 <code>4x4</code> 的生成分辨率逐级增加，直到 <code>1024x1024</code>，判别器结构以及整个过程和 PGGAN 一致。</p>
<p>&emsp;&emsp;在 $g$ 的前向过程中，$w$ 在不同尺度下使用不同的可学习的仿射矩阵 $A$ 融合进入 $g$ 的特征图，融合方式为 $AdaIN$，这样可以将原本的特征替换为新的 $w$ 决定的特征，而多个 $A$ 的方式可以分开不同尺度的风格。</p>
<p>&emsp;&emsp;为了进一步进行属性分离，使用了混合正则化的方式，所谓混合正则化，就是同时采样两个 $z$，生成两个不同的 $w$，在训练 $g$ 时，在某个固定的层之前使用 $w_1$ 通过对应的 $A$ 生成 <code>style</code>，在之后使用 $w_2$，通过这种方式强制网络认为不同粒度之间的非耦合性。</p>
<p>&emsp;&emsp;同时，为了防止在 GAN 的训练过程中出现无法生成图像的隐空间（即 $\cal W$ 中低密度的区域），本文使用了截断 $w$ 的技巧，这并非本文首次提出，截断的目的是缩小采样空间的范围，使其更加密集。具体的做法为设定超参数 $\psi$，对于从训练好的 $\cal W$ 中采样出的 $w$，其操作可以描述为：$w’=\bar w+\psi(w-\bar w)$，可以看出当 $\psi=0$ 时，用于生成图像的 $w$ 是固定的 $\mathbb E(\cal  W)$，从而只能生成同样的“平均脸”。具体来说，$\psi$ 控制了采样的隐编码和分布本身的权重关系，限制了一定的多样性同时提高了稳定性。</p>
<p>&emsp;&emsp;下图展示了不同 $\psi$ 下同一个隐编码 $w$ 的生成结果，有趣的是在不加限制的情况下学到的网络可以将 $\psi$ 正负调转显式地投影到属性的翻转上。</p>
<p><img src="/styleGAN/image-20230106153718003.png" alt="image-20230106153718003"></p>
<h4 id="感知路径长度-amp-线性可分性"><a href="#感知路径长度-amp-线性可分性" class="headerlink" title="感知路径长度 &amp; 线性可分性"></a>感知路径长度 &amp; 线性可分性</h4><p>&emsp;&emsp;为了度量生成图像的质量，需要比较其和训练集图像的分布一致性，这部分的研究非常多，但是 styleGAN 研究的是如何对尺度进行解耦合，因此需要另外的指标判断解耦合的程度。</p>
<p>&emsp;&emsp;对于感知路径长度 <code>Perceptual path length</code>，其本质上进行了如下操作：</p>
<ul>
<li>从 $\cal Z$ 中采样 $z_1, z_2$，从 $U(0,e_t)$ 中采样 $t$，固定超参数 $\epsilon=10^{-4}$，对于 <code>full</code>，$e_t=1$，对于 <code>end</code>，$e_t=0$</li>
<li>对 $z_1,z_2$ 进行插值，比例分别为 $t,t+\epsilon$，得到 $s_1=lerp(z_1,z_2;t),s_2= lerp(z_1,z_2;t+\epsilon)$</li>
<li>将 $s_1,s_2$ 使用 <code>VGGLoss</code> 计算结构误差，计算出的结果除以 $\epsilon^2$</li>
<li>多次采样求期望即可</li>
</ul>
<p>&emsp;&emsp;对于线性可分性 <code>Linear separability</code>，其本质上是度量一个图像是否可以由两个分类器在不同 <code>level</code> 上进行属性区分。举例来说，对于性别属性（必须是二值属性），使用 SVM 分类隐编码是否能够得到和 CNN 分类生成图像一致的结果即为线性可分性。其选择的属性共包括 40 个：</p>
<p><img src="/styleGAN/image-20230106151916310.png" alt="image-20230106151916310"></p>
<p>&emsp;&emsp;记 SVM 的结果为 $X$，CNN 的结果为 $Y$，则最终的线性可分性即为 $\exp (\sum_iH(Y_i|X_i))$，其中 $H$ 是条件熵，$i$ 表示某一个属性，$i\in 0\to40$。</p>
<p>&emsp;&emsp;下图展示了 FFHQ 中映射网络的效果。方法名称中的数字表示映射网络的深度。可以看到，FID、可分离性和路径长度都受益于映射网络。此外，深度映射网络通常比浅映射网络表现更好。</p>
<p><img src="/styleGAN/image-20230106152818674.png" alt="image-20230106152818674" style="zoom: 50%;"></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;对于消融实验，本文逐渐向 PGGAN 中加入了自己的策略，从而得到了最终的 styleGAN，这个消融实验的方式也和 PGGAN 的很接近。消融实验展示了各个策略的有效性，其中指标为 FID，用于度量数据的分布一致性。其中加入噪音没有使指标更好，其本质的作用是使图像具备多样性。这些指标均为叠加使用策略。</p>
<p><img src="/styleGAN/image-20230106153352569.png" alt="image-20230106153352569" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>styleVideoGAN</title>
    <url>/styleVideoGAN/</url>
    <content><![CDATA[<h2><center> StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN </center></h2>

<p>【人脸视频生成】【<a href="https://arxiv.org/abs/2107.07224v1">paper</a>】【<a href>code未开源</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文使用预训练的 styleGAN 为基础，采用类似 pSp 的架构进行 $\mathcal W+$ 空间的视频序列监督，同时采用基于 GRU 架构的 RNN 网络自回归生成视频，为了避免视频生成时的片段重复现象，提出了“梯度角惩罚”。另外本文展示了效果不太好的“偏移技巧”，即将生成视频的运动转移到不同的主题。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/styleVideoGAN/image-20230410151337116.png" alt="image-20230410151337116"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>在预训练 styleGAN 的 $\mathcal W+$ 空间上对时间序列进行训练以生成 video，模型完全在 $\mathcal W+$ 空间监督</li>
<li>提出一种 offset trick，用以将生成的视频运动转化至不同的主题</li>
<li>使用 gradient angle penalty 惩罚 RNN，用以避免 RNN 生成视频时可能出现的循环问题</li>
<li>该方法可以生成手部视频</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/styleVideoGAN/image-20230410151303114.png" alt="image-20230410151303114"></p>
<p> &emsp;&emsp;本文的架构是一个 WGAN，具体来说，输入为 $(i,s)$，其中 $i\sim \mathcal N(0,1)^{32},\ s\sim \mathcal N(0,1)^{32\times(t-1)}$，也就是说，$i$ 就代表视频的第一帧，其余的 $s$ 都代表剩余帧。这些初始输入通过一系列 RNN 得到 $l$（这里的 $l$ 对标 styleGAN 中的 $z$），然后经过 $T$ 的特征映射（对标 styleGAN 的 $F$）并进一步学习仿射变换得到 $\mathcal W+$ 空间的各个帧的隐向量 $w$（这里的 $w$ 对标到 pSp 之中是 $18\times 512$ 的向量，包含一帧的所有粒度特征控制）。之后的 $C$ 即判别器，只是换了一个名字，之所以判别器需要包括 $E,\ TConv$ 两个部分，主要考虑到生成器与判别器的网络容量大小关系。</p>
<p>&emsp;&emsp;整体来说，本文的前向流程如下：</p>
<ul>
<li>对于初始视频 $V$ 其中包含 $t$ 帧 $I_t$，首先将每一帧通过预训练的 pSp 映射至 $w_k^+\in\mathcal W+ ,\ k\in[0,t)$</li>
<li>接着从 $\mathcal N(0,1)$ 中采样 $i$ 和 $t-1$ 个 $s$，输入生成器 $G$，$G(i,s)=\{l_k\},k\in[0,t)$</li>
<li>将得到的 $t$ 个 $l$ 分别通过映射 $T$ 和 18 个仿射映射，得到 $w_k\in\mathcal W+,\ l\in[0,t)$</li>
<li>对于 $w_k,\ w_k^+$，将其分别作为 fake 和 real 输入判别器 $C$，通过对抗训练得到合适的 $G$</li>
</ul>
<p>&emsp;&emsp;⛔本文的架构理论上可以生成任意长度的视频（测试时），训练为了考虑判别器还是只能产生 25 帧</p>
<h4 id="G-的网络结构"><a href="#G-的网络结构" class="headerlink" title="$G$ 的网络结构"></a>$G$ 的网络结构</h4><p>&emsp;&emsp;生成器包含一个 $H$ 以及一个由 4 个GRU单元组成的 $P$，$P$ 处理“每时间步随机”，目的是产生多样的运动方式。为了初始化 GRU 内存，让 MLP $H$ 伪生成前三个单元的一些内存内容，而最后一个单元则用 $i$ 初始化。使用数学语言描述为：</p>
<script type="math/tex; mode=display">
(h_{0,0},h_{0,1},h_{0,2}):= H(i)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ h_{0,3}:= i\\
((h_{k+1,0}, . . . ,h_{k+1,3}),l_{k+1}) := P(s_k,(h_{k,0}, . . . ,h_{k,3}))</script><p>&emsp;&emsp;简单来说，第 $k$ 层 $P_k$ 接收输入 $s_k$，和上一层传下来的四个隐特征 $\{h_{k-1,0\to3}\}$， 输出下一层 $P_{k+1}$ 所需要的隐特征，同时每一层输出的隐特征的最后一个直接当做输出。</p>
<h4 id="损失函数（“梯度角惩罚”）（gradient-angle-penalty）"><a href="#损失函数（“梯度角惩罚”）（gradient-angle-penalty）" class="headerlink" title="损失函数（“梯度角惩罚”）（gradient angle penalty）"></a>损失函数（“梯度角惩罚”）（gradient angle penalty）</h4><p>&emsp;&emsp;本文的损失描述为：</p>
<script type="math/tex; mode=display">
\mathcal L =\mathcal L_{WGAN} + λ_{GP}\mathcal L_{GP} + λ_{GAP}\mathcal L_{GAP}</script><p>&emsp;&emsp;其中前两项记为 WGAN 的损失函数描述，最后一项即本文新提出的梯度角惩罚，其目的是防止使用 RNN 架构生成长视频时出现“循环视频片段”的情况。</p>
<p>&emsp;&emsp;其具体的做法为：</p>
<ul>
<li>计算最后一帧相对于第一帧的偏移 $d,\ d=norm(l_{t-1}-l_0)$</li>
<li>计算 $d$ 对每一个 $s$ 的梯度（$t-1$ 个 ），对其计算二阶范数（一阶应该也行），作为分子</li>
<li>计算 $d$ 对 $i$ 的梯度，作为分母</li>
<li>取 arctan，得到角度 $\phi$，最终的 $\mathcal L_{GAP}=max(0, \frac\pi 4-\phi)^2$</li>
</ul>
<p>即：</p>
<p><img src="/styleVideoGAN/image-20230410213039520.png" alt="image-20230410213039520" style="zoom: 50%;"></p>
<p>&emsp;&emsp;这个式子的有效性原理在于：假如视频的生成出现了循环，那么就说明生成的 video 没有太和随机采样的 $s$ 有关，即更多地和 $i$ 有关，此时关于 $i$ 的梯度会明显大于关于其他采样的 $s$ 的梯度，这时候 $\phi$ 就会接近 0，返回的 loss 也就会变大。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;在短视频和长视频上的 FID 和 FVD 的值，先 mark 一下，到时候可能要和他比一比</p>
<p><img src="/styleVideoGAN/image-20230410213403647.png" alt="image-20230410213403647"></p>
<p>&emsp;&emsp;生成的视频质量，还是很棒的（毕竟基于 pSp，至少放一些直接 reverse 的结果也不会差）</p>
<p><img src="/styleVideoGAN/image-20230410213641532.png" alt="image-20230410213641532" style="zoom:50%;"></p>
<p>&emsp;&emsp;偏移技巧：将生成的视频转化为不同的主题（也就是视频驱动人脸生成视频），效果满差的感觉，下图中上面是生成的正常视频，下面的左一，左二是普通的方法，左三，左四是他提出的基于 PCA 的正交迁移方法，因为效果属实不好，需要的话后期再看</p>
<p><img src="/styleVideoGAN/image-20230410214149442.png" alt="image-20230410214149442"></p>
<p>&emsp;&emsp;本文还专门强调自己虽然是专注于人脸视频生成，但是所设计的方法之中没有针对人脸设计的模块，因此也放了一些其他 domain 的结果，可以看出确实多了些多样性出来</p>
<p><img src="/styleVideoGAN/image-20230410213805162.png" alt="image-20230410213805162"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>Swin_Transformer</title>
    <url>/swin_transformer/</url>
    <content><![CDATA[<h2 id="Swin-Transformer-Hierarchical-Vision-Transformer-Using-Shifted-Windows-【图像分类、目标检测、语义分割】-ICCV"><a href="#Swin-Transformer-Hierarchical-Vision-Transformer-Using-Shifted-Windows-【图像分类、目标检测、语义分割】-ICCV" class="headerlink" title="Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows 【图像分类、目标检测、语义分割】 ICCV"></a><a href="https://arxiv.org/abs/2103.14030">Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows</a> 【图像分类、目标检测、语义分割】 ICCV</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;SwinTransformer 同样是一篇尝试探索如何将 NLP 领域的 transformer 迁移到 CV 领域的工作，其指出，文字的密度和图像的像素密度差距较大，直接迁移如 VIT 将会产生大图像难以计算的问题，因此 swinTransformer 提出了一种层级式的计算方式，不仅将计算复杂度降低了很多，同时提取到了不同尺度的特征，解决了 VIT 的大尺寸图像处理困难、任务场景较为单一等问题。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/swin_transformer/image-20220925222255645.png" alt="image-20220925222255645"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>使用基于 Window 的 MSA 来进行大尺寸图像的分割处理，降低 transformer 的计算复杂度，使 transformer 可以计算大尺寸的图像</li>
<li>使用 shift Window 来加强不同 window 之间的通信，使全局的信息更容易被学习</li>
<li>通过 patch merging 获得多尺度的信息，以此产生不同的感受野，适应了多种任务场景构造特征图的需要</li>
<li>构造了掩码，用以加速当 shift window 产生大小不同的 Windows 时的额外的 padding 计算</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/swin_transformer/image-20220926080127210.png" alt="image-20220926080127210"></p>
<p>&emsp;&emsp;由于 swinTransformer 提出的是一种通用架构，所以这里的网络模型并没有给出具体到分类 / 分割 / 检测应该如何继续实现，事实上直接按照常见的实现方式，分类用 1x1 Conv，分割用 Unet，检测用 pyramid feature。这里的输入是每个 window 的输入，每个 Swin Transformer Block 都会产生一个输出，Patch Merging 是下采样操作，类似于 pixel shuffle，接下来按照前向过程逐步分析一下网络的实现。</p>
<hr>
<h4 id="patch-embedding"><a href="#patch-embedding" class="headerlink" title="patch embedding"></a>patch embedding</h4><p>&emsp;&emsp;这一部分主要是预处理，将图片打成不同的 patch，并且把 patch 拉平，作用和 VIT 的一样，但是由于 swinTransformer 将其进一步分成了不同的 window，因此略有细节上的不同。</p>
<h5 id="patch-partition"><a href="#patch-partition" class="headerlink" title="patch partition"></a>patch partition</h5><p>&emsp;&emsp;对于输入的图像 image $H\times W$（Swin-T 中图像大小为 224x224），按照每个 patch 为  $P\times P$  切分为多个 patch，在标准 swinTransformer 中，$P=4$。此时每个 patch 中有 $P\times P\times 3$ 个值，Swin-T 中这里为 48,。因此共有 $\frac{H}{4}\times \frac{W}{4}$ 个 patch，每个 patch 48 个值。</p>
<h5 id="linear-embedding"><a href="#linear-embedding" class="headerlink" title="linear embedding"></a>linear embedding</h5><p>&emsp;&emsp;接下来和 VIT 一致，通过一个 linear layer 将 48 映射到 transformer 接受的维度 $C$，在 Swin-T 中为 96，此时我们拿到了 56x56 个 patch，每个 patch 96 维，也即 $\frac{H}{4}\times \frac{W}{4}\times C$。  </p>
<hr>
<h4 id="Swin-Transformer-block"><a href="#Swin-Transformer-block" class="headerlink" title="Swin Transformer block"></a>Swin Transformer block</h4><p>&emsp;&emsp;这里的部分是整个工作最重要的部分，在这里讲述了窗口自注意力机制的计算方式，以及如何进行 shift window，包括之后的移位回位以及掩码设置。整体来说，对于上一个输入 $Z^{i-1}$，需要经过 $\rm LayerNorm, WMSA,Add,SWMSA,MLP$ 这些操作才能得到 $Z^{i+1}$。也就是每一个 swinTransformerBlock 需要做两次自注意力的计算。具体地，这个过程可以用如下公式表示，其中 $\hat Z$ 是中间变量：</p>
<script type="math/tex; mode=display">
\hat Z^{i}=W\_MSA(LN(Z^{i-1}))+Z^{i-1}</script><script type="math/tex; mode=display">
Z^i=MLP(LN(\hat Z^i))+\hat Z^i</script><script type="math/tex; mode=display">
\hat Z^{i+1}=SW\_MSA(LN(Z^i))+Z^i</script><script type="math/tex; mode=display">
Z^{i+1}=MLP(LN(\hat Z^{i+1}))+\hat Z^{i+1}</script><h5 id="W-MSA"><a href="#W-MSA" class="headerlink" title="W-MSA"></a>W-MSA</h5><p>&emsp;&emsp;这是整个 swinTransformer 里面最核心的部分，后面的 shift window 只不过是多加了一个移位的操作后处理了一下额外计算量。回到之前我们得到的输入，这时候我们拿到的输入维度是 $\frac{H}{4}\times \frac{W}{4}\times C$，在 Swin-T 中，这里也达到了 3136x96 的级别，而 3136 的序列长度对于 transformer 来说，复杂度太高，因此将其切分成 window。</p>
<p>&emsp;&emsp;在切分 window 的时候，每个 window 有固定 7x7=49 个 patch，这 49 个 patch 线性排列，形成 $\frac{H}{28}\times \frac{W}{28}$ 个 window，每个 window 序列维度为 $(49,C)$ ，接下来对每个 window 做 self-attention。这里的 SA 操作在一些实现细节上略有区别于 VIT 和 标准transformer，首先是 $Q,K,V$ 的维度，这里的 $W_{Q/K/V}$ 矩阵维度为 $(C,\frac C {heads})$，因此，得到的 $Z\to (49, \frac C {heads}),Z_{mix}\to (49,C)$。这样得到的 $Z_{mix}$ 再和 VIT 一样经过一个 linear，不过这里的 linear 是 $C\to C$ 的，其余操作和 VIT 无异。这样输出的结果维度仍然为 $\frac{H}{4}\times \frac{W}{4}\times C$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">WindowAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.</span></span><br><span class="line"><span class="string">    It supports both of shifted and non-shifted window.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        window_size (tuple[int]): The height and width of the window.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0</span></span><br><span class="line"><span class="string">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, window_size, num_heads, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.window_size = window_size  <span class="comment"># Wh, Ww</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">        self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">            torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment"># 2*Wh-1 * 2*Ww-1, nH</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">        coords_h = torch.arange(self.window_size[<span class="number">0</span>])</span><br><span class="line">        coords_w = torch.arange(self.window_size[<span class="number">1</span>])</span><br><span class="line">        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  <span class="comment"># 2, Wh, Ww</span></span><br><span class="line">        coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># 2, Wh*Ww</span></span><br><span class="line">        relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># 2, Wh*Ww, Wh*Ww</span></span><br><span class="line">        relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># Wh*Ww, Wh*Ww, 2</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] += self.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">        relative_coords[:, :, <span class="number">1</span>] += self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># Wh*Ww, Wh*Ww</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">        trunc_normal_(self.relative_position_bias_table, std=<span class="number">.02</span>)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input features with shape of (num_windows*B, N, C)</span></span><br><span class="line"><span class="string">            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        B_, N, C = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(B_, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        q = q * self.scale</span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">            self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># Wh*Ww,Wh*Ww,nH</span></span><br><span class="line">        relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># nH, Wh*Ww, Wh*Ww</span></span><br><span class="line">        attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nW = mask.shape[<span class="number">0</span>]</span><br><span class="line">            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">            attn = attn.view(-<span class="number">1</span>, self.num_heads, N, N)</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line"></span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h5 id="SW-MSA"><a href="#SW-MSA" class="headerlink" title="SW-MSA"></a>SW-MSA</h5><p>&emsp;&emsp;shift window multi-head self attention 是这项工作最创新的地方，事实上，如果删除了 shift window，这项工作在发表之前很短时间恰好有一篇 PVT 的工作通过堆叠不重合窗口实现大图像 transformer。当然，swinTransformer 具备更全的应用范围，更优的实验设计，仍然会成为一项颠覆性的工作。</p>
<p>&emsp;&emsp;整体来说，SW-MSA 首先进行窗口的移动，准确来说是分割线移动 1/2 个 window_size（Swin-T 中是7/2=3），这样会产生与之前不同的窗口，并且新窗口的范围和旧窗口范围有部分重叠，然后在新窗口上做 W-MSA即可，但是这样的话会产生更多的窗口，并且会产生非 7x7 的窗口，一种产生这种 “不规整情况” 的示意图如下：</p>
<p><img src="/swin_transformer/image-20220926130957112.png" alt="image-20220926130957112" style="zoom:50%;"></p>
<p>&emsp;&emsp;这种不规则将会给批处理图像带来极大的困难，一种普通的想法是采用 padding 策略，对不同的 window 都补 0 成 7x7，但这样增加了很多计算量。swinTransformer 没有选择采用 padding 策略修补这些窗口，而是采用了一种巧妙的移位。一个完整的 SW-MSA 工作流程如下：</p>
<p><img src="/swin_transformer/image-20220926131342976.png" alt="image-20220926131342976"></p>
<p>&emsp;&emsp;在这个过程中，cyclic shift 和 reverse cyclic shift 操作直接采用数组切片即可完成，这里主要难以理解的是 masked MSA，即为什么要用掩码，以及怎么赋值掩码矩阵？</p>
<p>&emsp;&emsp;至于为什么要用到掩码，这是因为在 cyclic shift 形成的新四个窗口中，对于左下、右上、右下的窗口，他们都含有图像不相邻位置的像素，这些像素可能实际上没有任何关联，如左下角的上半部分可能是草原，而其下半部分可能是上面移过来的天空。这种情况下他们是不应该互相进行计算注意力的 ，然而我们只要用 W-MSA 就一定会计算他们之间的注意力，所以我们需要给一个 mask，让我们不需要计算的部分消失掉。</p>
<p>&emsp;&emsp;对于这四种情况，在进行 SA 之后会得到四种不同的值分布，如下图所示：</p>
<p><img src="/swin_transformer/image-20220926132139506.png" alt="image-20220926132139506" style="zoom:50%;"></p>
<p>&emsp;&emsp;在右边这几幅小图中，赋 0 的位置表示我们希望保留的计算，赋 -100 的位置表示我们希望忽略的计算，这些 mask 将会作用于 $\rm softmax(\frac{QK^T}{\sqrt d_k} + mask)$，此时 -100 将会经过 $\rm softmax$ 变成 0，从而消除其掩码所在位置的影响。</p>
<h5 id="Add-amp-LayerNorm-amp-MLP"><a href="#Add-amp-LayerNorm-amp-MLP" class="headerlink" title="Add &amp; LayerNorm &amp; MLP"></a>Add &amp; LayerNorm &amp; MLP</h5><p>&emsp;&emsp;这些无论和 VIT 还是和 transformer 都没有任何区别，不再赘述。</p>
<hr>
<h4 id="patch-merging"><a href="#patch-merging" class="headerlink" title="patch merging"></a>patch merging</h4><p>&emsp;&emsp;这里的 patch merging 就是一个单纯的降采样的过程，其操作和 pixel shuffle 基本一致，其具体的方法和操作流程如下图：</p>
<p><img src="/swin_transformer/image-20220926132919270.png" alt="image-20220926132919270" style="zoom:67%;"></p>
<p>&emsp;&emsp;我们可以看到，patch merging 将原本 4x4x1 的 tensor 变成了 2x2x2 的 tensor，具体来说使长宽小一半，通道加一倍，从而达到降采样的目的，swinTransformer 中总共进行了 3 次 patch merging，将 $\frac{H}{4}\times \frac{W}{4}\times C$ 最终变成了 $\frac{H}{32}\times \frac{W}{32}\times 8C$。这样的操作使感受野逐渐增大，从而感知不同尺寸的图片信息，更广泛地应用于更多任务。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Patch Merging Layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_resolution (tuple[int]): Resolution of input feature.</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_resolution = input_resolution</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        H, W = self.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f&quot;x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even.&quot;</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>&emsp;&emsp;swinTransformer 在多个应用领域均取得了超越 SOTA 的指标，由于其在多个领域都做了应用，因此根据不同的实现目标选择了不同的损失函数。</p>
<hr>
<h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3><p>&emsp;&emsp;总而言之，swinTransformer 相对于 VIT 来说，进一步激发了 VIT 在视觉领域的潜力，将其扩展到了更多的任务，并在各种任务上面都成为了 SOTA，对 VIT 的改编无疑是巨大的成功，目前基于 CNN 的 SOTA 已经很难找到了。但这项工作同时有着一定的局限性，由于其是 VIT 在视觉领域的改编，其借鉴了很多 CV 的先验知识，尤其是很多之前 CNN 架构的处理方法，这意味着 transformer 在 NLP 到 CV 领域的迁移还没有完全性地实现，如何像 VIT 一样，在几乎不对网络结构做 CV/NLP 方向的特殊适应的情况下，直接修改数据结构就可以一个网络做多个任务，这是 swinTransformer 的团队挖出的一个大坑，也许会有下一个颠覆性的工作完成这项预期。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>videoGPT</title>
    <url>/videoGPT/</url>
    <content><![CDATA[<h2><center> VideoGPT: Video Generation using VQ-V AE and Transformers </center></h2>

<p>【视频生成】【arXiv2021】【<a href="http://arxiv.org/abs/2104.10157">paper</a>】【<a href="https://wilson1yan. github.io/videogpt/index.html">code</a>】</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;本文使用 VQVAE 作为整体结构，第一阶段编解码器为 Conv3D，不使用 GAN 策略进行训练。同时采用 iGPT 作为第二阶段的 transformer 结构，直接对 video 进行编码，同时提出了一些训练 VQVAE 的经验，总结来说或许是趁着 iGPT 出了一个 paper，整体视觉效果和现在已经完全比不上了。</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/videoGPT/image-20230411194600022.png" alt="image-20230411194600022"></p>
<span id="more"></span>
<hr>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><ul>
<li>使用带有时间注意力和位置编码的 Conv3D 结构作为 VQVAE 第一阶段的编解码器</li>
<li>采用 iGPT 作为第二阶段的 transformer 结构</li>
</ul>
<h3 id="tricks"><a href="#tricks" class="headerlink" title="tricks"></a>tricks</h3><ul>
<li>使用 EMA 更新 codebook 可以更快地收敛（经验发现）</li>
<li>训练之前将所有的图像数据进行归一化（经验发现）</li>
<li>在第一阶段训练时使用编码器输出随机复制重启 codebook（参考了20 年的 Jukebox）</li>
<li>计算 loss 之前首先对 codebook 进行 l2norm（和 improved VQGAN 思路一致）</li>
</ul>
<h3 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h3><p><img src="/videoGPT/image-20230411195246226.png" alt="image-20230411195246226"></p>
<p>&emsp;&emsp;本文的前向过程非常简单，整体来说完全参考 VQVAE 的设计，第一阶段将编解码器改为自己设计的 Res3D 结构，直接对 video 进行编码，第二阶段采用 iGPT 作为 transformer。</p>
<p>&emsp;&emsp;对于第一阶段的编解码器，其架构由一系列三维卷积组成，在时空上进行下采样，然后是注意力残留块，具体来说如下图所示，其中的 Axial Attention 参考 <a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_CCNet_Criss-Cross_Attention_for_Semantic_Segmentation_ICCV_2019_paper.html">Ccnet</a>。对应的，解码器完全反向编码器。</p>
<p><img src="/videoGPT/image-20230411195447722.png" alt="image-20230411195447722" style="zoom: 50%;"></p>
<p>&emsp;&emsp;对于第二阶段的 iGPT，其实目前常用的自回归架构基本和 GPT2 没啥区别，只不过需要区分 BERT，而这篇文章只提到自己沿用了 iGPT 的结构，但是 iGPT 在 GPT2 和 BERT 的两种做法（最大似然后一个与中间一个）都做了探索，不确定这篇文章用的什么（有 code，但是感觉没必要看）。</p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>&emsp;&emsp;也就是普通的 VQVAE 训练时候的 loss，效果不好是意料之中的，根据经验，在不经过 GAN 对抗的情况下，仅依靠 MAE/PSNR 等指标进行优化会导致重建更加趋向平滑。</p>
<p><img src="/videoGPT/image-20230411200043026.png" alt="image-20230411200043026" style="zoom: 47%;"></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>&emsp;&emsp;在不同数据集上的 FVD：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><img src="/videoGPT/image-20230411200338418.png" alt="image-20230411200338418" style="zoom:50%;"></th>
<th><img src="/videoGPT/image-20230411200400893.png" alt="image-20230411200400893" style="zoom:50%;"></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;目前看来糊的离谱的 video：</p>
<p><img src="/videoGPT/image-20230411200534849.png" alt="image-20230411200534849"></p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
      </categories>
  </entry>
  <entry>
    <title>video_transformers</title>
    <url>/video_transformers/</url>
    <content><![CDATA[<h2><center>视频理解综述-从 I3D 到 video transformers</center></h2>

<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>&emsp;&emsp;这篇文章将会从 I3D 接手记录近年来在视频理解领域的各个 SOTA。所有内容包括 3DCNN 和 transformer 两个大类，具体来说包含如下论文：</p>
<script type="math/tex; mode=display">
\rm 3DCNN: I3D\to  Non-local\to R(2+1)D\to SlowFast</script><script type="math/tex; mode=display">
\rm Transformer:VTN\to Timesformer\to  MViTv1/2\to VideoSwinT</script><span id="more"></span>
<p>&emsp;&emsp;在 I3D 发布之后，I3D 迅速成为了视频理解的 SOTA，其提出的结构具备易训练、效果好等特点。在此之后 3DCNN 开始了大流行，基本完全代替了 2DCNN+RNN 的位置，但是仍然存在两个问题：1.使用 I3D 的网络必须以一个 2D 网络为 backbone，2.仍然依赖光流。因此接下来的一部分工作仍然致力于尝试从头训练 3D 网络以及试着剔除光流。</p>
<p>&emsp;&emsp;为了能够从头训练 3D 网络，受到 Non-local-mean 算子的启发，FAIR 在网络中引入了 non-local 算子，成功在不使用 3DConv 的情况下获得了比 I3D 更好的结果。当然，在 I3D 中加入 non-local 将会更好。同年，为了降低 3D 网络的参数，FAIR 提出了一种拆分 3D 卷积核的方法，即先做空间卷积再做时间卷积，也就是 R(2+1)D，这项工作显著降低了 3D 网络的训练难度，可以从头训练 3D 结构，但是效果仅和 I3D 持平。在这之后，FAIR 借鉴人眼细胞中观察静态和动态视频的细胞数目，摒弃光流，将视频帧按照不同的步长截取 clip，分别进入不同参数量的网络，得到了 SlowFast，同时加上 NL 操作，在 K400/600 上全面超过了 I3D。</p>
<p>&emsp;&emsp;此时时间已经到达了 2021 年，随着 VIT 的提出，将 VIT 向视频领域的迁移完全盖过了 3DCNN 的风头，theator 首先提出了 VTN，以一个 CNN 结构抽取特征，后接 transformer 结构进行时间注意力建模，以 VIT-B 为backbone 的结构在 K400 上超过了 SlowFast。同年，借助 R(2+1)D 的思路，FAIR 又提出了 Timesformer，分别对时空做自注意力，显著降低了 3D-SA 的复杂度，将 K400 的 top-1 分数刷到了 80+。同时 Google Research 提出了 ViViT，开发出了独立的时空注意力模块（其中的 SOTA 方法和 Timesformer 一样的，就别写了，JFT 效果好和我等屁民也无关），以小尺寸的输入和 IN-21K 的预训练超了 Timesformer，同时在 JFT 预训练上拉到了 83+，一骑绝尘。片刻之后，FAIR 又提出了以多尺度图像信息作为输入的 MViT，在 K400 上超了 Timesformer，拉平了在 IN-21K 上做预训练的 ViViT，毕竟 MViT 没有做预训练。时至今日，MSRA 在 swinTransformer 的基础上做出了 VideoSwinTransformer，在 IN-21K 上预训练，超过了 ViViT，在 K400 的 top-1 上得到了 84.9 的准确率。</p>
<p>&emsp;&emsp;FAIR 🐮🍺</p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="/video_transformers/image-20221010173024409.png" alt="image-20221010173024409"></p>
<hr>
<h3 id="Non-local"><a href="#Non-local" class="headerlink" title="Non-local"></a>Non-local</h3><p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf">Non-local Neural Networks</a> 【视频动作识别】 CVPR2018</p>
<h4 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h4><ul>
<li>提出了一种可以接入在 2D/3D 网络结构任意位置的模块 non-local block（NL）</li>
<li>NL 可以计算全局的位置信息，但区别于 DNN，更接近 SA</li>
</ul>
<h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>&emsp;&emsp;这项工作选取了 Res50 和 Res101 作为 backbone，分别在 C2D 和 I3D 上不同位置加入了 NL，区别于原本的双流 I3D，取消了光流的输入，以在 Res50 C2D 版本的 res2/3 中加入 5 个 NL 来说，大致网络结构如下：</p>
<p><img src="/video_transformers/res50_NL_5.png" alt></p>
<p>&emsp;&emsp;如果除去蓝色小块，剩下的就是标准的 Res50，对于不同的 NL 个数：1/5/10，全部在 stage2/3 之中加入，或隔一个加一个，或全部后接 NL。这样做的原因是：1.NL 操作复杂度较高，直接在 stage1 后接，会导致训练变慢，2.在 stage4 后接的话，此时的特征图已经太小，并且这个时候的感受野已经很大了，再接入 NL 意义不大。具体到某一个 NL_block，其具体的计算方式如下：</p>
<p><img src="/video_transformers/image-20221011115043135.png" alt="image-20221011115043135"></p>
<p>&emsp;&emsp;看上去是很复杂的，其实就是带残差连接的 SA。上面的这个实现可以直接复现出源码，在论文中，作者也给出了抽象的 NL_block 的实现方式：</p>
<p><img src="/video_transformers/image-20221011115228865.png" alt="image-20221011115228865"></p>
<p>&emsp;&emsp;这个实现方式也十分直观，但在实现的源码中，这里的  embedding 之后并不一定是 512 维，而是可变的。除此之外，无论是 2D/3D 的卷积层得到的输出 (b,c,h,w)/(b,c,d,h,w) 都可以直接进入 NL_block，输出的维度和原本一致，由于输入的 x 自带 shape 信息，因此真的完全不需要任何其他的输入，你只需要指定你想要的 embedding_dim 就可以了。</p>
<p>&emsp;&emsp;除了如何在代码意义上实现 NL，作者自然给出了具体的数学表达式：</p>
<script type="math/tex; mode=display">
y_i = \frac 1 {C( x)} \sum_{\forall j} f( {x_i},{x_j})g({x_i})\tag1</script><p>&emsp;&emsp;其中，$ x$ 表示信号，一般就是特征图，$i$ 表示时空位置索引，$f$ 计算 $ {x_i , x_j}$ 的相似度，$g$ 计算特征图在 $j$ 位置的表示，并且通过 $C(x)$ 进行标准化处理。写得挺好就是有点不好，看不太懂，举个例子就会好一些：对于输入的 $x$，在上图的计算方式（也就是 embedded Gaussion）下，出现了 $\theta,\phi,g$ 三个函数，其中 $g$ 就对应公式中的 $g$ 函数，而公式中的 $f$ 函数对应 $e^{\theta(x_i)^T\phi(x_j)}$，$C(x)=\Sigma_{\forall j}f(x_i, x_j)$，如果我们以此重写方程 $(1)$，将会变为：</p>
<script type="math/tex; mode=display">
y_i=\frac{1}{\Sigma_{\forall j}e^{\theta(x_i)^T\phi(x_j)}}e^{\theta(x_i)^T\phi(x_j)}g(x) \tag 2</script><p>&emsp;&emsp;并且，$\theta,\phi,g$ 也就是对应的 $Q,K,V$，具体来说有：</p>
<script type="math/tex; mode=display">
\theta(x)=W_{\theta}\times x</script><script type="math/tex; mode=display">
\phi(x) = W_\phi \times x</script><script type="math/tex; mode=display">
g(x)=W_g\times x</script><p>&emsp;&emsp;显然，对于 方程 $(2)$ 的前半部分，正是 $\rm softmax(Q^TK)$ 部分，这其实就是 SA 的一种简单的形式化表示，相对于标准 SA，缺少了对 $\sqrt{d_k}$ 的放缩，多了残差连接，是很小的改动。并且作者同样提出了很多种其他的 $f$ 形式，表明了 $\rm softmax$ 操作实际上在 NL 里并不是重要的，直接使用 $f(x) = \theta(x_i)^T\phi(x_j),C(x)=N$ 也可以取得很好的结果。</p>
<h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>&emsp;&emsp;对于结果，当然是很棒的，有着很高的 K400 得分，并且在 COCO 上 2D 分类也能通过 NL 提 1 个点左右。除此之外，作者做了很多的实验，分别探索了以 Res50/Res101 的 C2D/I3D 形式下，加入1/5/10 个 NL，采用Gaussian、Gaussian embed、dot-product、concatenation 的 NL 算法，在 K400 上测试了多组结果。在很大程度上超过了原始的 I3D。</p>
<p>&emsp;&emsp;最令人惊喜的是，对于 NL，我们可以仅使用 RGB 输入而舍弃光流，在仅使用 RGB 的情况下仍然超过了使用双流的 I3D，虽然 NL 也不是很廉价的操作，但是对比于光流已经改进了不少。</p>
<p><img src="/video_transformers/image-20221011124534878.png" alt="image-20221011124534878"></p>
<p>&emsp;&emsp;图中标灰的使用了 3 模态，相比于单模态的 NL I3D 优势太大，尽管如此也没有超过 NL I3D。因为此时的 I3D 还没有发以 ResNet 为 backbone 的结果，因此只比较了以 inception 为 backbone 的 I3D。</p>
<hr>
<h3 id="R-2-1-D"><a href="#R-2-1-D" class="headerlink" title="R(2+1)D"></a>R(2+1)D</h3><p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Tran_A_Closer_Look_CVPR_2018_paper.pdf">A Closer Look at Spatiotemporal Convolutions for Action Recognition</a> 【视频动作识别】 CVPR2018</p>
<h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h4><p>&emsp;&emsp;在视频理解逐渐火热的时候，出现了一个尴尬的问题：使用 res151 在 sports-1M 上逐帧输入的情况下仍然得到了接近 SOTA 的结果。也就是即使不使用大家普遍认为的关键点，即多帧信息，仍然是很好的结果。</p>
<p>&emsp;&emsp;因此 FAIR 做了这个工作，说白了，就是将 3DCNN 拆分成对于空间的 2DCNN 和 时间上的 1DCNN，通过多方对比，证实了 3DCNN 还是有效的，并且以同样的参数取得了相比 3DCNN 更好的效果，是个大型消融实验现场，同样的还有 ConvNext，挖坑等填。</p>
<h4 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h4><p><img src="/video_transformers/image-20221018101252155.png" alt="image-20221018101252155"></p>
<p>&emsp;&emsp;本文总共尝试了 5 种网络结构，分别从 2D，先 3D 后 2D，先 2D 后 3D，全 3D，全 R(2+1)D 五个方面做了测试，并且在输入帧的层次上尝试了 8 帧和 16 帧的输入，并且对比了 RGB 和 FLOW 以及两者结合的输入方式。在结果上，对比了 I3D 在 UCF-101，K400，sports-1M 上的结果，离谱的是完全没打过 I3D（打不过 I3D 不离谱，毕竟 I3D 巧夺天工，作者圆的很离谱，属于浑身上下只有嘴是硬的）</p>
<p>&emsp;&emsp;需要注意的时候，这些所有的网络使用的都是具有 5 个 stage 的 ResNet，具体来说，包括 res18 和 res34。</p>
<h5 id="R2D-＆-f-R2D-amp-R3D"><a href="#R2D-＆-f-R2D-amp-R3D" class="headerlink" title="R2D ＆ f-R2D &amp; R3D"></a>R2D ＆ f-R2D &amp; R3D</h5><p>&emsp;&emsp;对于 2DCNN，有两种处理方式：1.将多个帧进行堆叠，对于 RGB 通道的图像，堆叠 L 帧，得到的输入通道就是 3L，接下来按照普通 2DCNN 做就行，2.将多个帧逐个输入，得到多个帧的独立特征图，然后经过顶部池化进行融合，其实没啥区别，R2D 就相当于在 f-R2D 的池化前面加一个 1x1Conv。</p>
<p>&emsp;&emsp;至于 3DCNN，本文表示：使用经典的 3DCNN 网络，相当单纯，甚至对标的是 2015 年的 paper。</p>
<h5 id="MCx-amp-rMCx"><a href="#MCx-amp-rMCx" class="headerlink" title="MCx &amp; rMCx"></a>MCx &amp; rMCx</h5><p>&emsp;&emsp;由于有这么一种说法：运动建模（即 3D 卷积）可能在早期层中特别有用，而在更高级别的语义抽象（晚期层）中，运动或时间建模不是必需的。因此本文探索了 (r)MC2-5 共 8 中结构。</p>
<h5 id="R-2-1-D-1"><a href="#R-2-1-D-1" class="headerlink" title="R(2+1)D"></a>R(2+1)D</h5><p><img src="/video_transformers/image-20221018104815582.png" alt="image-20221018104815582"></p>
<p>&emsp;&emsp;所谓的 R(2+1)D 其实只是将一个 txdxd 的 3D 卷积块拆分成先进行 1xdxd 的 2D 空间卷积，再进行 tx1x1 的时间卷积，在两者中间按照惯例加入一个 ReLU。在保证输入和输出的通道、维度一样的情况下，对于 R(2+1)D 实际上多了一个参数，就是选取的 2D 卷积核的个数。作者所谓“为了公平比较”，将通道数通过一个计算公式得出，保证 R(2+1)D 的参数量大约等于同层数的 3DCNN。</p>
<p>&emsp;&emsp;这种结构有两个好处：1.由于 ReLU 的非线性，整个网络的非线性程度将会扩大一倍。2.这种拆分的形式能够能加有利于网络的训练（第 2 条纯粹是结果导向的结论罢了）</p>
<h5 id="参数量-帧数-准确度-对比"><a href="#参数量-帧数-准确度-对比" class="headerlink" title="参数量-帧数-准确度 对比"></a>参数量-帧数-准确度 对比</h5><p><img src="/video_transformers/image-20221018111403788.png" alt="image-20221018111403788" style="zoom:50%;"></p>
<p>&emsp;&emsp;可以看出​：1.3DCNN 真的很大，效果比 2D 确实好了一点点。2.R(2+1)D 效果确实比 3D 好了一点点。3.多帧输入确实比单帧好一点点。</p>
<p><img src="/video_transformers/image-20221018111833490.png" alt="image-20221018111833490"></p>
<p>&emsp;&emsp;这张图显示出的效果其实也没啥太大冲击力，比较对比的是 15 年的老方法，也没有太明显的超越。不过还是能看出 R(2+1)D 有些小优势，更深的 res34 相比于 res18 也更好一些。</p>
<h4 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h4><p>&emsp;&emsp;整体来说反正不如 I3D，也没啥好看的，但是有意思的来了嗷，在 UCF-101 和 HMDB51 上验证时，有这张图：</p>
<p><img src="/video_transformers/image-20221018112811765.png" alt="image-20221018112811765"></p>
<p>&emsp;&emsp;这张图中呢，显然 R(2+1)D 是不如 I3D 的，但是作者解释为：</p>
<p><img src="/video_transformers/image-20221018112538340.png" alt="image-20221018112538340"></p>
<p>&emsp;&emsp;他说自己不如人家 I3D，但是 I3D 用了 ImageNet 预训练，话里话外就是说 I3D 如果不用 ImageNet 预训练就不如自己呗，但是事实并非如此捏：</p>
<p><img src="/video_transformers/image-20221018112733777.png" alt="image-20221018112733777"></p>
<p>&emsp;&emsp;关于《 I3D 如果不用 ImageNet 预训练就不如 R(2+1)D》，不能说是有理有据，基本也是信口雌黄。当然，R(2+1)D 在思路上是很棒的，之后也有很多工作基于 R(2+1)D 开展，取得了比 I3D 更好的结果，只是这张图和解释，真是老太太钻被窝了属于是👴。</p>
<hr>
<h3 id="SlowFast"><a href="#SlowFast" class="headerlink" title="SlowFast"></a>SlowFast</h3><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.pdf">SlowFast Networks for Video Recognition</a> 【视频动作识别】 ICCV2019</p>
<h4 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h4><p>&emsp;&emsp;在动物识别视觉信息时，有观察静态帧和动态帧的两种细胞，举例来说，对于 DOTA，静态帧即不会动的地图，动态帧即时刻在变化的英雄和小兵。并且这两类细胞有着一定的比例，基本上动态帧细胞占据全部数量的 20% 左右。基于这种直觉，FAIR 提出了两个同时进行数据帧的采样和卷积的网络结构 SlowFast，其总共有两个网络，是一种通用的网络结构，适用于多个 backbone，一个网络是 SLOW 网络，以较低的帧采样率通过较多参数的网络，另一个 FAST 网络则以较高的帧采样率通过较少参数的网络，网络之中进行横向连接。</p>
<h4 id="创新-1"><a href="#创新-1" class="headerlink" title="创新"></a>创新</h4><ul>
<li>网络有两条互相补充的路径分别在高低时间分辨率下工作</li>
<li><p>FAST 分支中通道数较少，SLOW 分支中通道数较多</p>
</li>
<li><p>✔️用了细胞生物学的方式启发网络设计✔️</p>
</li>
</ul>
<h4 id="网络-1"><a href="#网络-1" class="headerlink" title="网络"></a>网络</h4><p><img src="/video_transformers/image-20221018143019767.png" alt="image-20221018143019767" style="zoom:60%;"></p>
<p>&emsp;&emsp;整体来说，这个网络可以适配多种 backbone，在本文中采用了 3D Res50 网络结构，两个网络分别采样，并且从 FAST 分支向 SLOW 分支进行横向连接，需要注意的是，SlowFast 在进行 3D 卷积时不进行任何时间方向的下采样。 </p>
<h5 id="SLOW-分支"><a href="#SLOW-分支" class="headerlink" title="SLOW 分支"></a>SLOW 分支</h5><p>&emsp;&emsp;对于 SLOW 分支，其采样率更低，具体来说是每 16 帧取 1 帧，最终得到 $T$ 个帧。其参数量更大，具体体现于卷积的过程中通道数更多，初始每个帧 3 通道，之后的特征图通道数用 $C$ 表示，且 $C$ 在不断变化。初始的图像大小为 $H\times W$ ，$H=W=224$。</p>
<h5 id="FAST-分支"><a href="#FAST-分支" class="headerlink" title="FAST 分支"></a>FAST 分支</h5><p>&emsp;&emsp;对于 FAST 分支，其具备更高的采样率，这里的高采样率相比于 SLOW 分支进行讨论，具体来说，是 SLOW 分支采样率的 $\alpha$ 倍（$\alpha &gt; 1 $，默认为 8），即最终采样得到 $\alpha T$ 帧。同时其具备更小的参数量，具体体现在于卷积的过程中参数量较少，即相对于 SLOW 分支，每个对应阶段的通道数都是其 $\beta$ 倍（$\beta &lt;1$，默认为 $\frac 1 8$），即每个对应阶段的通道数均为 $\beta C$。$H,W$ 和 SLOW 一致。</p>
<h5 id="横向连接"><a href="#横向连接" class="headerlink" title="横向连接"></a>横向连接</h5><p>&emsp;&emsp;由于在生物体内，动态细胞和静态细胞之间是信息共享的，因此在每个阶段结束之后理论上都需要将两个分支的信息共享给对方，但是这里作者通过消融实验证明了：从 SLOW 分支向 FAST 分支共享信息是没有提升的，因此只通过 FAST 分支向 SLOW 分支共享信息即可。</p>
<p>&emsp;&emsp;由于 FAST 分支的每个阶段的 $\rm shape=(N,\alpha T,\beta C,H,W)$，而对于对应的 SLOW 分支，每个阶段的 $\rm shape=(N,T,C,H,W)$，因此需要一个映射使他们 $\rm shape$ 一致，作者提出了三个可能的方法：</p>
<ul>
<li>直接进行 reshape 操作，将 $\rm (N,\alpha T,\beta C,H,W)\to (N,T,\alpha\beta C,H,W)$</li>
<li>每 $\alpha$ 个帧抽取 1 个通道进行融合，得到 $\rm (N,T,\beta C,H,W)$</li>
<li>通过一个 3D 卷积层，$\rm Ks=(5,1,1),stride=(\alpha,1,1),padding=(2,0,0)$，通道数为 $2\beta C$ 得到 $\rm shape=(N,T,2\beta C,H,W)$</li>
</ul>
<p>&emsp;&emsp;对从 FAST 分支变来的新特征和 SLOW 的特征进行相加或级联（❓通道不同如何相加❓）</p>
<p>&emsp;&emsp;ANSWER：通过阅读<a href="https://github.dev/facebookresearch/SlowFast">代码</a>:slowfast/models/video_model_builder.py:149 可知，传参的时候有个 fusion_conv_channel_ratio 参数确定了输出到底是几倍的 $\beta C$，以及最终是直接级联 cat。</p>
<h5 id="实例参数"><a href="#实例参数" class="headerlink" title="实例参数"></a>实例参数</h5><p>&emsp;&emsp;作者给出了以 res50 为 backbone 的通道数、帧数、在不同阶段下的图像大小等参数信息，如下图：</p>
<p><img src="/video_transformers/image-20221018155442444.png" alt="image-20221018155442444" style="zoom:80%;"></p>
<h4 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h4><p><img src="/video_transformers/image-20221018155851588.png" alt="image-20221018155851588" style="zoom:60%;"></p>
<p>&emsp;&emsp;话不多说，真的无敌，特别是在 K400 上，就算是面对在 ImageNet 上预训练并且使用光流的 I3D，基本也是完虐，甚至不加 NL 都超之前的 NL R101 预训练模型，而这个 NL R101 其实是 KaiMing 的上一个 SOTA，在刷新记录这方面有种博尔特的感觉。</p>
<hr>
<h3 id="VTN"><a href="#VTN" class="headerlink" title="VTN"></a>VTN</h3><p><a href="https://openaccess.thecvf.com/content/ICCV2021W/CVEU/papers/Neimark_Video_Transformer_Network_ICCVW_2021_paper.pdf">Video Transformer Network</a> 【视频动作识别】 ICCV2021</p>
<p>⚠️⚠️⚠️⚠️⚠️</p>
<p>从这里开始一些工作出现于 2021 年，因此这些 2021 年的工作之间并没有互相比较，仅对比了 SlowFast</p>
<p>⚠️⚠️⚠️⚠️⚠️</p>
<h4 id="创新-2"><a href="#创新-2" class="headerlink" title="创新"></a>创新</h4><ul>
<li>使用 transformer 结构代替了 3DCNN，提出的网络对 full-video 更加友好</li>
<li>使用了 LongFormer 作为时间网络</li>
</ul>
<h4 id="网络-2"><a href="#网络-2" class="headerlink" title="网络"></a>网络</h4><p><img src="/video_transformers/image-20221019105212615.png" alt="image-20221019105212615" style="zoom:77%;"></p>
<p>&emsp;&emsp;这个网络结构图非常清晰，只需要简单的解释即可，但是⚠️这个图居然不是矢量图⚠️，离大谱，放大之后就糊的一批。这也能发 ICCV 🤷❓网络由三个部分组成：空间部分、时间部分、分类头，也就是分别对应 $f(x)$，Encoder，MLP。分别介绍这前两个部分，MLP很普通，不再赘述：</p>
<h5 id="空间网络"><a href="#空间网络" class="headerlink" title="空间网络"></a>空间网络</h5><p>&emsp;&emsp;这里的空间网络是一个通用的网络结构，这里可以是任何一个能够提取特征的网络作为 backbone，无论是 CNN 还是 transformer。本文中尝试了 Res50/101，VIT，DeiT 三种骨干网络作为空间网络，并且分别探索了是否预训练、以及是否冻结参数进行训练的差别。最终实际上各种选择差别有但不大，整体来说最好的是使用 VIT-B 作为 backbone 的网络，并且基于 ImageNet-21K 预训练，在训练的过程中进行 FT（符合直觉）。</p>
<h5 id="时间网络"><a href="#时间网络" class="headerlink" title="时间网络"></a>时间网络</h5><p>&emsp;&emsp;在空间网络提取出特征图之后，特征图首先合并 PE，再经过一个 Longformer 网络，同时通过加入 CLS 标志位的方法进行分类。同时作者探索了输入视频帧的跨度和数目问题</p>
<p>&emsp;&emsp;首先对于 PE，作者提出了三种不同的 PE 方式：1.使用可以学习的编码，以输入的索引作为输入学习一个可变的编码，2.使用固定的编码，具体参考 DERT，3.不使用任何位置编码。在作者的实验中，这些不同的编码方式在准确率上仅产生了不到 1% 的差别，其中最好的方式居然是不编码，并且 shuffle 之后的结果均更好。以结果论的方式前推，这可能是因为视频的多帧之间的先后顺序并不重要，倒放的视频表示的同样是一个动作，甚至乱序播放的视频也不影响其本身的语义。</p>
<p>&emsp;&emsp;对于 Longformer，其本质上是基于窗口的 transformer，是针对 NLP 领域提出的网络结构，目的也是解决 $O(n^2)$ 复杂度的问题，其相对于原本的注意力窗口提出了如下三种不同的窗口计算方式。对于 $(b)$，是指每个位置仅计算和自己左右各 $\frac 1 2 w$ 窗口内的位置的注意力（$w$ 是窗口大小），对于 $(c)$，即每个位置向左右各计算 $\frac 1 2 w$ 个位置的注意力，对于 $(d)$，这里表示的是带有 CLS 的情况，CLS 需要对全局进行建模，其他位置较多仅关注 local，少部分位置关注 non-local。</p>
<p><img src="/video_transformers/image-20221019173442348.png" alt="image-20221019173442348"></p>
<p>&emsp;&emsp;因此，Longformer 可以进行高复杂度的全局建模，并且更适配 CLS。在这里作者尝试了多层的 Longformer，有意思的是，在 1,3,6,12 层中几乎没有差别，甚至 1,3 层 MHA 的效果是最好的。具体的配置包括在不同情况下的 dropout 和 窗口大小、MHA 内隐空间维度详见论文。</p>
<p>&emsp;&emsp;由于 Longformer 可以进行全局性质的建模，相比于 3DCNN，自然有更好的全局信息能力，因此相比于传统的 multi-view 输入（即对小片段视频进行帧采样），Longformer 也许可以使用 full-video 输入（即对整个视频进行帧采样，自然帧率会更低），对于结果来说，所有的传统方法，full-video 都比 multi-view 效果明显降低，而对于 VTN，基本上 full-video 和 multi-view 完全一致，也没有什么提升。</p>
<p><img src="/video_transformers/image-20221019175825456.png" alt="image-20221019175825456" style="zoom:60%;"></p>
<h4 id="结果-3"><a href="#结果-3" class="headerlink" title="结果"></a>结果</h4><p>&emsp;&emsp;就本文来说，是对 transformer 简单迁移，基本就是组合了 VIT 和 Longformer，分别作为空间网络和时间网络，效果上平了 SlowFast（这个表上的 SlowFast 没有加 NL，加了 NL 之后的准确率是 79.8 &amp; 93.9），不过毕竟是 transformer，参数量大约是 SlowFast 的两倍。同时本文做了很多的消融实验和思考，包括长时间片段建模的好坏、不同位置编码方式的好坏、预训练后微调产生的优势等等，基本来说，作为第一批应用 transformer 到 video 的工作之一，探索的点还挺全的。</p>
<hr>
<h3 id="Timesformer"><a href="#Timesformer" class="headerlink" title="Timesformer"></a>Timesformer</h3><p><a href="https://arxiv.org/pdf/2102.05095.pdf">Is Space-Time Attention All You Need for Video Understanding?</a> 【视频动作识别】 ICML2021</p>
<h4 id="创新-3"><a href="#创新-3" class="headerlink" title="创新"></a>创新</h4><ul>
<li>使用 transformer 结构代替了 3DCNN，提出的网络对 full-video 更加友好</li>
<li>探索了多种时空组合的 3D VIT 形式</li>
</ul>
<h4 id="网络-3"><a href="#网络-3" class="headerlink" title="网络"></a>网络</h4><p>&emsp;&emsp;其实对于 Timesformer，因为这篇工作和 R(2+1)D 同为 FAIR 出品，甚至二作就是 R(2+1)D 的一作，因此风格和 R(2+1)D 非常像，基本上是实验性的工作，并且同样提了五种实验性的网络结构，甚至提网络也是同样的思路：3D 开销大？拆！</p>
<p><img src="/video_transformers/image-20221020162559210.png" alt="image-20221020162559210" style="zoom:70%;"></p>
<p>&emsp;&emsp;这五种网络结构实际上对应了五种不同的对于输入 clip 的注意力计算方式，作者为了更好地解释这五种注意力计算方式，还给出了下面这张图，下面这张图表明了某个 patch 在各种注意力计算方式下具体和其他 clip 中的哪些帧进行了注意力计算。该说不说，这个图画得挺直观的。</p>
<p><img src="/video_transformers/image-20221020163011342.png" alt="image-20221020163011342"></p>
<h5 id="S"><a href="#S" class="headerlink" title="S"></a>S</h5><p>&emsp;&emsp;这是一种 baseline 的计算方式，对于输入的 clip，就按照 VIT 的方式逐帧进行处理，在注意力图中，每个 patch 只和同属于同一个时间维度的 patch 计算注意力。这种计算方式开销和 VIT 一致，但是效果自然不会太好，实际上这里的效果在测试的时候在 K400 上展现出了还不错的效果，但是在 SSv2 上效果很差，这是因为 K400 中的动作并不太依赖时间跨度，而 SSv2 更依赖时间跨度。</p>
<h5 id="ST"><a href="#ST" class="headerlink" title="ST"></a>ST</h5><p>&emsp;&emsp;同上，这也是一种 baseline，只不过在这里按照 3DCNN 的方式进行简单的扩张，每个 patch 和当前 clip 中的所有 patch 都计算自注意力，这种计算方式理论上能够取得最好的效果，毕竟对时空全都做了注意力计算。就结果来说，ST 的计算方式确实和 T+S 一致取得了最好的结果。但是这种方式复杂度太高，对于稍微长一点的 clip 就会爆内存。</p>
<h5 id="T-S"><a href="#T-S" class="headerlink" title="T+S"></a>T+S</h5><p>&emsp;&emsp;这种方式就是 R(2+1)D，即先做时间维度上的注意力，再做空间维度上的注意力，在注意力可视化图中刻意看到，蓝色的 patch 首先和绿色的 patch 进行了自注意力操作（需要注意这里虽然只列出来了三个相邻帧，实际上是对 clip 内的所有帧都计算时间注意力的），接下来再和红色的 patch 进行自注意力计算。这种方式按照直觉也是最好的，因为平衡了时空注意力计算和计算复杂度。</p>
<h5 id="L-G"><a href="#L-G" class="headerlink" title="L+G"></a>L+G</h5><p>&emsp;&emsp;所谓 L+G，指的是 local+global，即对于全部时空 patch 都计算注意力开销太大，因此先计算对当前 patch 离得近的，这里只考虑空间的近，也就是说蓝色 patch 会计算和粉红色、黄色 patch 的注意力，然后再和紫色 patch 进行计算。这种方式相对于 ST 在某种程度上也降低了计算复杂度，但是在空间上的效果可能还不如 baseline，而众所周知，时间信息固然会加点，空间信息是更加重要的，因此结果并不好。</p>
<h5 id="T-W-H"><a href="#T-W-H" class="headerlink" title="T+W+H"></a>T+W+H</h5><p>&emsp;&emsp;即在三个维度上分别先后做自注意力，这样操作使得注意力关注的范围小了很多很多，虽然计算复杂度降下来了，但是对空间信息的过分不关注导致了降点。</p>
<h4 id="结果-4"><a href="#结果-4" class="headerlink" title="结果"></a>结果</h4><p><img src="/video_transformers/image-20221020164636754.png" alt="image-20221020164636754" style="zoom:80%;"></p>
<p>&emsp;&emsp;可以看出对于单帧自注意力，K400 上效果还不错，但是 SSv2 上效果很差，这是因为数据集之间的区别。至于 S+T 的效果最好是意料之中的。</p>
<p>&emsp;&emsp;像是所有的 transformer，因为更大的感受野，因此对于 full-video 的支持更好，同时和其他的 transformer 工作一样，Timesformer 也提出了 S-HR-L 多种参数形态，在最大参数最高帧率下，Timesformer 在 K400 达到了 80.9，算是超越了 SlowFast R101+NL。</p>
<hr>
<h3 id="MViTv1-2"><a href="#MViTv1-2" class="headerlink" title="MViTv1/2"></a>MViTv1/2</h3><p><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Fan_Multiscale_Vision_Transformers_ICCV_2021_paper.pdf">Multiscale Vision Transformers</a> 【视频动作识别】 ICCV2021 </p>
<p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.pdf">MViTv2: Improved Multiscale Vision Transformers for Classification and Detection</a> 【视频动作识别】 CVPR2022</p>
<h4 id="创新-4"><a href="#创新-4" class="headerlink" title="创新"></a>创新</h4><ul>
<li>MViT<ul>
<li>使用多尺度的方式处理图像，得到不同感受野的信息（此研究独立于 SwinT，同时实验表明这种方式能够更好地学到时间偏移）</li>
<li>通过扩展多阶段同时减少各阶段序列长度的方式，减少了很多计算量</li>
</ul>
</li>
<li>MViTv2<ul>
<li>对 MViT 进行了改进，加入了相对位置编码</li>
<li>为了消除池化操作产生的信息消失，加入了残差连接</li>
<li>将 MViT 应用于了基于 FPN 的 mask-RCNN</li>
</ul>
</li>
</ul>
<h4 id="网络-4"><a href="#网络-4" class="headerlink" title="网络"></a>网络</h4><ul>
<li>MViT</li>
</ul>
<p><img src="/video_transformers/image-20221024163455341.png" alt="image-20221024163455341" style="zoom:60%;"></p>
<h5 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h5><center>⚠️⚠️ 多次出现的池化可以选择 max / avg / conv，这里最终选择的是 conv⚠️⚠️</center>

<p>&emsp;&emsp;整个 MViT 的前向过程是基于 stage 的，也就是在不同的阶段有不同的深度和具体注意力计算的维度设置，对每个阶段，作者提出了一种新的计算多头自注意力的方式 MHPA（$\rm MultiHeadPoolingAttention$）。这个层会将输出逐层进行降采样，因此最终的输出比原输入的 shape 要小，类似 swinTransformer。</p>
<p>&emsp;&emsp;首先对于输入视频进行帧采样：得到 $(D,T,H,W)$，其中 $D$ 表示通道数，$T$ 表示帧采样数，然后打 patch 并加入 PE，这里的 PE 是绝对 PE，打 patch 的时候每个 patch size 为 4x4，这是因为这里的 patch 是 3D 的，太大了会导致计算复杂度较高。基本上就是 3D 版本 VIT，但是没有用学习位置编码，<strong>这其实是一个缺陷，因为这种位置编码破坏了图像的先验知识平移不变性，在 MViTv2 中得到改进</strong>。</p>
<p>&emsp;&emsp;接着通过多层的 MHPA+MLP，这里的 MHPA 会进行降采样，对于输入 $X\to(L,D)$ 会降低序列长度 $L$ 并且增大 $D$，因此通过 MHPA 之后的 $X’\to(\hat L,2D)$，之后再通过 MLP，多次循环 MHPA+MLP 即可得到最终输出，在多层的输出中，每层的 output shape 由下图给出。</p>
<p><img src="/video_transformers/image-20221024224948541.png" alt="image-20221024224948541" style="zoom:80%;"></p>
<p>&emsp;&emsp;这里稍微需要注意的是，由于每个 MHPA 都会降采样，而每一个 stage 都会执行多次 MHPA，因此实际上在执行时每个 stage 只有第一次执行 MHPA 的时候会进行降采样，其他的 MHPA 中的池化卷积核都是 1x1x1。</p>
<h5 id="MHPA"><a href="#MHPA" class="headerlink" title="MHPA"></a>MHPA</h5><p>&emsp;&emsp;MHPA 简单来说就是在计算出 $Q,K,V$ 之后加入一个池化层从而使其维度下降，具体来说 MHPA 如下图所示：</p>
<p><img src="/video_transformers/image-20221024172000599.png" alt="image-20221024172000599" style="zoom:70%;"></p>
<p>&emsp;&emsp;对于输入 $X\to(L,D),L=THW$，首先正常获得 $\hat Q,\hat K,\hat V\to(L,D)$，接着就是关键的池化操作，即将 $\hat Q,\hat K,\hat V$ 都经过一个 池化层进行降采样，计算公式很简单，即 $\hat L = [\frac{L-K+2P}{S}]+1$，于是得到的 $Q,K,V\to(\hat L,2D)$，具体来说对于 $\hat L$，其中的 $(\hat T,\hat H,\hat W)=(T,\frac H 2,\frac W 2)$，同时通道数扩大了二倍，这是遵循 CNN 的设计法则：当降采样到原 size 的 $\frac 1 4$ 时，通道应该扩大一倍。之后的计算和 VIT 保持一致，同时加入了残差连接，但是由于此时的输出已经变了，所以对 $X$ 也进行一个 pooling 再连上去。<strong>这里其实也是一个缺陷，因为对 $X$ 的池化显然会损失相当多的信息，残差连接的效果会变差，在 MViTv2 中得到改进</strong></p>
<p>&emsp;&emsp;需要注意的是池化本质的目的是降低整个 MHPA 之后的 shape，而计算过程中 $QK^TV\to(Q.shape[0],D)$ 实际上对于 $K^T\times V$ 过程里的 $\rm K.shape[1]=V.shape[0]$ 具体值无所谓，也就是说其实只需要更改 $Q$ 的第一个维度，即仅对 $Q$ 进行 pooling 即可使维度降低，对 $K,V$ 理论上不需要进行 pooling，但是实验表明对 $K,V$ 的池化甚至更加重要，并且为了能更好地适应不同状态下的池化，这里令每个 stage 的每一次 MHPA 都对 $K,V$ 进行池化，并且这些池化卷积核在仅在同一个 stage 内相等，即 $\Theta_K=\Theta_V$。</p>
<ul>
<li>MViTv2</li>
</ul>
<p>⚠️对于结合 FPN-mask-RCNN 做目标检测，因为任务和研究方向毫不相关，因此只看 MHPA。</p>
<h5 id="MHPA-improved"><a href="#MHPA-improved" class="headerlink" title="MHPA improved"></a>MHPA improved</h5><p>&emsp;&emsp;改进后的 MHPA 基本就是按照上述 MViT 中的改进点加入了两项改动：1.加入相对位置编码，即在 $QK^T$ 之后加入相对位置编码，以此来保证相对位置一致的 patch 之间不会因为绝对位置编码产生不同的影响。2.加入残差池化连接，即对池化之后的 $Q$ 进行残差连接至 $Z$，消融实验表明残差连接是有效的，加入这项改进的原因是对 $K,V$ 的池化操作实际在同一个 stage 的每一步都在执行，而 $Q$ 只在第一步执行了池化。</p>
<p><img src="/video_transformers/image-20221025094744686.png" alt="image-20221025094744686" style="zoom:80%;"></p>
<p>&emsp;&emsp;对于相对位置编码，具体来说，该编码是可学习的，记其为 $R_{p(i),p(j)}$，其中 $p(i),p(j)$ 分别代表在时空维度内 $i,j$ 下标的位置，$R$ 即为这两个位置之间的 PE。结合方式为：</p>
<script type="math/tex; mode=display">
\rm Attn(Q,K,V)=Softmax(\frac{QK^T+E^{(rel)}}{\sqrt d})V</script><script type="math/tex; mode=display">
\rm where \quad E_{ij}^{(rel)} = Q_i\times R_{p(i),p(j)}</script><p>&emsp;&emsp;但是这种计算方法由于需要考虑三个维度的位置，具体来说计算复杂度为 $O(THW)$，因此作者尝试将其进行分解，三个维度分别计算再相加，这种方式算出的结果或许会有重复但是不影响相对位置的思路。具体的分解方式为 $R_{p(i),p(j)}=R_{h(i),h(j)}^H+R_{w(i),w(j)}^W+R_{t(i),t(j)}^T$，这个公式很明确，简单以下图举例，在下图中展示了一种简单的 $HW$ 相对位置编码示例，在这里只需要计算两个维度的位置编码，具体的位置编码使用横纵坐标求和。具体的实现代码稍微有点多，处理上中规中矩但比较复杂，详见 github_repo:MViT/mvit/models/attention.py:45。</p>
<p><img src="/video_transformers/MViTv2_pos.png" style="zoom:50%;"></p>
<p>&emsp;&emsp;对于残差池化连接，具体来说就是 $\rm Z=Attn(Q,K,V)+Q$，实现起来也就几行代码。</p>
<h4 id="结果-5"><a href="#结果-5" class="headerlink" title="结果"></a>结果</h4><ul>
<li>MViT</li>
</ul>
<p><img src="/video_transformers/image-20221024230546563.png" alt="image-20221024230546563" style="zoom:67%;"></p>
<p>&emsp;&emsp;结果随便举个在 K600 上的测试对比，可以看出来简直无敌，且不说超了需要基于 IN-21K 预训练的 ViViT一个点多，甚至参数量和 flops 都是数量级级别的下降，是非常友好于复现和自主训练的，这要多多归功于每过一个 stage 之后的序列长度都会减少。由于此工作独立于 swinTransformer，其实其效果是不如 swinTransformer 的，但是在 MViTv2 还是超过了 swinTransformer。</p>
<p><img src="/video_transformers/image-20221024231006088.png" alt="image-20221024231006088" style="zoom:70%;"></p>
<p>&emsp;&emsp;除此之外这个结果也很大程度上证明了这项工作的有效性，通过 shuffle 输入的方式分别训练 MViT 和 VIT，可以看出 MViT 效果大打折扣，而 VIT 几乎没有变化，而视频的前后性是一项特别在视频领域的知识，显然 VIT 没有利用上这一知识，而 MViT 学到了这项知识。</p>
<ul>
<li>MViTv2</li>
</ul>
<p>&emsp;&emsp;这里做了物体检测、图像分类等多种实验，但是只看 video K400 的（毕竟只有 video 和 HR 有点关系）</p>
<p><img src="/video_transformers/image-20221031104134639.png" alt="image-20221031104134639"></p>
<p>&emsp;&emsp;对比 MViT 的结果，这里大致提了两个点左右，这说明提出的两点改进是有效的，并且参数量和计算量甚至还降了一点点。至于更大的模型，在不经过预训练的情况下明显超过了 swin，注意这里的 Swin-L 是下一篇 paper Video Swin Transformer。好像顺序反了，MViTv2 更好一点呢好像 😅</p>
<hr>
<h3 id="Video-Swin-Transformer"><a href="#Video-Swin-Transformer" class="headerlink" title="Video Swin Transformer"></a>Video Swin Transformer</h3><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Video_Swin_Transformer_CVPR_2022_paper.pdf">video swin transformer</a> 【视频动作识别】 CVPR2022</p>
<p>&emsp;&emsp;在视频领域有 ViViT，Timesformer 等工作珠玉在前，MSRA 之前又有一篇绝对领跑级别的工作 swinTransformer，毫无疑问 MSRA 会将其迁移到视频领域，于是有了 video swinTransformer，看名字大致就知道干了什么，除了效果好、实验多之外没有可圈可点的地方，简要介绍一下本文的 3D 处理。</p>
<h4 id="网络-5"><a href="#网络-5" class="headerlink" title="网络"></a>网络</h4><h5 id="前向过程-1"><a href="#前向过程-1" class="headerlink" title="前向过程"></a>前向过程</h5><p><img src="/video_transformers/Video_ST.png" alt="Video_ST"></p>
<p>&emsp;&emsp;基础的网络完全接近于 <a href="https://bnucsy.github.io/swin_transformer/">swinTransformer</a>，基本上仅在细节处有所不同。大致可以分为 embedding，backbone，head 三个部分。</p>
<p>&emsp;&emsp;embedding 具体来说，采样之后的 video 为 32x224x224，即 $T\times H\times W\times 3$，在打 patch 的时候每一个 patch 的 size 为 $P\times M\times M$，标准 VST 里面是 2x4x4，每个 window 具有 8x7x7 个 patch。</p>
<p>&emsp;&emsp;在经过和 swinTransformer 一样的打 patch 和线性投射层之后就分别通过 4 个 stage，后三个 stage 先进行 patch merging 再通过 VST 的 transformer 块，最后输出得到 $\frac T 2\times \frac H {32}\times \frac W{32}\times 8C$ 的输出，这里的输出是具备语义信息的表征向量，接下来需要再经过一个 head 网络融合这些 3D 信息，作者使用的是 I3D。</p>
<h5 id="网络模块"><a href="#网络模块" class="headerlink" title="网络模块"></a>网络模块</h5><p>&emsp;&emsp;对于 transformer 块，整体来说由上述网络图可以大致看出和 swinTransformer 基本一样，只有 3D W-MSA 和 3D SW-MSA 多了 3D，具体来说，3D W-MSA 将得到的 tokens 分成不同的 window 进行分窗口的独立自注意力计算，然后进行移位，3D SW-MSA 计算各个窗口之间的自注意力，大致过程如下图。</p>
<p><img src="/video_transformers/image-20221031095401323.png" alt="image-20221031095401323" style="zoom:80%;"></p>
<p>&emsp;&emsp;在移位之后会遇到和 swinTransformer 一样的问题，也就是计算量过大，在 3D 的情况下按照如下操作进行 cycle shift。</p>
<p><img src="/video_transformers/Video_ST_Cycle_shift-1667182367555.png" style="zoom:45%;"></p>
<p>&emsp;&emsp;对于 patch merging，结合上述移位操作的图，也就是对每一个 $T$ 维度进行和 swinTransformer 一样的降采样操作。 </p>
<h5 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h5><p>&emsp;&emsp;相比于 2D 版本的 swinTransformer，3D 模型的训练更加复杂和困难，而为了解决这个问题，本文采取了和 I3D 思路基本一致的初始化方法。具体来说，是对于已经经过预训练的 swinTransformer，embedding层输入的 shape 时间维度为 2，因此将权重直接复制两次，然后每个值乘 0.5；transformer 块中输入 shape 时间维度为 $2P-1$，因此将权重直接复制 $2P-1$ 次。两个复制一个做平均，另一个没有做，各有各的理由，反正都是结论倒退的，看个乐就得了。</p>
<h4 id="结果-6"><a href="#结果-6" class="headerlink" title="结果"></a>结果</h4><p>&emsp;&emsp;本文做了很多消融实验，意图按照之前的 ViViT、VTN、Timesformer 等网络的设置改进 VST，当然结果是改了之后也不如本身，不然就是改完之后的是 backbone 了。</p>
<p>&emsp;&emsp;对于不同时空注意力设计的消融实验如下图，本文共设计了三种不同的注意力形式，其中 joint 是 backbone 的默认形式，split 是在 2D swinTransformer 中的 MSA 之后接两个时间注意力层，factorized 是在同样的位置加一个时间自注意力层。</p>
<p><img src="/video_transformers/image-20221031101707798.png" alt="image-20221031101707798" style="zoom:90%;"></p>
<p>&emsp;&emsp;对于时间维度的值的消融实验不展示了，结论是无论是帧采样还是窗口大小，越大的时间维度就意味着越好的效果。同样的还有 3D SW-MSA 的消融实验，结论是无论是时间 shift 还是空间 shift 都不可或缺。</p>
<p>&emsp;&emsp;对于 backbone 和 head 的学习率的消融实验如下图，本文发现了由于 head 随机初始化，对于预训练参数初始化的 backbone 最好使用更低的学习率训练，具体来说，本文尝试了 $\rm ratio=\frac{lr_{backbone}}{lr_{head}}=1/0.1$，发现 0.1 更加合适，然后给了个结论倒推的原因，即：在拟合新的视频输入时，主干会慢慢忘记预先训练好的参数和数据，从而更好地泛化。</p>
<p><img src="/video_transformers/image-20221031103531905.png" alt="image-20221031103531905" style="zoom:77%;"></p>
<p>&emsp;&emsp;和 MViT 一样，本文也测试了 shuffle 之后的结果，不过他既然测了这个，结果肯定是 shuffle 之后效果变差了，这样才能说明自己学到了时间信息。</p>
<p>&emsp;&emsp;具体到网络的结果，本文最高的 Swin-L 在 200M 参数、2107 Flops 、IN-21K 预训练的情况下达到了 84.9，整体上由于 ViViT，不过很快就被 MViTv2 超了。</p>
]]></content>
      <categories>
        <category>paper</category>
        <category>cv</category>
        <category>video</category>
      </categories>
  </entry>
  <entry>
    <title>图像对齐和图像拼接</title>
    <url>/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E5%AF%B9%E9%BD%90/</url>
    <content><![CDATA[<p>👉👉👉👉👉👉本次实验选取来自 <a href="http://www.liushuaicheng.org/TIP/VideoStitching2016/index.html">论文</a> 的数据集  video2 的视频帧，通过 SIFT 特征点检测进行图像对齐，其第一帧的左，右视图如下：</p>
<p><img src="/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E5%AF%B9%E9%BD%90/image-20221127210155800.png" alt="image-20221127210155800" style="zoom: 150%;"></p>
<span id="more"></span>
<h3 id="图像的-SIFT-特征检测原理"><a href="#图像的-SIFT-特征检测原理" class="headerlink" title="图像的 SIFT 特征检测原理"></a>图像的 SIFT 特征检测原理</h3><p>&emsp;&emsp;SIFT 特征全称 Scale Invariant Feature Transform（尺度不变特征变换），通过 SIFT 生成的特征点具备对旋转、尺度缩放、平移、亮度变化、视角变化、仿射变换、噪声的鲁棒性。</p>
<p>&emsp;&emsp;为了寻找视觉不变性的特征，需要对图像使用尺度空间方法，即在图像模型中引入尺度参数。常见的尺度空间由 LoG 产生，但是 LoG 较难计算，后面发现 DoG 能够近似代替 LoG，因此 SIFT 使用 DoG 空间作为尺度空间获取关键点。</p>
<h4 id="DoG-空间的计算"><a href="#DoG-空间的计算" class="headerlink" title="DoG 空间的计算"></a>DoG 空间的计算</h4><p>&emsp;&emsp;对 DoG 空间的有效性可以这样理解：使用高 $\sigma$ 和低 $\sigma$ 分别产生的模糊与不模糊的图像作差即可得到边缘。这也解释了 DoG 空间检测出的极值点实际上对边缘具备更强的响应。</p>
<p>&emsp;&emsp;在计算 DoG 空间之前需要先计算 SIFT 高斯金字塔，区别于普通的高斯金字塔，SIFT 高斯金字塔每一层具有多个图像，每一层最低端的图像单独拿出来就是普通的高斯金字塔。在金字塔的每一层内，从下往上的 $\sigma$ 逐渐增大，即在每一层内越向上的图像越模糊。</p>
<p>&emsp;&emsp;对于构建好的 SIFT 金字塔中每个层的图像做上下差分，即可得到 DoG 空间。</p>
<h4 id="使用-DoG-空间获取极值点"><a href="#使用-DoG-空间获取极值点" class="headerlink" title="使用 DoG 空间获取极值点"></a>使用 DoG 空间获取极值点</h4><p>&emsp;&emsp;这里采用的判断逻辑是斑点的判断方式，但是和斑点不同的是 DoG 的极值点需要额外对比上下两层的点，类似于使用 3Dconv 进行卷积。 </p>
<p><img src="/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E5%AF%B9%E9%BD%90/image-20221127203341901.png" alt="image-20221127203341901"></p>
<h4 id="关键点处理"><a href="#关键点处理" class="headerlink" title="关键点处理"></a>关键点处理</h4><p>&emsp;&emsp;对于上一步中获得的极值点，其具备一些缺点：1. 对边缘响应太大，2. 同一个特征检测出的特征点较多，3. 特征点是离散像素，位置不够精确。而对于这些缺点，接下来需要进一步处理这些特征点。</p>
<p>&emsp;&emsp;对于边缘响应问题，需要计算特征点处的 Hessian 矩阵 H，这个矩阵的特征值代表了 x, y 方向的梯度，接下来需要利用两个特征值的比计算出一个阈值，将横边缘相比垂直边缘具备较大主曲率的特征点剔除；对于特征点多的问题，采用 NMS 过一遍即可；对于离散像素位置不精确问题，需要对尺度空间进行曲线拟合，利用 DoG 函数的泰勒展开计算出插值中心的偏移量，针对偏移量的大小判断是否需要改变关键点位置并逐次迭代，最终插值中心就是原位置加上基于尺度的偏移量。</p>
<h4 id="估计关键点方向"><a href="#估计关键点方向" class="headerlink" title="估计关键点方向"></a>估计关键点方向</h4><p>&emsp;&emsp;由于关键点同时应当具备旋转不变形，因此需要确定关键点的方向，其做法如下：</p>
<ul>
<li>在不同的尺度上计算图像的梯度，计算范围是特征点的邻域，然后统计成直方图</li>
<li>将直方图中最大值的梯度定为主方向，超过最大值80%的方向定为辅方向【将所有的梯度归到八个正交方向上】</li>
</ul>
<p><img src="/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E5%AF%B9%E9%BD%90/image-20221127204838014.png" alt="image-20221127204838014" style="zoom:50%;"></p>
<h4 id="生成关键点的描述子"><a href="#生成关键点的描述子" class="headerlink" title="生成关键点的描述子"></a>生成关键点的描述子</h4><p>&emsp;&emsp;为了保证旋转不变性，需要将特征点邻域内的图像旋转一个方向角 $\theta$，然后重新栅格化图像，更新梯度。接着对每个特征点的邻域分成十六个子邻域，每个子邻域的梯度分到八个方向，统计各个方向的梯度信息最终形成一个 128 维度的向量作为特征描述。</p>
<p>&emsp;&emsp;为了去除光照产生的影响，对特征向量进行归一化处理，然后为了去除相机饱和度等因素的影响，对梯度较大的值进行截断（0.2），然后再进行一次归一化，最终根据特征点的尺度对特征向量进行排序即可得到最终的描述子。</p>
<p>👉👉👉👉👉👉经过 SIFT 特征检测得到的特征如下图所示</p>
<p><img src="/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E5%AF%B9%E9%BD%90/image-20221127210417139.png" alt="image-20221127210417139" style="zoom:150%;"></p>
<h3 id="特征描述子的距离匹配和-RANSAC-方法"><a href="#特征描述子的距离匹配和-RANSAC-方法" class="headerlink" title="特征描述子的距离匹配和 RANSAC 方法"></a>特征描述子的距离匹配和 RANSAC 方法</h3><p>&emsp;&emsp;为了描述特征的距离，可以使用简单的 L2 距离，但是这样对于比较接近的特征点会出现错误匹配的情况。</p>
<p>&emsp;&emsp;因此更多的选用 ratio distance，即 $ d=\frac{||f_1-f_2||}{||f_1-f_2’||}$，这种方式计算了第一相似的点和第二相似的点之间的相似度差距，这种差距比较大是符合我们期望的，因为比较大就代表了我们得到的特征混淆可能性低，反之则代表相似的特征点比较多，这个特征不太合适用作对齐。</p>
<p>&emsp;&emsp;而对于 RANSAC 方法，其本身是一个从误匹配或离群点较多的数据中正确估计数学模型的方法。该方法是基于迭代的概率性算法，其假设为：内点表示正常的数据点，而外点表示误匹配和离群点，整个数据集由内点和外点组成。</p>
<ul>
<li>选择出可以估计出模型的最小数据集（对于 H 矩阵随机选择 4 个点即可）</li>
<li>使用这个数据集计算出模型 H</li>
<li>将所有数据代入 H，计算内点的数目，比较当前的内点数目和之前最好的内点数，并且选择是否更新模型</li>
<li>重复前三步，直到目前的内点数目大于给定的阈值或超过迭代次数</li>
</ul>
<p>&emsp;&emsp;这里的迭代次数是可以计算的，假设总数据中内点的概率为 $t$，预期能够获取的模型 H 的正确概率为 $P$，每次模型采样 $N$ 个点，（H 矩阵为 4）则迭代次数 $k=\frac{\log (1-P)}{\log (1-t^N)}$。</p>
<p>👉👉👉👉👉👉对 SIFT 特征子的距离匹配（仅匹配概率最大的前 100 个点）</p>
<p><img src="/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E5%AF%B9%E9%BD%90/image-20221127211847840.png" alt="image-20221127211847840" style="zoom:150%;"></p>
<p>👉👉👉👉👉👉使用 RANSAC 方法找到在上述匹配中的内点，并且构造出模型 H，结果为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">H = [[ <span class="number">9.03698224e-01</span> -<span class="number">3.28876988e-02</span>  <span class="number">1.58813667e+02</span>]</span><br><span class="line"> [-<span class="number">2.37023637e-02</span>  <span class="number">9.45220683e-01</span>  <span class="number">5.04870429e+01</span>]</span><br><span class="line"> [-<span class="number">7.30423646e-05</span> -<span class="number">2.73295440e-05</span>  <span class="number">1.00000000e+00</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="基于单应矩阵-H-的图像变换与拼接"><a href="#基于单应矩阵-H-的图像变换与拼接" class="headerlink" title="基于单应矩阵 H 的图像变换与拼接"></a>基于单应矩阵 H 的图像变换与拼接</h3><p>&emsp;&emsp;基于已经求出的单应矩阵 H，使用 warp 方法对左图进行 Perspective 变换得到 warp 之后的图像以及合成之后的总图像：</p>
<p><img src="/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E5%AF%B9%E9%BD%90/image-20221127212531760.png" alt="image-20221127212531760" style="zoom:150%;"></p>
<p>&emsp;&emsp;其中图像融合的方法为：</p>
<ul>
<li>对左图进行 warp</li>
<li>将右图填充到左图的左侧</li>
</ul>
<p>&emsp;&emsp;这里无论是计算 left 映射到 right 的 H 矩阵还是计算 right 到 left 的 H 矩阵都可以得到同样的结果，从效果上看，右面似乎更好一些，但是这是和图像质量密切相关的：</p>
<p><img src="/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E5%AF%B9%E9%BD%90/image-20221127213833895.png" alt="image-20221127213833895" style="zoom:150%;"></p>
<h3 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h3><p>&emsp;&emsp;实验的关键在于是否使用 RANSAC 方法，也就是对于不同特征点的匹配性，消融实验如下：</p>
<p><img src="/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E5%AF%B9%E9%BD%90/image-20221127214148342.png" alt="image-20221127214148342" style="zoom:150%;"></p>
<p>&emsp;&emsp;这里尽管没有使用 RANSAC 方法，warp 之后的图像看上去也没有失真，这是因为之前 match 的特征点仅保留了最接近的 100 个，可见尽管如此仍然由不少的离群点，而如果保留 1000 个match 特征点，不使用 RANSAC 方法则会完全失真，实验结果如下。</p>
<p><img src="/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E5%AF%B9%E9%BD%90/image-20221127214843068.png" alt="image-20221127214843068" style="zoom:150%;"></p>
<h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#%% import</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#%% read image</span></span><br><span class="line">left_img = cv2.imread(<span class="string">&#x27;left.png&#x27;</span>)</span><br><span class="line">right_img = cv2.imread(<span class="string">&#x27;right.png&#x27;</span>)</span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">ax[<span class="number">0</span>].imshow(left_img)</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&#x27;Left Image&#x27;</span>)</span><br><span class="line">ax[<span class="number">0</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>].imshow(right_img)</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&#x27;Right Image&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"><span class="comment">#%% SIFT feature detection</span></span><br><span class="line">li_gray = cv2.cvtColor(left_img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">ri_gray = cv2.cvtColor(right_img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">sift = cv2.SIFT_create()</span><br><span class="line">kp1, des1 = sift.detectAndCompute(li_gray, <span class="literal">None</span>)</span><br><span class="line">kp2, des2 = sift.detectAndCompute(ri_gray, <span class="literal">None</span>)</span><br><span class="line">li, ri = left_img.copy(), right_img.copy()</span><br><span class="line">cv2.drawKeypoints(li, kp1, li, (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">cv2.drawKeypoints(ri, kp2, ri, (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">ax[<span class="number">0</span>].imshow(li)</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&#x27;Left Image&#x27;</span>)</span><br><span class="line">ax[<span class="number">0</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>].imshow(ri)</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&#x27;Right Image&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"><span class="comment">#%% feature matching</span></span><br><span class="line">matcher = cv2.DescriptorMatcher_create(<span class="string">&quot;BruteForce&quot;</span>)</span><br><span class="line">matches = <span class="built_in">list</span>(matcher.match(des1, des2, <span class="literal">None</span>))</span><br><span class="line">matches.sort(key=<span class="keyword">lambda</span> x: x.distance, reverse=<span class="literal">False</span>)</span><br><span class="line">matches_all = matches.copy()</span><br><span class="line">matches = matches[:<span class="number">100</span>]</span><br><span class="line">imMatches = cv2.drawMatches(left_img, kp1, right_img, kp2, matches, <span class="literal">None</span>)</span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">ax.imshow(imMatches)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Matches&#x27;</span>)</span><br><span class="line">ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"><span class="comment">#%% RANSAC to find homography H</span></span><br><span class="line">match_kp_left = np.array([kp1[m.queryIdx].pt <span class="keyword">for</span> m <span class="keyword">in</span> matches])</span><br><span class="line">match_kp_right = np.array([kp2[m.trainIdx].pt <span class="keyword">for</span> m <span class="keyword">in</span> matches])</span><br><span class="line"><span class="comment"># (H, status) = cv2.findHomography(match_kp_left, match_kp_right, cv2.RANSAC, 4.0)</span></span><br><span class="line">(H2, status2) = cv2.findHomography(match_kp_left, match_kp_right, <span class="literal">None</span>, <span class="number">4.0</span>)</span><br><span class="line"></span><br><span class="line">match_kp_left_all = np.array([kp1[m.queryIdx].pt <span class="keyword">for</span> m <span class="keyword">in</span> matches_all])</span><br><span class="line">match_kp_right_all = np.array([kp2[m.trainIdx].pt <span class="keyword">for</span> m <span class="keyword">in</span> matches_all])</span><br><span class="line">(H, status) = cv2.findHomography(match_kp_left_all, match_kp_right_all, cv2.RANSAC, <span class="number">4.0</span>)</span><br><span class="line">(H3, status3) = cv2.findHomography(match_kp_left_all, match_kp_right_all, <span class="literal">None</span>, <span class="number">4.0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;H = <span class="subst">&#123;H&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;H2 = <span class="subst">&#123;H2&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;H3 = <span class="subst">&#123;H3&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># H =&gt; 使用 RANSAC 算法计算的单应性矩阵，保留特征点前 100 个</span></span><br><span class="line"><span class="comment"># H2 =&gt; 不使用 RANSAC 算法计算的单应性矩阵，保留特征点前 100 个</span></span><br><span class="line"><span class="comment"># H3 =&gt; 不使用 RANSAC 算法计算的单应性矩阵，保留特征点前 1000 个</span></span><br><span class="line"><span class="comment">#%% warp image</span></span><br><span class="line">warp_left = cv2.warpPerspective(left_img, H, (left_img.shape[<span class="number">1</span>] + right_img.shape[<span class="number">1</span>], left_img.shape[<span class="number">0</span>]))</span><br><span class="line">warp_all = warp_left.copy()</span><br><span class="line">warp_all[<span class="number">0</span>:right_img.shape[<span class="number">0</span>], <span class="number">0</span>:right_img.shape[<span class="number">1</span>]] = right_img</span><br><span class="line">fig, ax = plt.subplots(<span class="number">2</span>, <span class="number">1</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">ax[<span class="number">0</span>].imshow(warp_left)</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&#x27;Warp Left Image&#x27;</span>)</span><br><span class="line">ax[<span class="number">0</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>].imshow(warp_all)</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&#x27;Warp All Image&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"><span class="comment">#%% warp image w/o RANSAC in 100 matches</span></span><br><span class="line">warp_left_none = cv2.warpPerspective(left_img, H2, (left_img.shape[<span class="number">1</span>] + right_img.shape[<span class="number">1</span>], left_img.shape[<span class="number">0</span>]))</span><br><span class="line">warp_all_none = warp_left_none.copy()</span><br><span class="line">warp_all_none[<span class="number">0</span>:right_img.shape[<span class="number">0</span>], <span class="number">0</span>:right_img.shape[<span class="number">1</span>]] = right_img</span><br><span class="line">fig, ax = plt.subplots(<span class="number">2</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">0</span>].imshow(warp_left_none)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">0</span>].set_title(<span class="string">&#x27;Warp Left Image none&#x27;</span>)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">0</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">1</span>].imshow(warp_all_none)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">1</span>].set_title(<span class="string">&#x27;Warp All Image none&#x27;</span>)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">1</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">0</span>].imshow(warp_left)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">0</span>].set_title(<span class="string">&#x27;Warp Left Image RANSAC&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">0</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">1</span>].imshow(warp_all)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">1</span>].set_title(<span class="string">&#x27;Warp All Image RANSAC&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">1</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"><span class="comment">#%% warp image w/o RANSAC in 1000 matches</span></span><br><span class="line">warp_left_all = cv2.warpPerspective(left_img, H3, (left_img.shape[<span class="number">1</span>] + right_img.shape[<span class="number">1</span>], left_img.shape[<span class="number">0</span>]))</span><br><span class="line">warp_all_all = warp_left_all.copy()</span><br><span class="line">warp_all_all[<span class="number">0</span>:right_img.shape[<span class="number">0</span>], <span class="number">0</span>:right_img.shape[<span class="number">1</span>]] = right_img</span><br><span class="line">fig, ax = plt.subplots(<span class="number">2</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">0</span>].imshow(warp_left_all)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">0</span>].set_title(<span class="string">&#x27;Warp Left Image None 1000&#x27;</span>)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">0</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">1</span>].imshow(warp_all_all)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">1</span>].set_title(<span class="string">&#x27;Warp All Image None 1000&#x27;</span>)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">1</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">0</span>].imshow(warp_left)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">0</span>].set_title(<span class="string">&#x27;Warp Left Image RANSAC 1000&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">0</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">1</span>].imshow(warp_all)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">1</span>].set_title(<span class="string">&#x27;Warp All Image RANSAC 1000&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>, <span class="number">1</span>].axis(<span class="string">&#x27;off&#x27;</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>methods</category>
      </categories>
  </entry>
  <entry>
    <title>复数</title>
    <url>/%E5%A4%8D%E6%95%B0/</url>
    <content><![CDATA[<h3 id="分母有理化"><a href="#分母有理化" class="headerlink" title="分母有理化"></a>分母有理化</h3><span id="more"></span>
<p>定义：将分母中含有根式的分数通过变化使得分母中仅含有有理数</p>
<p>:warning:平方差公式：$(a+b)(a-b)=a^2-b^2$</p>
<ul>
<li>分母中仅含有根式</li>
</ul>
<script type="math/tex; mode=display">
\frac {1+\sqrt 2}{\sqrt 3}\to(上下同\times分母)\to\frac {(1+\sqrt2)\sqrt3}{\sqrt3\times\sqrt3}\to\frac{\sqrt3+\sqrt6}{3}</script><ul>
<li>分母中含有根式和整式</li>
</ul>
<script type="math/tex; mode=display">
\frac{1+\sqrt2}{\sqrt3+1}\to(上下同\times分母的平方差公式)\to\frac{(1+\sqrt2)(\sqrt3-1)}{(\sqrt3+1)(\sqrt3-1)}\to\frac{\sqrt3-1+\sqrt6-\sqrt2}{2}</script><hr>
<h3 id="虚数-复数"><a href="#虚数-复数" class="headerlink" title="虚数 / 复数"></a>虚数 / 复数</h3><p><img src="/%E5%A4%8D%E6%95%B0/image-20221029094307137.png" alt="image-20221029094307137"></p>
<ul>
<li>虚数即在实数轴之外的部分，基本单位是 $i$ ，并且 $i^2=-1$<ul>
<li>虚数的运算除了 $i^2=-1$ 之外，其余和实数完全一致</li>
<li>$i^3=i^2\times i=-1\times i=-i$，更复杂的计算如下：</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
(3i^3+2)(i-1)\\=3i^4-3i^3+2i-2\\=3+3i+2i-2\\=1+5i</script><ul>
<li><strong>复数</strong>即虚数与实数的组合，基本表示形式为 $z=a+bi$，其中 $a,b\in R$<ul>
<li>实部/虚部<ul>
<li>其中 $a$ 叫做复数 $z$ 实部，$bi$ 叫做复数 $z$ 的虚部</li>
<li>:warning: $a,b$ 均可以为 0，也就是说 <strong>所有的 纯实数/纯虚数 本身就是复数</strong></li>
</ul>
</li>
<li>复平面<ul>
<li>复数 $a+bi$ 在复平面对应点 $(a,b)$</li>
</ul>
</li>
<li>复数的模<ul>
<li>对于 $z=a+bi$，$z$ 的模 $|z|=\sqrt{a^2+b^2}$</li>
</ul>
</li>
<li>共轭复数<ul>
<li>对于 $z=a+bi$，$z$ 的共轭(复数) $\bar z=a-bi$ </li>
</ul>
</li>
</ul>
</li>
<li>复数的例子：<ul>
<li>对于 $z=1+2i$<ul>
<li>$z$ 的实部为 $1$，虚部为 $2i$</li>
<li>$z$ 在复平面对应的点为 $(1,2)$</li>
<li>$z$ 的模 $|z|=\sqrt{1^2+2^2}=\sqrt 5$</li>
<li>$z$ 的共轭 $\bar z=1-2i$</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="复数分母实数化"><a href="#复数分母实数化" class="headerlink" title="复数分母实数化"></a>复数分母实数化</h3><p>定义：将分母中含有虚部的分数通过变化使得分母中仅含有实数</p>
<ul>
<li>分母中仅含有虚部</li>
</ul>
<script type="math/tex; mode=display">
\frac{1+2i}{3i}\to(上下同\times i)\to \frac{(1+2i)i}{3i^2}\to\frac{i+2i^2}{3i^2}\to\frac{i-2}{-3}\to\frac{2-i}{3}\\=\frac2 3-\frac1 3i</script><ul>
<li>分母中实部虚部都不为零</li>
</ul>
<script type="math/tex; mode=display">
\frac{1+2i}{3i+1}\to(上下同\times分母的平方差公式)\to\frac{(1+2i)(3i-1)}{(3i+1)(3i-1)}\to\frac{3i-1+6i^2-2i}{9i^2-1}\to\frac{i-7}{-10}\\=\frac7 {10}-\frac1{10}i</script><p>例题1：</p>
<p><img src="/%E5%A4%8D%E6%95%B0/image-20221029100826673.png" alt="image-20221029100826673"></p>
<script type="math/tex; mode=display">
解：\frac{i(1+i)}{(1-i)(1+i)}=\frac{i+i^2}{1-i^2}\to\frac{i-1}{2}\\=-\frac1 2+\frac1 2i</script><p>因此对应点坐标为 $(-\frac1 2,\frac1 2)$ 即第二象限，选 $B$</p>
]]></content>
      <categories>
        <category>math_high_school</category>
      </categories>
  </entry>
  <entry>
    <title>空间解析几何</title>
    <url>/%E7%A9%BA%E9%97%B4%E8%A7%A3%E6%9E%90%E5%87%A0%E4%BD%95/</url>
    <content><![CDATA[<h2 id="空间解析几何"><a href="#空间解析几何" class="headerlink" title="空间解析几何"></a>空间解析几何</h2><span id="more"></span>
<p><img src="/%E7%A9%BA%E9%97%B4%E8%A7%A3%E6%9E%90%E5%87%A0%E4%BD%95/image-20221029104845216.png" alt="image-20221029104845216"></p>
<p>求解步骤：</p>
<ul>
<li>:pensive:写坐标（建立空间直角坐标系 $o-xyz$，并写出各个点的空间坐标）</li>
<li>:smile:写向量（根据写出的坐标求出一些向量的坐标）</li>
<li>:pensive:求法向量（根据同一个平面上两个向量坐标求出平面法向量）</li>
<li>:pensive:求解（解 二面角 / 线面角 的 正弦 / 余弦 / 角度）</li>
</ul>
<h3 id="建系-写坐标"><a href="#建系-写坐标" class="headerlink" title="建系+写坐标"></a>建系+写坐标</h3><ul>
<li>右手坐标系</li>
</ul>
<p><img src="/%E7%A9%BA%E9%97%B4%E8%A7%A3%E6%9E%90%E5%87%A0%E4%BD%95/image-20221029110717885.png" alt="image-20221029110717885"></p>
<h3 id="写向量"><a href="#写向量" class="headerlink" title="写向量"></a>写向量</h3>]]></content>
      <categories>
        <category>math_high_school</category>
      </categories>
  </entry>
</search>
